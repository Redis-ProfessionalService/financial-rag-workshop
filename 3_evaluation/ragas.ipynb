{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <div><img src=\"../assets/redis_logo.svg\" style=\"width: 130px\"> </div>\n",
    "    <div style=\"display: inline-block; text-align: center; margin-bottom: 10px;\">\n",
    "        <span style=\"font-size: 36px;\"><b>Evaluation with RAGAS</b></span>\n",
    "        <br />\n",
    "    </div>\n",
    "    <br />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG\n",
    "\n",
    "The extent to which you can **evaluate** your system is the extent to which you can **improve** your system. Before going to prod, it is in your best interest to establish a framework for quickly and effectively understanding the quality of your RAG application. In this notebook, we will use the RAGAS framework, as proposed by [this paper](https://arxiv.org/pdf/2309.15217), to evaluate our RAG application.\n",
    "\n",
    "Before we dive into the theory though, let's setup the necessary environment and basic RAG application for evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import dotenv\n",
    "# mute warnings\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')\n",
    "# load env vars from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "parent_directory = os.path.dirname(dir_path)\n",
    "os.environ[\"ROOT_DIR\"] = parent_directory\n",
    "\n",
    "#setting the local downloaded sentence transformer models f\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{parent_directory}/models\"\n",
    "SCHEMA_PATH = f\"{parent_directory}/2_RAG_patterns_with_redis/sec_index.yaml\"\n",
    "SOURCE_DOC = '../resources/filings/AAPL/AAPL-2023-10K.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Redis and create chunks to populate the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init Redis connection and index\n",
    "import os\n",
    "from redisvl.index import SearchIndex\n",
    "from redis import Redis\n",
    "\n",
    "# init Redis connection\n",
    "# Replace values below with your own if using Redis Cloud instance\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "\n",
    "prefix = 'chunk'\n",
    "client = Redis.from_url(REDIS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", cache_folder=os.getenv(\"TRANSFORMERS_CACHE\", f\"{parent_directory}/models\"))\n",
    "\n",
    "loader = UnstructuredFileLoader(SOURCE_DOC, mode=\"single\", strategy=\"fast\")\n",
    "\n",
    "# for use later with parent-doc index\n",
    "source_doc = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=0)\n",
    "chunks = loader.load_and_split(text_splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_objs = [\n",
    "    {\n",
    "        \"chunk_id\": f\"{chunk.metadata['source']}-{str(uuid.uuid4())}\",\n",
    "        \"source_doc\": f\"{chunk.metadata['source']}\",\n",
    "        \"content\": chunk.page_content,\n",
    "        \"doc_type\": \"10k\",\n",
    "        \"text_embedding\": np.array(embeddings.embed_query(chunk.page_content)).astype(np.float32).tobytes()\n",
    "    }\n",
    "    for chunk in chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.schema import IndexSchema\n",
    "\n",
    "index_name = 'eval'\n",
    "\n",
    "schema = IndexSchema.from_dict(\n",
    "    {\n",
    "        \"index\": {\n",
    "            \"name\": index_name,\n",
    "            \"prefix\": prefix,\n",
    "            \"storage_type\": \"hash\",\n",
    "        },\n",
    "        \"fields\": [\n",
    "            {\"name\": \"chunk_id\", \"type\": \"tag\"},\n",
    "            {\"name\": \"source_doc\", \"type\": \"tag\"},\n",
    "            {\"name\": \"doc_type\", \"type\": \"tag\"},\n",
    "            {\"name\": \"content\", \"type\": \"text\"},\n",
    "            {\n",
    "                \"name\": \"text_embedding\", \n",
    "                \"type\": \"vector\", \n",
    "                \"attrs\": {\"type\": \"float32\", \"dims\": 384, \"distance_metric\": \"COSINE\", \"algorithm\": \"flat\"},\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# create an index from schema and the client\n",
    "index = SearchIndex(schema, client)\n",
    "index.create(overwrite=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = index.load(index_objs, id_field=\"chunk_id\")\n",
    "len(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vector store\n",
    "This is the same processes as we have done in the previous examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Redis as LangChainRedis\n",
    "\n",
    "\n",
    "# with langchain we can manually modify the default vector schema configuration\n",
    "vector_schema = {\n",
    "    \"name\": \"text_embedding\",        # name of the vector field in langchain\n",
    "    \"algorithm\": \"HNSW\",           # could use HNSW instead\n",
    "    \"dims\": 384,                   # set based on the HF model embedding dimension\n",
    "    \"distance_metric\": \"COSINE\",   # could use EUCLIDEAN or IP\n",
    "    \"datatype\": \"FLOAT32\",\n",
    "}\n",
    "\n",
    "# here we can define the entire schema spec for our index in LangChain\n",
    "index_schema = {\n",
    "    \"vector\": [vector_schema],\n",
    "    \"text\": [{\"name\": \"content\"}, {\"name\": \"source_doc\"}, {\"name\": \"doc_type\"}, {\"name\": \"chunk_id\"}],\n",
    "    \"content_vector_key\": \"text_embedding\" ,   # name of the vector field in langchain\n",
    "}\n",
    "\n",
    "\n",
    "rds = LangChainRedis.from_existing_index(\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name,\n",
    "    schema=index_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out!\n",
    "We can see the vector store is populated and returning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='In May 2023, the Company announced a new share repurchase program of up to $90 billion and raised its quarterly dividend from $0.23 to $0.24 per share beginning in May 2023. During 2023, the Company repurchased $76.6 billion of its common stock and paid dividends and dividend equivalents of $15.0 billion.\\n\\nMacroeconomic Conditions\\n\\nMacroeconomic conditions, including inﬂation, changes in interest rates, and currency ﬂuctuations, have directly and indirectly impacted, and could in the future materially impact, the Company’s results of operations and ﬁnancial condition.\\n\\nApple Inc. | 2023 Form 10-K | 20\\n\\nSegment Operating Performance\\n\\nThe following table shows net sales by reportable segment for 2023, 2022 and 2021 (dollars in millions):\\n\\n2023\\n\\nChange\\n\\n2022\\n\\nChange\\n\\nNet sales by reportable segment:\\n\\nAmericas Europe Greater China Japan Rest of Asia Paciﬁc\\n\\nTotal net sales\\n\\n$\\n\\n$\\n\\n162,560 94,294 72,559 24,257 29,615 383,285\\n\\n(4)% $ (1)% (2)% (7)% 1 % (3)% $\\n\\n169,658 95,118 74,200 25,977 29,375 394,328\\n\\n11 % $ 7 % 9 % (9)% 11 %\\n\\n8 % $\\n\\nAmericas\\n\\nAmericas net sales decreased 4% or $7.1 billion during 2023 compared to 2022 due to lower net sales of iPhone and Mac, partially oﬀset by higher net sales of Services.\\n\\nEurope\\n\\nEurope net sales decreased 1% or $824 million during 2023 compared to 2022. The weakness in foreign currencies relative to the U.S. dollar accounted for more than the entire year-over-year decrease in Europe net sales, which consisted primarily of lower net sales of Mac and Wearables, Home and Accessories, partially oﬀset by higher net sales of iPhone and Services.\\n\\nGreater China\\n\\nGreater China net sales decreased 2% or $1.6 billion during 2023 compared to 2022. The weakness in the renminbi relative to the U.S. dollar accounted for more than the entire year-over-year decrease in Greater China net sales, which consisted primarily of lower net sales of Mac and iPhone.\\n\\nJapan\\n\\nJapan net sales decreased 7% or $1.7 billion during 2023 compared to 2022. The weakness in the yen relative to the U.S. dollar accounted for more than the entire year-over-year decrease in Japan net sales, which consisted primarily of lower net sales of iPhone, Wearables, Home and Accessories and Mac.\\n\\nRest of Asia Paciﬁc', metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10k.pdf-4b28c4c9-508d-4398-8489-6b201a59e96f', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10k.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10k.pdf-4b28c4c9-508d-4398-8489-6b201a59e96f'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rds.similarity_search(\"What was apples revenue last year?\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "\n",
    "# # for use in ragas increase context window\n",
    "# llm = Ollama(model=\"llama3\", num_ctx=4097, temperature=0.1)\n",
    "\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "\n",
    "vllm = VLLMOpenAI(\n",
    "            openai_api_key=os.environ[\"HF_MODEL_HUB_TOKEN\"], # vllm token key for huggingface through openai like interface\n",
    "            openai_api_base=os.environ[\"VLLM_URL\"],\n",
    "            model_name=os.environ[\"LOCAL_VLLM_MODEL\"],\n",
    "            temperature=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt():\n",
    "    \"\"\"Create the QA chain.\"\"\"\n",
    "    from langchain.prompts import PromptTemplate\n",
    "\n",
    "    # Define our prompt\n",
    "    prompt_template = \"\"\"Use the following pieces of context from financial 10k filings data to answer the user question at the end. Only use the result from tools and evidence provided to you. If you don't know the answer, say that you don't know, don't try to make up an answer. Provide the source of the document that you used to get the answer.\n",
    "\n",
    "    This should be in the following format:\n",
    "\n",
    "    Question: [question here]\n",
    "    Answer: [answer here]\n",
    "    Source: [source document here]\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Context:\n",
    "    ---------\n",
    "    {context}\n",
    "    ---------\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def get_search_kwargs(filters, distance_threshold):\n",
    "    return {\"distance_threshold\":distance_threshold,\"filter\":filters}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=vllm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=rds.as_retriever(search_type=\"similarity_distance_threshold\", search_kwargs={\"distance_threshold\":0.8, 'include_metadata': True}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": get_prompt()},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have our RAG QA to test out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robert.shelton/.pyenv/versions/3.11.9/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': \"What was Apple's revenue last year compared to this year??\",\n",
       " 'result': \"Question: What was Apple's revenue last year compared to this year??\\n\\nAnswer: According to the provided context from Apple's 2023 Form 10-K, Apple's total net sales for 2022 were $394,328 million, and for 2023, they were $383,285 million. Therefore, Apple's revenue decreased by approximately $11 billion or 3% from 2022 to 2023.\\n\\nSource: Apple Inc. | 2023 Form 10-K | Pages 20-21\",\n",
       " 'source_documents': [Document(page_content='In May 2023, the Company announced a new share repurchase program of up to $90 billion and raised its quarterly dividend from $0.23 to $0.24 per share beginning in May 2023. During 2023, the Company repurchased $76.6 billion of its common stock and paid dividends and dividend equivalents of $15.0 billion.\\n\\nMacroeconomic Conditions\\n\\nMacroeconomic conditions, including inﬂation, changes in interest rates, and currency ﬂuctuations, have directly and indirectly impacted, and could in the future materially impact, the Company’s results of operations and ﬁnancial condition.\\n\\nApple Inc. | 2023 Form 10-K | 20\\n\\nSegment Operating Performance\\n\\nThe following table shows net sales by reportable segment for 2023, 2022 and 2021 (dollars in millions):\\n\\n2023\\n\\nChange\\n\\n2022\\n\\nChange\\n\\nNet sales by reportable segment:\\n\\nAmericas Europe Greater China Japan Rest of Asia Paciﬁc\\n\\nTotal net sales\\n\\n$\\n\\n$\\n\\n162,560 94,294 72,559 24,257 29,615 383,285\\n\\n(4)% $ (1)% (2)% (7)% 1 % (3)% $\\n\\n169,658 95,118 74,200 25,977 29,375 394,328\\n\\n11 % $ 7 % 9 % (9)% 11 %\\n\\n8 % $\\n\\nAmericas\\n\\nAmericas net sales decreased 4% or $7.1 billion during 2023 compared to 2022 due to lower net sales of iPhone and Mac, partially oﬀset by higher net sales of Services.\\n\\nEurope\\n\\nEurope net sales decreased 1% or $824 million during 2023 compared to 2022. The weakness in foreign currencies relative to the U.S. dollar accounted for more than the entire year-over-year decrease in Europe net sales, which consisted primarily of lower net sales of Mac and Wearables, Home and Accessories, partially oﬀset by higher net sales of iPhone and Services.\\n\\nGreater China\\n\\nGreater China net sales decreased 2% or $1.6 billion during 2023 compared to 2022. The weakness in the renminbi relative to the U.S. dollar accounted for more than the entire year-over-year decrease in Greater China net sales, which consisted primarily of lower net sales of Mac and iPhone.\\n\\nJapan\\n\\nJapan net sales decreased 7% or $1.7 billion during 2023 compared to 2022. The weakness in the yen relative to the U.S. dollar accounted for more than the entire year-over-year decrease in Japan net sales, which consisted primarily of lower net sales of iPhone, Wearables, Home and Accessories and Mac.\\n\\nRest of Asia Paciﬁc', metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10k.pdf-4b28c4c9-508d-4398-8489-6b201a59e96f', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10k.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10k.pdf-4b28c4c9-508d-4398-8489-6b201a59e96f'}),\n",
       "  Document(page_content='Rest of Asia Paciﬁc net sales increased 1% or $240 million during 2023 compared to 2022. The weakness in foreign currencies relative to the U.S. dollar had a signiﬁcantly unfavorable year-over-year impact on Rest of Asia Paciﬁc net sales. The net sales increase consisted of higher net sales of iPhone and Services, partially oﬀset by lower net sales of Mac and iPad.\\n\\nApple Inc. | 2023 Form 10-K | 21\\n\\n2021\\n\\n153,306 89,307 68,366 28,482 26,356 365,817\\n\\nProducts and Services Performance\\n\\nThe following table shows net sales by category for 2023, 2022 and 2021 (dollars in millions):\\n\\n2023\\n\\nChange\\n\\n2022\\n\\nChange\\n\\nNet sales by category:\\n\\n(1)\\n\\niPhone (1) Mac iPad Wearables, Home and Accessories Services\\n\\n(1)\\n\\n(2)\\n\\nTotal net sales\\n\\n(1)\\n\\n$\\n\\n$\\n\\n200,583 29,357 28,300 39,845 85,200 383,285\\n\\n(2)% $\\n\\n(27)% (3)% (3)% 9 % (3)% $\\n\\n205,489 40,177 29,292 41,241 78,129 394,328\\n\\n7 % $\\n\\n14 % (8)% 7 % 14 %\\n\\n8 % $\\n\\n(1) Products net sales include amortization of the deferred value of unspeciﬁed software upgrade rights, which are\\n\\nbundled in the sales price of the respective product.\\n\\n(2) Services net sales include amortization of the deferred value of services bundled in the sales price of certain\\n\\nproducts.\\n\\niPhone\\n\\niPhone net sales decreased 2% or $4.9 billion during 2023 compared to 2022 due to lower net sales of non-Pro iPhone models, partially oﬀset by higher net sales of Pro iPhone models.\\n\\nMac\\n\\nMac net sales decreased 27% or $10.8 billion during 2023 compared to 2022 due primarily to lower net sales of laptops.\\n\\niPad\\n\\niPad net sales decreased 3% or $1.0 billion during 2023 compared to 2022 due primarily to lower net sales of iPad mini and iPad Air, partially oﬀset by the combined net sales of iPad 9th and 10th generation.\\n\\nWearables, Home and Accessories\\n\\nWearables, Home and Accessories net sales decreased 3% or $1.4 billion during 2023 compared to 2022 due primarily to lower net sales of Wearables and Accessories.\\n\\nServices\\n\\nServices net sales increased 9% or $7.1 billion during 2023 compared to 2022 due to higher net sales across all lines of business.\\n\\nApple Inc. | 2023 Form 10-K | 22\\n\\n2021\\n\\n191,973 35,190 31,862 38,367 68,425 365,817\\n\\nGross Margin\\n\\nProducts and Services gross margin and gross margin percentage for 2023, 2022 and 2021 were as follows (dollars in millions):\\n\\n2023\\n\\n2022\\n\\nGross margin:\\n\\nProducts Services\\n\\nTotal gross margin\\n\\n$\\n\\n$\\n\\n108,803 $ 60,345 169,148 $\\n\\n114,728 $ 56,054 170,782 $\\n\\nGross margin percentage:\\n\\nProducts Services', metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10k.pdf-48c5e1a9-06bb-4c44-8910-2cae91c26520', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10k.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10k.pdf-48c5e1a9-06bb-4c44-8910-2cae91c26520'}),\n",
       "  Document(page_content='Consolidated Statements of Comprehensive Income for the years ended September 30, 2023,\\n\\nSeptember 24, 2022 and September 25, 2021\\n\\nConsolidated Balance Sheets as of September 30, 2023 and September 24, 2022 Consolidated Statements of Shareholders’ Equity for the years ended September 30, 2023, September\\n\\n24, 2022 and September 25, 2021\\n\\nConsolidated Statements of Cash Flows for the years ended September 30, 2023, September 24, 2022\\n\\nand September 25, 2021\\n\\nNotes to Consolidated Financial Statements Reports of Independent Registered Public Accounting Firm\\n\\nAll ﬁnancial statement schedules have been omitted, since the required information is not applicable or is not present in amounts suﬃcient to require submission of the schedule, or because the information required is included in the consolidated ﬁnancial statements and accompanying notes.\\n\\nApple Inc. | 2023 Form 10-K | 27\\n\\nPage\\n\\n28\\n\\n29 30\\n\\n31\\n\\n32 33 49\\n\\nApple Inc.\\n\\nCONSOLIDATED STATEMENTS OF OPERATIONS (In millions, except number of shares, which are reﬂected in thousands, and per-share amounts)\\n\\nYears ended\\n\\nSeptember 30, 2023\\n\\nSeptember 24, 2022\\n\\nSeptember 25, 2021\\n\\nNet sales: Products Services\\n\\nTotal net sales\\n\\n$\\n\\n298,085 $ 85,200 383,285\\n\\n316,199 $ 78,129 394,328\\n\\n297,392 68,425 365,817\\n\\nCost of sales: Products Services\\n\\nTotal cost of sales Gross margin\\n\\n189,282 24,855 214,137 169,148\\n\\n201,471 22,075 223,546 170,782\\n\\n192,266 20,715 212,981 152,836\\n\\nOperating expenses:\\n\\nResearch and development Selling, general and administrative\\n\\nTotal operating expenses\\n\\n29,915 24,932 54,847\\n\\n26,251 25,094 51,345\\n\\n21,914 21,973 43,887\\n\\nOperating income Other income/(expense), net Income before provision for income taxes Provision for income taxes Net income\\n\\n$\\n\\n114,301 (565) 113,736 16,741 96,995 $\\n\\n119,437 (334) 119,103 19,300 99,803 $\\n\\n108,949 258 109,207 14,527 94,680\\n\\nEarnings per share:\\n\\nBasic Diluted\\n\\n$ $\\n\\n6.16 $ 6.13 $\\n\\n6.15 $ 6.11 $\\n\\n5.67 5.61\\n\\nShares used in computing earnings per share:\\n\\nBasic Diluted\\n\\n15,744,231 15,812,547\\n\\n16,215,963 16,325,819\\n\\n16,701,272 16,864,919\\n\\nSee accompanying Notes to Consolidated Financial Statements.\\n\\nApple Inc. | 2023 Form 10-K | 28\\n\\nApple Inc.\\n\\nCONSOLIDATED STATEMENTS OF COMPREHENSIVE INCOME (In millions)\\n\\nYears ended\\n\\nSeptember 30, 2023\\n\\nSeptember 24, 2022\\n\\nSeptember 25, 2021\\n\\nNet income Other comprehensive income/(loss):\\n\\n$\\n\\n96,995 $\\n\\n99,803 $\\n\\n94,680\\n\\nChange in foreign currency translation, net of tax\\n\\n(765)\\n\\n(1,511)\\n\\n501', metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10k.pdf-559d611f-f193-4abe-8d60-c48810e526af', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10k.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10k.pdf-559d611f-f193-4abe-8d60-c48810e526af'}),\n",
       "  Document(page_content='For the sale of third-party products where the Company obtains control of the product before transferring it to the customer, the Company recognizes revenue based on the gross amount billed to customers. The Company considers multiple factors when determining whether it obtains control of third-party products, including evaluating if it can establish the price of the product, retains inventory risk for tangible products or has the responsibility for ensuring acceptability of the product. For third-party applications sold through the App Store, the Company does not obtain control of the product before transferring it to the customer. Therefore, the Company accounts for all third- party application–related sales on a net basis by recognizing in Services net sales only the commission it retains.\\n\\nApple Inc. | 2023 Form 10-K | 34\\n\\nNet sales disaggregated by signiﬁcant products and services for 2023, 2022 and 2021 were as follows (in millions):\\n\\n2023\\n\\n2022\\n\\n2021\\n\\niPhone (1) Mac iPad Wearables, Home and Accessories Services\\n\\n(1)\\n\\n(1)\\n\\n(2)\\n\\nTotal net sales\\n\\n(1)\\n\\n$\\n\\n$\\n\\n200,583 $ 29,357 28,300 39,845 85,200 383,285 $\\n\\n205,489 $ 40,177 29,292 41,241 78,129 394,328 $\\n\\n191,973 35,190 31,862 38,367 68,425 365,817\\n\\n(1) Products net sales include amortization of the deferred value of unspeciﬁed software upgrade rights, which are\\n\\nbundled in the sales price of the respective product.\\n\\n(2) Services net sales include amortization of the deferred value of services bundled in the sales price of certain\\n\\nproducts.\\n\\nTotal net sales include $8.2 billion of revenue recognized in 2023 that was included in deferred revenue as of September 24, 2022, $7.5 billion of revenue recognized in 2022 that was included in deferred revenue as of September 25, 2021, and $6.7 billion of revenue recognized in 2021 that was included in deferred revenue as of September 26, 2020.\\n\\nThe Company’s proportion of net sales by disaggregated revenue source was generally consistent for each reportable segment in Note 13, “Segment Information and Geographic Data” for 2023, 2022 and 2021, except in Greater China, where iPhone revenue represented a moderately higher proportion of net sales.', metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10k.pdf-077c2044-187b-4333-9eda-449983934a03', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10k.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10k.pdf-077c2044-187b-4333-9eda-449983934a03'})]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What was Apple's revenue last year compared to this year??\"\n",
    "res=qa(query)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup complete!\n",
    "\n",
    "In the resources we have included a pre-generated set of test data for evaluation generated with the TestsetGenerator class from the ragas library for demo speed sake. The code used to generate this data is provided as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "testset = pd.read_csv(\"resources/full_testset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TestsetGenerator example code for generate testset\n",
    "\n",
    "This can be a time consuming process so we have gone ahead and pregenerated this with the following code. See more on creating test sets [here](https://docs.ragas.io/en/latest/getstarted/testset_generation.html).\n",
    "\n",
    "Note: while we are using synthetic test set here RAGAS can be utilized with human labeled data and [self created test sets](https://docs.ragas.io/en/stable/howtos/applications/data_preparation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if problems with nltk data\n",
    "# import os\n",
    "# os.environ[\"NLTK_DATA\"] = '/Users/<user>/nltk_data'\n",
    "\n",
    "if not len(testset):\n",
    "    from ragas.testset.generator import TestsetGenerator\n",
    "    from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "    from llama_index.llms.ollama import Ollama\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "    generator_llm = vllm\n",
    "    critic_llm = vllm\n",
    "    embeddings = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "    generator = TestsetGenerator.from_llama_index(\n",
    "        generator_llm=generator_llm,\n",
    "        critic_llm=critic_llm,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "\n",
    "    reader = SimpleDirectoryReader(input_files=[SOURCE_DOC])\n",
    "\n",
    "    documents = reader.load_data()\n",
    "\n",
    "    testset = generator.generate_with_llamaindex_docs(\n",
    "        documents,\n",
    "        test_size=20,\n",
    "        distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
    "    )\n",
    "\n",
    "    testset.to_pandas().to_csv(\"full_testset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin evaluation\n",
    "[The ragas library](https://docs.ragas.io/en/stable/index.html) provides helpful classes for abstracting the complexity of creating test sets and evaluating apps that use generative technology. Above we demonstrated how the TestsetGenerator class can be used to create an example dataset with. Now we will create a few helper functions to store and aggregate the answers/ context generated/retrieved from the RAG QA app we defined earlier. This data will be what we pass to the ragas library for calculating our performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "def parse_contexts(source_docs):\n",
    "    return [doc.page_content for doc in source_docs]\n",
    "\n",
    "def create_evaluation_dataset(chain, testset):\n",
    "    res_set = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": [],\n",
    "        \"ground_truth\": []\n",
    "    }\n",
    "\n",
    "    for _, row in testset.iterrows():\n",
    "        # call QA chain\n",
    "        result = chain.invoke(row[\"question\"])\n",
    "\n",
    "        res_set[\"question\"].append(row[\"question\"])\n",
    "        res_set[\"answer\"].append(result[\"result\"])\n",
    "\n",
    "        contexts = parse_contexts(result[\"source_documents\"])\n",
    "        \n",
    "        if not len(contexts):\n",
    "            print(f\"no contexts found for question: {row['question']}\")\n",
    "        res_set[\"contexts\"].append(contexts)\n",
    "        res_set[\"ground_truth\"].append(str(row[\"ground_truth\"]))\n",
    "\n",
    "    return res_set\n",
    "\n",
    "def evaluate_chain(chain, testset, test_name, metrics, llm, embeddings):\n",
    "    eval_dataset = create_evaluation_dataset(chain, testset)\n",
    "\n",
    "    parsed = Dataset.from_dict(eval_dataset)\n",
    "\n",
    "    run_config = RunConfig()\n",
    "    run_config.max_retries = 1\n",
    "\n",
    "\n",
    "    eval_result = evaluate(\n",
    "        parsed,\n",
    "        metrics=metrics,\n",
    "        run_config=run_config,\n",
    "        llm=llm,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "    eval_df = eval_result.to_pandas()\n",
    "    # store the results of our test for future reference in csv\n",
    "    eval_df.to_csv(f\"{test_name}.csv\")\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First let's evaluate generation metrics\n",
    "Generation metrics quantify how well the RAG app did creating answers to the provided questions (i.e. the G in **R**etrival **A**ugments **G**eneration). We will calculate the generation metrics **faithfulness** and **answer relevancy** for this example.\n",
    "\n",
    "The ragas libary conveniently abstracts the calculation of these metrics so we don't have to write redundant code but please review the following definitions in order to build intuition around what these metrics actually measure.\n",
    "\n",
    "Note: the following examples are paraphrased from the [ragas docs](https://docs.ragas.io/en/stable/concepts/metrics/index.html)\n",
    "\n",
    "------\n",
    "\n",
    "### Faithfulness\n",
    "\n",
    "An answer to a question can be said to be \"faithful\" if the **claims** that are made in the answer **can be inferred** from the **context**.\n",
    "\n",
    "#### Mathematically:\n",
    "\n",
    "$$\n",
    "Faithfullness\\ score = \\frac{Number\\ of\\ claims\\ in\\ the\\ generated\\ answer\\ that\\ can\\ be\\ inferred\\ from\\ the\\ given\\ context}{Total\\ number\\ of\\ claim\\ in\\ the\\ generated\\ answer}\n",
    "$$\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "> Question: Where and when was Einstein born?\n",
    "> \n",
    "> Context: Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time\n",
    ">\n",
    "> answer: Einstein was born in Germany on 20th March 1879.\n",
    "\n",
    "Step 1: Use LLM to break generated answer into individual statements.\n",
    "- “Einstein was born in Germany.”\n",
    "- “Einstein was born on 20th March 1879.”\n",
    "\n",
    "Step 2: For each statement use LLM to verify if it can be inferred from the context.\n",
    "- “Einstein was born in Germany.” => yes. \n",
    "- “Einstein was born on 20th March 1879.” => no.\n",
    "\n",
    "Step 3: plug into formula\n",
    "\n",
    "#claims inferred from context = 1\n",
    "#tot claims = 2\n",
    "Faithfulness = 1/2\n",
    "\n",
    "### Answer Relevance\n",
    "\n",
    "An answer can be said to be relevant if it directly addresses the question (intuitively).\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "1. Use an LLM to generate \"hypothetical\" questions to a given answer with the following prompt:\n",
    "\n",
    "    > Generate a question for the given answer.\n",
    "    > answer: [answer]\n",
    "\n",
    "2. Embed the generated \"hypothetical\" questions as vectors.\n",
    "3. Calculate the cosine similarity of the hypothetical questions and the original question, sum those similarities, and divide by n.\n",
    "\n",
    "With data:\n",
    "\n",
    "> Question: Where is France and what is it’s capital?\n",
    "> \n",
    "> answer: France is in western Europe.\n",
    "\n",
    "Step 1 - use LLM to create 'n' variants of question from the generated answer.\n",
    "\n",
    "- “In which part of Europe is France located?”\n",
    "- “What is the geographical location of France within Europe?”\n",
    "- “Can you identify the region of Europe where France is situated?”\n",
    "\n",
    "Step 2 - Calculate the mean cosine similarity between the generated questions and the actual question.\n",
    "\n",
    "## Now let's implement using our helper functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fb291cf0b54533ac72c565220061b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")\n",
    "\n",
    "gen_metrics = [\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "]\n",
    "\n",
    "gen_basic_rag_test = evaluate_chain(qa, testset, \"generation_basic_rag\", gen_metrics, vllm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.846994</td>\n",
       "      <td>0.824561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.313799</td>\n",
       "      <td>0.325006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.890512</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.997366</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       answer_relevancy  faithfulness\n",
       "count         19.000000     19.000000\n",
       "mean           0.846994      0.824561\n",
       "std            0.313799      0.325006\n",
       "min            0.000000      0.000000\n",
       "25%            0.890512      0.833333\n",
       "50%            0.997366      1.000000\n",
       "75%            1.000000      1.000000\n",
       "max            1.000000      1.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_basic_rag_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next let's evaluate the retrieval metrics\n",
    "\n",
    "Retrieval metrics quantify how well the system performed at fetching the best possible context for generation. Like before please review the definitions below to understand what happens under-the-hood when we execute the evaluation code. \n",
    "\n",
    "-----\n",
    "\n",
    "### Context Relevance\n",
    "\n",
    "\"The context is considered relevant to the extent that it exclusively contains information that is needed to answer the question.\"\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "1. Use the following LLM prompt to extract a subset of sentences necessary to answer the question. The context is defined as the formatted search result from the vector database.\n",
    "\n",
    "    > Please extract relevant sentences from\n",
    "    > the provided context that can potentially\n",
    "    > help answer the following `{question}`. If no\n",
    "    > relevant sentences are found, or if you\n",
    "    > believe the question cannot be answered\n",
    "    > from the given context, return the phrase\n",
    "    > \"Insufficient Information\". While extracting candidate sentences you’re not allowed to make any changes to sentences\n",
    "    > from given `{context}`.\n",
    "\n",
    "2. Compute the context relevance score = (number of extracted sentences) / (total number of sentences in context)\n",
    "\n",
    "Moving from the initial paper to the active evaluation library ragas there are a few more insightful metrics to evaluate. From the library [source](https://docs.ragas.io/en/stable/concepts/metrics/index.html) let's introduce `context precision` and `context recall`. \n",
    "\n",
    "### Context recall\n",
    "Context can be said to have high recall if retrieved context aligns with the ground truth answer.\n",
    "\n",
    "#### Mathematically:\n",
    "\n",
    "$$\n",
    "Context\\ recall = \\frac{Ground\\ Truth\\ sentences\\ that\\ can\\ be\\ attributed\\ to\\ context}{Total\\ number\\ of\\ sentences\\ in\\ the\\ ground\\ truth}\n",
    "$$\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "Data:\n",
    "> question: Where is France and what is it’s capital?\n",
    "> ground truth answer: France is in Western Europe and its capital is Paris.\n",
    "> context: France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and the vast Palace of Versailles attest to its rich history.\n",
    ">\n",
    "> Note: ground truth answer can be created by critic LLM of with own human labeled data set.\n",
    "\n",
    "Step 1 - use an LLM to break the ground truth down into individual statements:\n",
    "- `France is in Western Europe`\n",
    "- `Its capital is Paris`\n",
    "\n",
    "Step 2 - for each ground truth statement, use an LLM to determine if it can be attributed from the context.\n",
    "- `France is in Western Europe` => yes\n",
    "- `Its capital is Paris` => no\n",
    "\n",
    "\n",
    "Step 3 - plug in to formula\n",
    "\n",
    "context recall = (1 + 0) / 2 = 0.5\n",
    "\n",
    "### Context precision\n",
    "\n",
    "This metrics relates to how chunks are ranked in a response. Ideally the most relevant chunks are at the top.\n",
    "\n",
    "#### Mathematically:\n",
    "\n",
    "$$\n",
    "Context\\ Precision@k = \\frac{precision@k}{total\\ number\\ relevant\\ items\\ in\\ the\\ top\\ k\\ results}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Precision@k = \\frac{true\\ positive@k}{true\\ positives@k + false\\ positives@k}\n",
    "$$\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "Data:\n",
    "> Question: Where is France and what is it’s capital?\n",
    "> \n",
    "> Ground truth: France is in Western Europe and its capital is Paris.\n",
    "> \n",
    "> Context: [ “The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and”, “France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower”]\n",
    "\n",
    "Step 1 - for each chunk use the LLM to check if it's relevant or not to the ground truth answer.\n",
    "\n",
    "Step 2 - for each chunk in the context calculate the precision defined as: ``\n",
    "- `“The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and”` => precision = 0/1 or 0.\n",
    "- `“France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower”` => the precision would be (1) / (1 true positive + 1 false positive) = 0.5. \n",
    "\n",
    "\n",
    "Step 3 - calculate the overall context precision = (0 + 0.5) / 1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f1cf21dc5d44f2bde34d71f837d4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ret_metrics = [\n",
    "    context_precision,\n",
    "    context_recall\n",
    "]\n",
    "\n",
    "ret_basic_rag_test = evaluate_chain(qa, testset, \"retrieval_basic_rag\", ret_metrics, vllm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.862573</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.286061</td>\n",
       "      <td>0.477567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       context_precision  context_recall\n",
       "count          19.000000       19.000000\n",
       "mean            0.862573        0.684211\n",
       "std             0.286061        0.477567\n",
       "min             0.000000        0.000000\n",
       "25%             0.958333        0.000000\n",
       "50%             1.000000        1.000000\n",
       "75%             1.000000        1.000000\n",
       "max             1.000000        1.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_basic_rag_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "We can see from the above results that our basic RAG did okay but not necessarily great. This is okay because now that we have a baseline for the performance of our RAG, we can begin to try different techniques to improve our results. The reason it is so important to have a framework in place for evaluation is now we can properly experiment with different techniques to see what improves our particular system.\n",
    "\n",
    "One technique we could try is to implement a parent document retriever. A parent document retriever attempts to optimize two competing objectives within RAG - 1) smaller chunks can lead to better embeddings since there is less context to lose the point (so to speak) 2) larger chunks help retain what could be valuable overall context to retrieval. Parent document retrieval allows for the initial query search on smaller chunks for specificity but returns the larger chunks for more complete context. \n",
    "\n",
    "Let's perform an experiment to see if this technique improves our metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import TextLoader, UnstructuredFileLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.redis import Redis as LangChainRedis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will make a new index for this example defined directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "PARENT_CHUNK_SIZE = 3000\n",
    "CHILD_CHUNK_SIZE = 400\n",
    "\n",
    "# This text splitter is used to create the parent documents aka larger chunks\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=PARENT_CHUNK_SIZE)\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHILD_CHUNK_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings for redis vector store\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: it is **critical** that our index includes the `doc_id` field otherwise the parent document linking will not happen correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# with langchain we can manually modify the default vector schema configuration\n",
    "vector_schema = {\n",
    "    \"name\": \"chunk_vector\",        # name of the vector field in langchain\n",
    "    \"algorithm\": \"HNSW\",           # could use HNSW instead\n",
    "    \"dims\": 384,                   # set based on the HF model embedding dimension\n",
    "    \"distance_metric\": \"COSINE\",   # could use EUCLIDEAN or IP\n",
    "    \"datatype\": \"FLOAT32\",\n",
    "}\n",
    "\n",
    "# here we can define the entire schema spec for our index in LangChain\n",
    "index_schema = {\n",
    "    \"vector\": [vector_schema],\n",
    "    \"text\": [{\"name\": \"content\"}, {\"name\": \"doc_id\"}],\n",
    "    \"content_vector_key\": \"chunk_vector\" ,   # name of the vector field in langchain\n",
    "}\n",
    "\n",
    "vector_store = LangChainRedis(\n",
    "    REDIS_URL,\n",
    "    \"child_docs\",\n",
    "    embeddings,\n",
    "    index_schema=index_schema\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "parent_doc_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we are adding the source documents and the ParentDocumentRetriever will automatically split them into parent and child documents\n",
    "parent_doc_retriever.add_documents(source_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Fiscal Year Highlights\\n\\nThe Company’s total net sales were $383.3 billion and net income was $97.0 billion during 2023.\\n\\nThe Company’s total net sales decreased 3% or $11.0 billion during 2023 compared to 2022. The weakness in foreign currencies relative to the U.S. dollar accounted for more than the entire year-over-year decrease in total net sales, which consisted primarily of lower net sales of Mac and iPhone, partially oﬀset by higher net sales of Services.\\n\\nThe Company announces new product, service and software oﬀerings at various times during the year. Signiﬁcant announcements during ﬁscal year 2023 included the following:\\n\\nFirst Quarter 2023:\\n\\n• • MLS Season Pass, a Major League Soccer subscription streaming service.\\n\\niPad and iPad Pro; Next-generation Apple TV 4K; and\\n\\nSecond Quarter 2023:\\n\\nMacBook Pro 14”, MacBook Pro 16” and Mac mini; and • Second-generation HomePod.\\n\\nThird Quarter 2023:\\n\\nMacBook Air 15”, Mac Studio and Mac Pro; •\\n\\nApple Vision Pro™, the Company’s ﬁrst spatial computer featuring its new visionOS™, expected to be available in early calendar year 2024; and iOS 17, macOS Sonoma, iPadOS 17, tvOS 17 and watchOS 10, updates to the Company’s operating systems.\\n\\n\\n\\nFourth Quarter 2023:\\n\\n•\\n\\niPhone 15, iPhone 15 Plus, iPhone 15 Pro and iPhone 15 Pro Max; and Apple Watch Series 9 and Apple Watch Ultra 2.\\n\\nIn May 2023, the Company announced a new share repurchase program of up to $90 billion and raised its quarterly dividend from $0.23 to $0.24 per share beginning in May 2023. During 2023, the Company repurchased $76.6 billion of its common stock and paid dividends and dividend equivalents of $15.0 billion.\\n\\nMacroeconomic Conditions\\n\\nMacroeconomic conditions, including inﬂation, changes in interest rates, and currency ﬂuctuations, have directly and indirectly impacted, and could in the future materially impact, the Company’s results of operations and ﬁnancial condition.\\n\\nApple Inc. | 2023 Form 10-K | 20\\n\\nSegment Operating Performance\\n\\nThe following table shows net sales by reportable segment for 2023, 2022 and 2021 (dollars in millions):\\n\\n2023\\n\\nChange\\n\\n2022\\n\\nChange\\n\\nNet sales by reportable segment:\\n\\nAmericas Europe Greater China Japan Rest of Asia Paciﬁc\\n\\nTotal net sales\\n\\n$\\n\\n$\\n\\n162,560 94,294 72,559 24,257 29,615 383,285\\n\\n(4)% $ (1)% (2)% (7)% 1 % (3)% $\\n\\n169,658 95,118 74,200 25,977 29,375 394,328\\n\\n11 % $ 7 % 9 % (9)% 11 %\\n\\n8 % $\\n\\nAmericas\\n\\nAmericas net sales decreased 4% or $7.1 billion during 2023 compared to 2022 due to lower net sales of iPhone and Mac, partially oﬀset by higher net sales of Services.\\n\\nEurope\\n\\nEurope net sales decreased 1% or $824 million during 2023 compared to 2022. The weakness in foreign currencies relative to the U.S. dollar accounted for more than the entire year-over-year decrease in Europe net sales, which consisted primarily of lower net sales of Mac and Wearables, Home and Accessories, partially oﬀset by higher net sales of iPhone and Services.\\n\\nGreater China', metadata={'source': '../resources/filings/AAPL/AAPL-2023-10k.pdf'})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test that the retirever works\n",
    "retrieved_docs = parent_doc_retriever.invoke(\"apples's revenue 2023\")\n",
    "retrieved_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the same but use our new retriever\n",
    "parent_doc_qa = RetrievalQA.from_chain_type(\n",
    "    llm=vllm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=parent_doc_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": get_prompt()},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Like before let's evaluate the generation metrics first\n",
    "\n",
    "Note: it is often practical to not calculate all the metrics at once for rate limiting reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c2619cd61748f6b53639c084a80599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_metrics = [\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "]\n",
    "\n",
    "gen_parent_doc_test = evaluate_chain(parent_doc_qa, testset, \"generation_parent_doc\", gen_metrics, vllm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.834437</td>\n",
       "      <td>0.761404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.371819</td>\n",
       "      <td>0.329845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.980172</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       answer_relevancy  faithfulness\n",
       "count         19.000000     19.000000\n",
       "mean           0.834437      0.761404\n",
       "std            0.371819      0.329845\n",
       "min            0.000000      0.000000\n",
       "25%            0.980172      0.583333\n",
       "50%            1.000000      1.000000\n",
       "75%            1.000000      1.000000\n",
       "max            1.000000      1.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_parent_doc_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the same for the retrieval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c70c4c24984b12b71d1233b47e5683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n"
     ]
    }
   ],
   "source": [
    "ret_metrics = [\n",
    "    context_precision,\n",
    "    context_recall\n",
    "]\n",
    "\n",
    "ret_parent_doc_test = evaluate_chain(parent_doc_qa, testset, \"retrieval_parent_doc\", ret_metrics, vllm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.960526</td>\n",
       "      <td>0.701754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.098999</td>\n",
       "      <td>0.425205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       context_precision  context_recall\n",
       "count          19.000000       19.000000\n",
       "mean            0.960526        0.701754\n",
       "std             0.098999        0.425205\n",
       "min             0.638889        0.000000\n",
       "25%             1.000000        0.416667\n",
       "50%             1.000000        1.000000\n",
       "75%             1.000000        1.000000\n",
       "max             1.000000        1.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_parent_doc_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "In this case, we observe that the increased context provided by the parent document retriever had slightly negative effect on the generation metrics potentially reducing answer clarity via increased information. And a positive effect on retrieval metrics, especially context precision, indicating that the smaller chunks for query comparison helped order the relevant context but it appears that wasn't a limiting factor from the base case for this test. More conclusive testing would be needed to draw more authoritative conclusions but this example show us how to compare option in order to find the highest priority strategies for a given application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "\n",
    "In this notebook we covered:\n",
    "- why it's important to have an evaluation framework\n",
    "- the basic theory of RAGAS\n",
    "- how to calculate and generate faithfulness, answer_relevancy, context_precision, and context_recall\n",
    "- code to evaluate two different RAG chains to monitor how using a different retrieval strategy effects performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps: end-to-end evaluation\n",
    "\n",
    "As your pipeline matures and human labeled ground truth data is created the following metrics can be added for increased rigor. These additional metrics can be implemented similarly as the ones showcased above.\n",
    "\n",
    "\n",
    "## Answer correctness\n",
    "\n",
    "A weighted average of semantic and factual similarity where weights can be passed as a parameter.\n",
    "\n",
    "## Answer semantic similarity\n",
    "\n",
    "Measure distance between ground truth and the generated answer.\n",
    "\n",
    "#### Example process:\n",
    "- vectorize the ground truth answer and the generated answer\n",
    "- compute the cosine similarity.\n",
    "\n",
    "## Answer factual similarity\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "F1\\ Score = \\frac{TP}{TP + 0.5(FP + FN)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "TP (True Positive): Facts or statements that are present in both the ground truth and the generated answer.\n",
    "\n",
    "FP (False Positive): Facts or statements that are present in the generated answer but not in the ground truth.\n",
    "\n",
    "FN (False Negative): Facts or statements that are present in the ground truth but not in the generated answer.\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "data:\n",
    "> Ground truth: Einstein was born in 1879 in Germany.\n",
    "> Generated Answer: Einstein was born in Spain in 1879.\n",
    "\n",
    "TP: [Einstein was born in 1879]\n",
    "\n",
    "FP: [Einstein was born in Spain]\n",
    "\n",
    "FN: [Einstein was born in Germany]\n",
    "\n",
    "F1 = (1 / 1 + 0.5(1 + 1)) = 1/2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
