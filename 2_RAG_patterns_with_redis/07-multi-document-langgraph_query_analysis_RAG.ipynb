{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425fb020-e864-40ce-a31f-8da40c73d14b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <div><img src=\"../assets/redis_logo.svg\" style=\"width: 130px\"> </div>\n",
    "    <div style=\"display: inline-block; text-align: center; margin-bottom: 10px;\">\n",
    "        <span style=\"font-size: 36px;\"><b>Multi-document RAG based on LangGraph with Query Understanding and Redis Retrieval Agents</b></span>\n",
    "        <br />\n",
    "    </div>\n",
    "    <br />\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00b2501be3b1a0",
   "metadata": {},
   "source": [
    "In [Notebook 06](06-multi-document-langgraph_react_agentic_RAG.ipynb), we explored the **ReAct agent** approach to Retrieval-Augmented Generation (RAG). While it may be suitable for planning in uncertain situations, such as robotics where problem parameters are unknown, it may not be ideal for search applications where tasks and steps are well-defined.\n",
    "\n",
    "In this notebook, we will pivot to a different approach and use **LangGraph** to construct a more deterministic agent that performs the key steps in the search process to answer a user's question. \n",
    "\n",
    "### Key Steps in the Search Process\n",
    "\n",
    "1. **Understanding the Query**: Every search begins with a user query. The primary challenge is understanding the user's intent, which may require personalized and contextual comprehension.\n",
    "2. **Query Expansion**: This involves refining the query, translating it into the target query language, and mapping it into a vector space using an embedding model.\n",
    "3. **Retrieving Relevant Documents**: Once the query is represented in the vector space, the next step is to fetch the most relevant documents.\n",
    "4. **Reranking Documents**: After retrieving documents, reranking occurs through semantic reranking, business logic adjustments (e.g., ad injection, reordering based on user's context such as location or session information), or even LLM-based reranking.\n",
    "\n",
    "### Approach for This Notebook\n",
    "\n",
    "We will cover each of these steps in detail, demonstrating how to construct an agent using LangGraph to execute them effectively. This agent will be designed to understand user queries, perform query expansion, retrieve relevant documents, and rerank the results to provide accurate and contextually relevant answers to the user's questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468c64492a20099",
   "metadata": {},
   "source": [
    "![Graph](query_understanding_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f1e55f5310c1b",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7fdd37f0036d45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T13:08:13.029186Z",
     "start_time": "2024-06-21T13:08:11.834495Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4958a8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T13:09:52.052197Z",
     "start_time": "2024-06-21T13:09:52.039803Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# load env vars from .env file\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2270bb-c73a-4862-87d3-57a5b5308b30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== ENVIRONMENT VARIABLES ==========\n",
      "Current Directory=/home/ec2-user/SageMaker/financial-rag-workshop/2_RAG_patterns_with_redis\n",
      "Parent Directory=/home/ec2-user/SageMaker/financial-rag-workshop\n",
      "System path=['/home/ec2-user/SageMaker/financial-rag-workshop/helpers', '/home/ec2-user/anaconda3/envs/python3/lib/python310.zip', '/home/ec2-user/anaconda3/envs/python3/lib/python3.10', '/home/ec2-user/anaconda3/envs/python3/lib/python3.10/lib-dynload', '', '/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages']\n",
      "---------------------------------\n",
      "LLM Engine: ollama\n",
      "LOCAL_VLLM_MODEL: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "LOCAL_OLLAMA_MODEL: llama3:8b\n",
      "VLLM_URL: http://localhost:8000/v1\n",
      "---------------------------------\n",
      "NLTK_DATA=\n",
      "---------------------------------\n",
      "Redis URL: redis://localhost:6379\n"
     ]
    }
   ],
   "source": [
    "# set working directory\n",
    "dir_path = os.getcwd()\n",
    "parent_directory = os.path.dirname(dir_path)\n",
    "sys.path.insert(0, f'{parent_directory}/helpers')\n",
    "os.environ[\"ROOT_DIR\"] = parent_directory\n",
    "\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "\n",
    "print(\"========== ENVIRONMENT VARIABLES ==========\")\n",
    "print(f\"Current Directory={dir_path}\")\n",
    "print(f\"Parent Directory={parent_directory}\")\n",
    "print(f\"System path={sys.path}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f'LLM Engine: {os.getenv(\"LOCAL_LLM_ENGINE\")}')\n",
    "print(f'LOCAL_VLLM_MODEL: {os.getenv(\"LOCAL_VLLM_MODEL\")}')\n",
    "print(f'LOCAL_OLLAMA_MODEL: {os.getenv(\"LOCAL_OLLAMA_MODEL\")}')\n",
    "print(f'VLLM_URL: {os.getenv(\"VLLM_URL\")}')\n",
    "print(\"---------------------------------\")\n",
    "print(f\"NLTK_DATA={os.getenv('NLTK_DATA')}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Redis URL: {REDIS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c22327a44dd1c4d",
   "metadata": {},
   "source": [
    "### Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe27d19af62e7bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T20:35:04.824902Z",
     "start_time": "2024-06-18T20:35:02.488901Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -r $ROOT_DIR/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc6b74f5a93fb61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T20:35:14.138817Z",
     "start_time": "2024-06-18T20:35:10.598980Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils as utils\n",
    "from ingestion import get_sec_data, redis_bulk_upload\n",
    "from custom_ners import get_redis_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba8eaf10e92f692",
   "metadata": {},
   "source": [
    "### Load embedding model\n",
    "We are using `SentenceTransformerEmbeddings` in this demo and here we specify the cache folder for loading model weights. If you already downloaded the models in a local file system, set this folder here, otherwise the library tries to download the models in this folder if is not locally available.\n",
    "\n",
    "In particular, this models will be downloaded if not present in the cache folder:\n",
    "\n",
    "`models/models--sentence-transformers--all-MiniLM-L6-v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c405bdf9381db835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T20:35:20.082741Z",
     "start_time": "2024-06-18T20:35:20.080118Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the local downloaded sentence transformer models folder\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{parent_directory}/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11190e3ee0b4ab1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T20:35:21.988151Z",
     "start_time": "2024-06-18T20:35:20.633366Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache_folder=os.getenv(\"TRANSFORMERS_CACHE\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd69c8360baaa0",
   "metadata": {},
   "source": [
    "### Build your Redis index \n",
    "Skip this section if you have already built your index in previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78bbc21dd49fb15f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T20:35:33.400040Z",
     "start_time": "2024-06-18T20:35:33.302434Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:52:59 redisvl.index.index INFO   Index already exists, overwriting.\n"
     ]
    }
   ],
   "source": [
    "from redisvl.index import SearchIndex\n",
    "from redisvl.schema import IndexSchema\n",
    "\n",
    "# Create a schema\n",
    "index_name = 'langchain'\n",
    "prefix = 'chunk'\n",
    "schema = IndexSchema.from_yaml('sec_index.yaml')\n",
    "\n",
    "# Create a search index from the schema and connect to redis\n",
    "index = SearchIndex(schema, redis_url=REDIS_URL)\n",
    "index.create(overwrite=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1ab8cbe7cdea875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T20:35:34.057139Z",
     "start_time": "2024-06-18T20:35:34.050783Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✅ Loaded doc info for  110 tickers...\n"
     ]
    }
   ],
   "source": [
    "# Skip if you have already done populated your index.\n",
    "sec_data = get_sec_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b87b9c94d496bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T20:36:27.486638Z",
     "start_time": "2024-06-18T20:35:35.288607Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 608 10K chunks for ticker=AAPL from AAPL-2022-10K.pdf\n",
      "✅ Loaded 561 10K chunks for ticker=AAPL from AAPL-2023-10K.pdf\n",
      "✅ Loaded 632 10K chunks for ticker=AAPL from AAPL-2021-10K.pdf\n",
      "✅ Loaded 149 earning_call chunks for ticker=AAPL from 2019-Apr-30-AAPL.txt\n",
      "✅ Loaded 149 earning_call chunks for ticker=AAPL from 2017-Jan-31-AAPL.txt\n",
      "✅ Loaded 163 earning_call chunks for ticker=AAPL from 2016-Apr-26-AAPL.txt\n",
      "✅ Loaded 158 earning_call chunks for ticker=AAPL from 2019-Oct-30-AAPL.txt\n",
      "✅ Loaded 146 earning_call chunks for ticker=AAPL from 2020-Jan-28-AAPL.txt\n",
      "✅ Loaded 146 earning_call chunks for ticker=AAPL from 2017-May-02-AAPL.txt\n",
      "✅ Loaded 136 earning_call chunks for ticker=AAPL from 2016-Jul-26-AAPL.txt\n",
      "✅ Loaded 160 earning_call chunks for ticker=AAPL from 2019-Jul-30-AAPL.txt\n",
      "✅ Loaded 145 earning_call chunks for ticker=AAPL from 2018-Nov-01-AAPL.txt\n",
      "✅ Loaded 151 earning_call chunks for ticker=AAPL from 2017-Aug-01-AAPL.txt\n",
      "✅ Loaded 123 earning_call chunks for ticker=AAPL from 2017-Nov-02-AAPL.txt\n",
      "✅ Loaded 143 earning_call chunks for ticker=AAPL from 2019-Jan-29-AAPL.txt\n",
      "✅ Loaded 151 earning_call chunks for ticker=AAPL from 2016-Oct-25-AAPL.txt\n",
      "✅ Loaded 157 earning_call chunks for ticker=AAPL from 2018-Feb-01-AAPL.txt\n",
      "✅ Loaded 167 earning_call chunks for ticker=AAPL from 2020-Jul-30-AAPL.txt\n",
      "✅ Loaded 164 earning_call chunks for ticker=AAPL from 2016-Jan-26-AAPL.txt\n",
      "✅ Loaded 130 earning_call chunks for ticker=AAPL from 2018-May-01-AAPL.txt\n",
      "✅ Loaded 155 earning_call chunks for ticker=AAPL from 2020-Apr-30-AAPL.txt\n",
      "✅ Loaded 142 earning_call chunks for ticker=AAPL from 2018-Jul-31-AAPL.txt\n",
      "✅ Loaded 736 10K chunks for ticker=AMZN from AMZN-2023-10K.pdf\n",
      "✅ Loaded 701 10K chunks for ticker=AMZN from AMZN-2022-10K.pdf\n",
      "✅ Loaded 721 10K chunks for ticker=AMZN from AMZN-2021-10K.pdf\n",
      "✅ Loaded 107 earning_call chunks for ticker=AMZN from 2019-Oct-24-AMZN.txt\n",
      "✅ Loaded 112 earning_call chunks for ticker=AMZN from 2018-Feb-01-AMZN.txt\n",
      "✅ Loaded 127 earning_call chunks for ticker=AMZN from 2019-Jan-31-AMZN.txt\n",
      "✅ Loaded 157 earning_call chunks for ticker=AMZN from 2016-Apr-28-AMZN.txt\n",
      "✅ Loaded 174 earning_call chunks for ticker=AMZN from 2016-Jan-28-AMZN.txt\n",
      "✅ Loaded 114 earning_call chunks for ticker=AMZN from 2020-Apr-30-AMZN.txt\n",
      "✅ Loaded 119 earning_call chunks for ticker=AMZN from 2017-Feb-02-AMZN.txt\n",
      "✅ Loaded 122 earning_call chunks for ticker=AMZN from 2019-Jul-25-AMZN.txt\n",
      "✅ Loaded 115 earning_call chunks for ticker=AMZN from 2020-Jul-30-AMZN.txt\n",
      "✅ Loaded 116 earning_call chunks for ticker=AMZN from 2020-Jan-30-AMZN.txt\n",
      "✅ Loaded 127 earning_call chunks for ticker=AMZN from 2017-Apr-27-AMZN.txt\n",
      "✅ Loaded 116 earning_call chunks for ticker=AMZN from 2017-Oct-26-AMZN.txt\n",
      "✅ Loaded 120 earning_call chunks for ticker=AMZN from 2019-Apr-25-AMZN.txt\n",
      "✅ Loaded 108 earning_call chunks for ticker=AMZN from 2018-Jul-26-AMZN.txt\n",
      "✅ Loaded 112 earning_call chunks for ticker=AMZN from 2018-Apr-26-AMZN.txt\n",
      "✅ Loaded 138 earning_call chunks for ticker=AMZN from 2016-Jul-28-AMZN.txt\n",
      "✅ Loaded 101 earning_call chunks for ticker=AMZN from 2017-Jul-27-AMZN.txt\n",
      "✅ Loaded 111 earning_call chunks for ticker=AMZN from 2016-Oct-27-AMZN.txt\n",
      "✅ Loaded 116 earning_call chunks for ticker=AMZN from 2018-Oct-25-AMZN.txt\n",
      "✅✅✅Loaded a total of 9106 chunks from 6 10Ks and 38 earning calls for 2 tickers.\n"
     ]
    }
   ],
   "source": [
    "redis_bulk_upload(sec_data, index, embeddings, tickers=['AAPL', 'AMZN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e4532",
   "metadata": {},
   "source": [
    "## Redis as a Langchain Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e50c9efe-4abe-42fa-b35a-05eeeede9ec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T20:36:27.618458Z",
     "start_time": "2024-06-18T20:36:27.487480Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Redis as LangChainRedis\n",
    "\n",
    "index_name = 'langchain'\n",
    "\n",
    "vec_schema, main_schema = utils.create_langchain_schemas_from_redis_schema('sec_index.yaml')\n",
    "\n",
    "# Create redis vector store using LangChain\n",
    "rds = LangChainRedis.from_existing_index(\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name,\n",
    "    schema=main_schema\n",
    ")\n",
    "\n",
    "redis_retriever = rds.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc76c0d6a83a135",
   "metadata": {},
   "source": [
    "Test if the Redis index is working and returning relevant document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39b95a9507cbf6a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T20:36:28.056754Z",
     "start_time": "2024-06-18T20:36:27.618990Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'chunk:AAPL-2021-10K.pdf-f2c63500-5229-40f2-bc65-0474ffc79d34', 'chunk_id': 'AAPL-2021-10K.pdf-f2c63500-5229-40f2-bc65-0474ffc79d34', 'source_doc': 'AAPL-2021-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}, page_content='2022 2023 2024 2025 2026 Thereafter\\n\\n$\\n\\nTotal\\n\\n$\\n\\nApple Inc. | 2021 Form 10-K | 48\\n\\n2019\\n\\n3,692 (3,857) 3,735 3,570\\n\\n4,551 2,165 984 405 51 28 8,184\\n\\nContingencies'),\n",
       " Document(metadata={'id': 'chunk:AAPL-2022-10K.pdf-4530885f-6c20-46e5-855b-ddfc6cbe7472', 'chunk_id': 'AAPL-2022-10K.pdf-4530885f-6c20-46e5-855b-ddfc6cbe7472', 'source_doc': 'AAPL-2022-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}, page_content='The Company announces new product, service and software offerings at various times during the year. Significant announcements during fiscal 2022 included the following:\\n\\nFirst Quarter 2022:\\n\\n•\\n\\nUpdated MacBook Pro 14” and MacBook Pro 16”, powered by the Apple M1 Pro or M1 Max chip; and Third generation of AirPods.\\n\\nSecond Quarter 2022:\\n\\n• • •')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rds.similarity_search(query=\"Apple in 2022\", k=2, distance_threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d2277-45b2-4ae8-a7d6-62b07fb4a002",
   "metadata": {},
   "source": [
    "## Agent components for the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56bbcffd-524d-475e-a045-492b641b2658",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Engine: ollama\n",
      "LOCAL_VLLM_MODEL: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "LOCAL_OLLAMA_MODEL: llama3:8b\n",
      "VLLM_URL: http://localhost:8000/v1\n"
     ]
    }
   ],
   "source": [
    "# Recall local LLM settings\n",
    "print(f'LLM Engine: {os.getenv(\"LOCAL_LLM_ENGINE\")}')\n",
    "print(f'LOCAL_VLLM_MODEL: {os.getenv(\"LOCAL_VLLM_MODEL\")}')\n",
    "print(f'LOCAL_OLLAMA_MODEL: {os.getenv(\"LOCAL_OLLAMA_MODEL\")}')\n",
    "print(f'VLLM_URL: {os.getenv(\"VLLM_URL\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5bf50-92e1-408b-ab21-81211805882c",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "\n",
    "The question answering step is arguably the simplest yet most important part of the workflow. At this point, we have retrieved relevant context from a source, and now we prompt the LLM to answer the user's question based on this information.\n",
    "\n",
    "**The key objective here is to ensure the answer remains truthful to the provided context, avoiding any hallucinations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae6b410735d14731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:00:46.214536Z",
     "start_time": "2024-06-17T19:00:34.218078Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def get_qna_chain():\n",
    "    # LLM\n",
    "    gen_llm = utils.get_chat_llm( \n",
    "        #local_llm_engine=os.getenv(\"LOCAL_LLM_ENGINE\"),\n",
    "        #vllm_url=os.getenv(\"VLLM_URL\"),\n",
    "        #vllm_model=os.getenv(\"LOCAL_VLLM_MODEL\"),\n",
    "        #ollama_model=os.getenv(\"LOCAL_OLLAMA_MODEL\"),\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # Prompt\n",
    "    gen_local_prompt = PromptTemplate(\n",
    "        template=\"\"\"You are an assistant for question-answering tasks over financial data. Use the following pieces of retrieved context\n",
    "        to answer the question. If you don't know the answer, just say that you don't know. Do not make anything up. Use three sentences\n",
    "        maximum and keep the answer concise.\n",
    "\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer:\"\"\",\n",
    "        input_variables=['context', 'question'])\n",
    "\n",
    "    # Chain\n",
    "    gen_rag_chain = gen_local_prompt | gen_llm | StrOutputParser()\n",
    "    return gen_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c993954f-3854-4937-a3ef-b8a3093b7299",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created VLLM ChatOpenAI using meta-llama/Meta-Llama-3-8B-Instruct served from http://localhost:8000/v1\n"
     ]
    }
   ],
   "source": [
    "# Make the chain!\n",
    "my_qna_chain = get_qna_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eec3886c-007a-4351-85f9-eb11e8e6c9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, Apple's deferred revenue as of September 24, 2022 was $12.4 billion.\n"
     ]
    }
   ],
   "source": [
    "# Test the QnA step\n",
    "\n",
    "question=\"What was Apple's deferred revenue in 2022?\"\n",
    "\n",
    "context=\"\"\"As of September 24, 2022 and September 25, 2021, \n",
    "            the Company had total deferred revenue of $12.4 \n",
    "            billion and $11.9 billion, respectively. As of \n",
    "            September 24, 2022, the Company expects 64% of \n",
    "            total deferred revenue to be realized in less \"\"\"\n",
    "\n",
    "response = my_qna_chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fecb700639afe1",
   "metadata": {},
   "source": [
    "### Query Analysis\n",
    "\n",
    "In this section, we will use our `llama3` LLM to analyze user questions and perform several tasks in a single call:\n",
    "\n",
    "- **Question Relevancy**: Determines whether a question is relevant to the domain of finance.\n",
    "- **Question Routing**: Identifies whether the question can be better answered by looking into `10k` reports or `earnings calls`.\n",
    "- **Question Intent**: Establishes whether the question seeks a numeric answer or an explanation. This distinction is crucial for downstream components to find or verify the right answer.\n",
    "  - An `explanation` is more textual and can be measured in terms of cosine similarity.\n",
    "  - A `numeric` answer can be verified by running a tool that performs mathematical calculations, such as a [PAL chain](https://api.python.langchain.com/en/latest/pal_chain/langchain_experimental.pal_chain.base.PALChain.html#).  \n",
    "  \n",
    "\n",
    "We will take advantage of LangChain's implementation of [structured output parsing](https://python.langchain.com/v0.2/docs/how_to/structured_output/) to handle extraction from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b8baedd-e862-4efc-8ba7-5ea6c2e2da28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define the Pydantic model for the output\n",
    "class QuestionAnalysis(BaseModel):\n",
    "    \"\"\"The analysis and classification of the user's question.\"\"\"\n",
    "\n",
    "    question_type: Literal['numeric', 'explain'] = Field(..., description=\"The type of answer expected for the given question: 'numeric' for numbers or 'explain' for explanations.\")\n",
    "    question_relevant: bool = Field(..., description=\"Whether or not the given question is relevant to the finance domain\")\n",
    "    new_question: Optional[str] = Field(None, description=\"The rewritten and refined question based on the detected question_class. Default to None if the question is not relevant.\")\n",
    "\n",
    "    # company_filter: Optional[str] = Field(None, description=\"The company name extracted from the question, if available.\")\n",
    "    # year_filter: Optional[int] = Field(None, description=\"The year extracted from the question, if available.\")\n",
    "    question_class: Optional[Literal['10K', 'earnings_call']] = Field(None, description=\"If the answer to the question can be found in '10K' filings or 'earnings call' datasets, or if it cannot be classified, default to None\")\n",
    "    note: Optional[str] = Field(None, description=\"Additional notes or explanations regarding the query analysis.\")\n",
    "\n",
    "\n",
    "def get_question_analyzer():\n",
    "    # LLM\n",
    "    question_analyzer_llm = utils.get_chat_llm(\n",
    "        # local_llm_engine=os.getenv(\"LOCAL_LLM_ENGINE\"),\n",
    "        # vllm_url=os.getenv(\"VLLM_URL\"),\n",
    "        # vllm_model=os.getenv(\"LOCAL_VLLM_MODEL\"),\n",
    "        # ollama_model=os.getenv(\"LOCAL_OLLAMA_MODEL\"),\n",
    "        temperature=0,\n",
    "        format=\"json\"\n",
    "    )\n",
    "\n",
    "    # Set up the parser\n",
    "    parser = PydanticOutputParser(pydantic_object=QuestionAnalysis)\n",
    "\n",
    "    # Define the prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are an expert in finance tasked with analyzing and classifying user questions to determine their relevance, expected answer type,\n",
    "                and other helpful information.\n",
    "\n",
    "                Return the query analysis as a pure JSON object without extra content. Use the following JSON format:\\n{format_instructions}\"\"\"\n",
    "            ),\n",
    "            (\"human\", \"Question: {question}\")\n",
    "        ]\n",
    "    ).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "    # Chain\n",
    "    return prompt | question_analyzer_llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95bf3894-78b4-43cb-8f0c-c9e7a669a66b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created VLLM ChatOpenAI using meta-llama/Meta-Llama-3-8B-Instruct served from http://localhost:8000/v1\n"
     ]
    }
   ],
   "source": [
    "my_question_analyzer = get_question_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0352a091-8727-4429-8fb4-079d30637286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnalysis(question_type='numeric', question_relevant=True, new_question='What is the revenue of Apple Inc. (AAPL) in 2022?', question_class='10K', note=\"The answer can be found in Apple's 2022 10-K filing or earnings call transcript.\")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_question_analyzer.invoke({\"question\": \"what is the aapl revenue in 2022?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4abf26d92fa5a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:01:08.132127Z",
     "start_time": "2024-06-17T19:00:57.763076Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnalysis(question_type='explain', question_relevant=True, new_question=\"What is the sentiment analysis of Tim Cook's tone in Apple's 2022 earnings calls?\", question_class='earnings_call', note=\"The question is relevant to finance as it involves analyzing the tone and sentiment of a public figure's statements during an earnings call, which can impact the stock market and investor sentiment.\")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_question_analyzer.invoke({\"question\": \"what was the mood of Tim Cook in the earning calls of 2022?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbb7f12ecd289a63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:01:19.464677Z",
     "start_time": "2024-06-17T19:01:08.133755Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnalysis(question_type='explain', question_relevant=False, new_question=None, question_class=None, note='The question appears to be a famous example of a sentence that is grammatically correct but semantically nonsensical, and is not relevant to the finance domain.')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_question_analyzer.invoke({\"question\": \"Why colorless green ideas are sleeping furiously?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e0b1a0bf5809f9",
   "metadata": {},
   "source": [
    "### Retrieval Grader\n",
    "This is a simple component that determines whether a retrieved document is relevant to user's question or not using a simple strategy. As we demonstrate below, this is not very effective. Firstly, if the documents are presented using a proper embedding model, then they are more likely to be semantically relevant to user's query than this simple strategy. Here we simply want to show this component as a placeholder. We recommend you to either replace this component with a fine-tuned LLM-based reranker or use a Learning To Rank (LTR) model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d636fcc416e5b53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:01:26.801105Z",
     "start_time": "2024-06-17T19:01:19.466131Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DocumentGrade(BaseModel):\n",
    "    \"\"\"The quality of the retrieved document relative to the user question.\"\"\"\n",
    "\n",
    "    relevant: bool = Field(..., description=\"A boolean value indicating whether or not the document is relevant to the user's query. 1 being yes, 0 being no.\")\n",
    "    reason: str = Field(..., description=\"A brief explanation for why the decision was made about the document relevancy\")\n",
    "\n",
    "\n",
    "def get_retrieval_grader():\n",
    "    # Chat LLM\n",
    "    retrieval_grader_llm = utils.get_chat_llm( \n",
    "        # local_llm_engine=os.getenv(\"LOCAL_LLM_ENGINE\"),\n",
    "        # vllm_url=os.getenv(\"VLLM_URL\"),\n",
    "        # vllm_model=os.getenv(\"LOCAL_VLLM_MODEL\"),\n",
    "        # ollama_model=os.getenv(\"LOCAL_OLLAMA_MODEL\"),\n",
    "        temperature=0,\n",
    "        format=\"json\"\n",
    "    )\n",
    "\n",
    "    # Set up a parser\n",
    "    parser = PydanticOutputParser(pydantic_object=DocumentGrade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are an expert in finance, business, and economics that is able to assess the\n",
    "            relevancy of a retrieved 10K or earnings call documents to a user question. In other words, your job is to judge\n",
    "            how likely the provided document is able to answer or speak-to the question.\n",
    "\n",
    "            Some guiding principles:\n",
    "            - If the document contains keywords related to the user question, grade it as relevant.\n",
    "            - It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "            - Give a binary relevancy score to indicate whether the document is relevant to the question as well as the reason for the decision.\n",
    "\n",
    "            Wrap the output in `json` tags\\n{format_instructions}\"\"\",\n",
    "            ),\n",
    "            (\"human\", \"Document: \\n {document} \\n\\n Question: {input}\"),\n",
    "        ]\n",
    "    ).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "    # Chain\n",
    "    return prompt | retrieval_grader_llm | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59f928a9-9261-4db7-8ceb-f58a193e3dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created VLLM ChatOpenAI using meta-llama/Meta-Llama-3-8B-Instruct served from http://localhost:8000/v1\n"
     ]
    }
   ],
   "source": [
    "my_retrieval_grader = get_retrieval_grader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee02ab94-401c-4a00-8227-41e39a1285c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year-over-year growth in selling, general and administrative expense in 2022 was driven primarily by increases in headcount- related expenses, advertising and professional services.\n",
      "\n",
      "Apple Inc. | 2022 Form 10-K | 23\n",
      "\n",
      "2020\n",
      "\n",
      "69,461 35,495 104,956\n",
      "\n",
      "31.5 % 66.0 % 38.2 %\n",
      "\n",
      "2020 18,752\n",
      "\n",
      "7 %\n",
      "\n",
      "19,916\n",
      "\n",
      "7 %\n",
      "\n",
      "38,668\n",
      "\n",
      "14 %\n",
      "\n",
      "Other Income/(Expense), Net\n",
      "\n",
      "Other income/(expense), net (“OI&E”) for 2022, 2021 and 2020 was as follows (dollars in millions):\n",
      "\n",
      "2022\n",
      "\n",
      "Change\n",
      "\n",
      "2021\n",
      "\n",
      "Change\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "question = \"apple revenue in 2022\"\n",
    "docs = redis_retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[0].page_content\n",
    "print(doc_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef4be48e-7fa9-439f-806a-665ad154655e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentGrade(relevant=False, reason=\"The document does not contain information about Apple's revenue in 2022. It only discusses the year-over-year growth in selling, general and administrative expense and other income/(expense), net.\")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_retrieval_grader.invoke({\"input\": question, \"document\": doc_txt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13b1ec0b06e274bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:01:29.288463Z",
     "start_time": "2024-06-17T19:01:26.803482Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentGrade(relevant=False, reason=\"The document does not contain any information about Apple's revenue in 2022. The document appears to be about Amazon's business model, products, and services, and does not mention Apple or its revenue.\")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_retrieval_grader.invoke({\"input\": question, \"document\": \"\"\"\n",
    "We serve consumers through our online and physical stores and focus on selection, price, and convenience. We design our\n",
    "stores to enable hundreds of millions of unique products to be sold by us and by third parties across dozens of product categories.\n",
    "Customers access our offerings through our websites, mobile apps, Alexa, devices, streaming, and physically visiting our stores. We\n",
    "also manufacture and sell electronic devices, including Kindle, Fire tablet, Fire TV , Echo, Ring, and other devices, and we develop\n",
    "and produce media content. We seek to offer our customers low prices, fast and free delivery, easy-to-use functionality, and timely\n",
    "customer service. In addition, we offer Amazon Prime, a membership program that includes unlimited free shipping on over 100\n",
    "million items, access to unlimited streaming of tens of thousands of movies and TV episodes, including Amazon Original content,\n",
    "and other benefits.\n",
    "\"\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1434635cf6709c5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:01:30.763736Z",
     "start_time": "2024-06-17T19:01:29.290169Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentGrade(relevant=False, reason=\"The provided document is Amazon's mission statement and does not contain information about the company's revenue in 2022. The document focuses on the company's business model, products, and services, but does not provide financial information.\")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_retrieval_grader.invoke({\"input\": \"Amazon's revenue in 2022\", \"document\": \"\"\"\n",
    "We serve consumers through our online and physical stores and focus on selection, price, and convenience. We design our\n",
    "stores to enable hundreds of millions of unique products to be sold by us and by third parties across dozens of product categories.\n",
    "Customers access our offerings through our websites, mobile apps, Alexa, devices, streaming, and physically visiting our stores. We\n",
    "also manufacture and sell electronic devices, including Kindle, Fire tablet, Fire TV , Echo, Ring, and other devices, and we develop\n",
    "and produce media content. We seek to offer our customers low prices, fast and free delivery, easy-to-use functionality, and timely\n",
    "customer service. In addition, we offer Amazon Prime, a membership program that includes unlimited free shipping on over 100\n",
    "million items, access to unlimited streaming of tens of thousands of movies and TV episodes, including Amazon Original content,\n",
    "and other benefits.\n",
    "\"\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829df8ffee98f5bf",
   "metadata": {},
   "source": [
    "As you can see this grader, grades this document as relevant. It seems to be relevant to Aamzon but there is no information around revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e5bf644e35239",
   "metadata": {},
   "source": [
    "### Query Construction and Filter Extraction\n",
    "To increase the precision of the retrieval and limit the search space, it's best to construct filters based on metadata.\n",
    "\n",
    "A simple vector representation often does not cut it.\n",
    "\n",
    "To demonstrate that consider these two queries:\n",
    "- `what was the performance of amzn in 2021?`\n",
    "- `what was the performance of amzn in 2022`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1c6edecf084ee00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:23:04.513112Z",
     "start_time": "2024-06-17T19:23:04.432327Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between<what was the performance of amzn in 2021?> and <what was the performance of amzn in 2022?> ---> 0.9115875851948418\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "q1= \"what was the performance of amzn in 2021?\"\n",
    "q2= \"what was the performance of amzn in 2022?\"\n",
    "q3= \"what was the performance of amzn in fiscal year 2022?\" \n",
    "\n",
    "em1 = np.array(embeddings.embed_query(q1))\n",
    "em2 = np.array(embeddings.embed_query(q2))\n",
    "em3 = np.array(embeddings.embed_query(q3))\n",
    "cosine = np.dot(em1,em2)/(np.linalg.norm(em1)*np.linalg.norm(em2))\n",
    "\n",
    "print(f\"Cosine Similarity between<{q1}> and <{q2}> --->\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26f392d442d8008d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:23:07.123591Z",
     "start_time": "2024-06-17T19:23:07.121041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between<what was the performance of amzn in 2022?> and <what was the performance of amzn in fiscal year 2022?> ---> 0.8379697614770356\n"
     ]
    }
   ],
   "source": [
    "cosine = np.dot(em2,em3)/(np.linalg.norm(em2)*np.linalg.norm(em3))\n",
    "print(f\"Cosine Similarity between<{q2}> and <{q3}> --->\", cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70244a316e5160ea",
   "metadata": {},
   "source": [
    "**The answer to these questions are wildly different.**\n",
    "\n",
    "But the high similarity between two queries will result in documents that might be from different years. However, if you apply a simple date NER and filter the document source year, you will eliminate that problem.\n",
    "\n",
    "> This problem also could appear in geospatial search where a longitude and latitude of two points could point to the same location but with different precision(e.g 12.12412414 is textually different from 12.1 but could be the same lat/lon). So again you can translate to a proper geo-filter (which Redis supports: see an example [here](https://redis.io/learn/howtos/solutions/geo/getting-started)).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843fb814b811882",
   "metadata": {},
   "source": [
    "We have a built a simple toy NER using spacy, and did a simple filter extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e266c6cdb624601e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T20:36:53.934540Z",
     "start_time": "2024-06-18T20:36:53.923813Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@ticker:{AMZN} | @exchange:{NASDAQ}'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_redis_filters(q):\n",
    "    filters = get_redis_filters(q)\n",
    "    return filters\n",
    "\n",
    "extract_redis_filters(\"what was the performance of amzn in 2021 in nasdaq?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b91bce77c915d3b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T20:36:55.778296Z",
     "start_time": "2024-06-18T20:36:55.770470Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@company_name:{APPLE INC}'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_redis_filters(\"what was the performance of Apple Inc in 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1a0d3b9f-47b0-4ab7-8b18-8bb730b277ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extract_redis_filters(\"What was Tim Cook's tone in the earnings call transcript from 2022?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2fc571ea60f6c",
   "metadata": {},
   "source": [
    "# Putting it all together: build RAG agent as a graph\n",
    "Now that we have all the components for our RAG logic we will connect them through a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f03f9e92f8596f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:23:22.539915Z",
     "start_time": "2024-06-17T19:23:22.534793Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Annotated, TypedDict, Union, Sequence, List\n",
    "\n",
    "\n",
    "# AgentState represents the contents shared between processing nodes in the graph\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    filters: str\n",
    "    question_relevant: bool\n",
    "    question_class: str\n",
    "    question_type: str\n",
    "    alternate_question: str\n",
    "    question_note: str\n",
    "    rewrite_num: int\n",
    "    generation: str\n",
    "    documents: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "993ba7ced09fdc22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:23:23.309828Z",
     "start_time": "2024-06-17T19:23:23.297705Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "MAX_RETRY_COUNT = 2\n",
    "MAX_TOKEN_LIMIT = 1000 #char\n",
    "TOP_DOC_LIMIT = 2\n",
    "\n",
    "\n",
    "### Edges\n",
    "def check_retrieval_relevancy(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question or not.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    retries = state[\"rewrite_num\"]\n",
    "\n",
    "    # Score each doc\n",
    "    relevant_docs = []\n",
    "    for doc in documents:\n",
    "        document_grade: DocumentGrade = my_retrieval_grader.invoke({\"input\": question, \"document\": doc})\n",
    "\n",
    "        if document_grade.relevant:\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            relevant_docs.append(doc)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "\n",
    "    if (len(documents) - len(relevant_docs)) > 2:\n",
    "        return \"generate\"\n",
    "    elif retries < MAX_RETRY_COUNT:\n",
    "        return \"rewrite\"\n",
    "    elif retries >= MAX_RETRY_COUNT:\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def check_question_relevancy(state) -> Literal[\"generate\", \"retrieve\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the asked question is relevant to our domain and if retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the question is relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    question_relevant = state[\"question_relevant\"]\n",
    "    if not question_relevant:\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        return \"retrieve\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def query_understanding(state):\n",
    "    \"\"\"\n",
    "    Analyzes the input query for better retrieval.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the result of query analysis\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n---QUERY Analysis---\")\n",
    "\n",
    "    def combine_filters(inferred_filters, doc_type_filter=\"10K\", filter_strategy=\"AND\"):\n",
    "        if inferred_filters is None:\n",
    "            return \"@doc_type:{\"+f\"{doc_type_filter}\"+\"}\"\n",
    "        else:\n",
    "            return \"@doc_type:{\"+f\"{doc_type_filter}\"+\"} \" + filter_strategy +f\" ({inferred_filters})\"\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    rewrite_num = state[\"rewrite_num\"]\n",
    "    if rewrite_num is None:\n",
    "        rewrite_num = 1\n",
    "    else:\n",
    "        rewrite_num = int(state[\"rewrite_num\"]) + 1\n",
    "\n",
    "    query_filters = extract_redis_filters(question)\n",
    "    question_analysis: QuestionAnalysis = my_question_analyzer.invoke({\"question\": question})\n",
    "    print(f\"---QUERY rewrite---question_analysis={question_analysis}\")\n",
    "\n",
    "    if question_analysis.question_class is not None:\n",
    "        print(f\"---Question is related to: {question_analysis.question_class}---\")\n",
    "        applied_filters = combine_filters(query_filters, doc_type_filter=question_analysis.question_class)\n",
    "    else:\n",
    "        applied_filters = combine_filters(query_filters)\n",
    "\n",
    "    alternate_question = question_analysis.new_question\n",
    "    if alternate_question is not None:\n",
    "        print(f\"---Alternate question is: {alternate_question}---\")\n",
    "\n",
    "    return {\n",
    "        \"filters\": applied_filters,\n",
    "        \"question_class\": question_analysis.question_class,\n",
    "        \"question_type\": question_analysis.question_type,\n",
    "        \"question_relevant\": question_analysis.question_relevant,\n",
    "        \"alternate_question\": alternate_question,\n",
    "        \"question_note\": question_analysis.note,\n",
    "        \"rewrite_num\": rewrite_num\n",
    "    }\n",
    "\n",
    "\n",
    "def redis_retriever(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from Redis.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"\\n---RETRIEVE FROM REDIS---\")\n",
    "    question = state[\"question\"]\n",
    "    alternate_question = state[\"alternate_question\"]\n",
    "    retries = state[\"rewrite_num\"]\n",
    "    filters = state[\"filters\"]\n",
    "\n",
    "    print(f\"---RETRIEVE FROM REDIS query={question} alternate_question={alternate_question} retries={retries}\")\n",
    "\n",
    "    if alternate_question is not None and retries > 1:\n",
    "        question = alternate_question\n",
    "\n",
    "    if filters is None:\n",
    "        documents = rds.similarity_search(query=question, k=4, distance_threshold=0.8)\n",
    "    else:\n",
    "        documents = rds.similarity_search(query=question, k=4, distance_threshold=0.8, filter=filters)\n",
    "\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n---GENERATE---\")\n",
    "    final_docs = state[\"documents\"]\n",
    "    final_question = state[\"alternate_question\"] or state[\"question\"] \n",
    "    question_relevant = state[\"question_relevant\"]\n",
    "    note = state[\"question_note\"]\n",
    "    # if state.get('note') is None:\n",
    "    #     note = state[\"question_note\"]\n",
    "\n",
    "    final_context = \"\"\n",
    "    if not question_relevant:\n",
    "        final_context = \"Your question does not seem to be relevant to finance. Please only ask questions that are relevant to financials of companies that are usually reported in 10K or earning calls.\"\n",
    "        if note is not None:\n",
    "            final_context = final_context + f\"\\n\\n{note}\"\n",
    "    elif question_relevant and final_docs is not None and len(final_docs) > 0:\n",
    "        final_context = str(\"\\n\".join(utils.format_docs(final_docs[:TOP_DOC_LIMIT])))[:MAX_TOKEN_LIMIT]\n",
    "\n",
    "    print(f\"\\nDEBUG:GENERATE === question={final_question}\")\n",
    "    print(f\"DEBUG:GENERATE === context={final_context}\")\n",
    "\n",
    "    # Run Gen LLM\n",
    "    generated_answer = my_qna_chain.invoke({\"context\": final_context, \"question\": final_question})\n",
    "\n",
    "    print(f\"DEBUG:GENERATE === generation={generated_answer}\")\n",
    "    return {\"messages\": [generated_answer], \"generation\": generated_answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2acd602638ce906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:23:25.109986Z",
     "start_time": "2024-06-17T19:23:24.164238Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"query_understanding_node\", query_understanding)\n",
    "workflow.add_node(\"redis_retriever_node\", redis_retriever)\n",
    "workflow.add_node(\"generate_node\", generate)\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"redis_retriever_node\",\n",
    "    check_retrieval_relevancy,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"rewrite\": \"query_understanding_node\",\n",
    "        \"generate\": \"generate_node\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"query_understanding_node\",\n",
    "    check_question_relevancy,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"retrieve\": \"redis_retriever_node\",\n",
    "        \"generate\": \"generate_node\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "workflow.add_edge(\"generate_node\", END)\n",
    "\n",
    "workflow.set_entry_point(\"query_understanding_node\")\n",
    "\n",
    "# Compile\n",
    "graphapp = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graphapp.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7823529303b98dfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:25:12.910129Z",
     "start_time": "2024-06-17T19:23:26.055153Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---QUERY Analysis---\n",
      "---QUERY rewrite---question_analysis=question_type='numeric' question_relevant=True new_question='What is the deferred revenue of Apple Inc. (AAPL) for the year 2022?' question_class='10K' note=\"The question is relevant to the finance domain and can be answered by referencing Apple's 2022 10-K filing.\"\n",
      "---Question is related to: 10K---\n",
      "---Alternate question is: What is the deferred revenue of Apple Inc. (AAPL) for the year 2022?---\n",
      "Output from node 'query_understanding_node':\n",
      "---\n",
      "{'filters': '@doc_type:{10K} AND (@ticker:{AAPL})', 'question_relevant': True, 'question_class': '10K', 'question_type': 'numeric', 'alternate_question': 'What is the deferred revenue of Apple Inc. (AAPL) for the year 2022?', 'question_note': \"The question is relevant to the finance domain and can be answered by referencing Apple's 2022 10-K filing.\", 'rewrite_num': 1}\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "---RETRIEVE FROM REDIS---\n",
      "---RETRIEVE FROM REDIS query=What was the deferred revenue of aapl in 2022? alternate_question=What is the deferred revenue of Apple Inc. (AAPL) for the year 2022? retries=1\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Output from node 'redis_retriever_node':\n",
      "---\n",
      "{'documents': [Document(metadata={'id': 'chunk:AAPL-2022-10K.pdf-d49928ba-ed1c-4c74-a991-4a0cdd677100', 'chunk_id': 'AAPL-2022-10K.pdf-d49928ba-ed1c-4c74-a991-4a0cdd677100', 'source_doc': 'AAPL-2022-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}, page_content='Includes $7.5 billion of revenue recognized in 2022 that was included in deferred revenue as of September 25, 2021, $6.7 billion of revenue recognized in 2021 that was included in deferred revenue as of September 26, 2020, and $5.0 billion of revenue recognized in 2020 that was included in deferred revenue as of September 28, 2019.'), Document(metadata={'id': 'chunk:AAPL-2021-10K.pdf-f1793499-d0cc-4f03-8ef1-3e52ed303929', 'chunk_id': 'AAPL-2021-10K.pdf-f1793499-d0cc-4f03-8ef1-3e52ed303929', 'source_doc': 'AAPL-2021-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}, page_content='Includes $6.7 billion of revenue recognized in 2021 that was included in deferred revenue as of September 26, 2020, $5.0 billion of revenue recognized in 2020 that was included in deferred revenue as of September 28, 2019, and $5.9 billion of revenue recognized in 2019 that was included in deferred revenue as of September 29, 2018.'), Document(metadata={'id': 'chunk:AAPL-2021-10K.pdf-acbac60f-6df7-466b-8ec3-458cb9b520c7', 'chunk_id': 'AAPL-2021-10K.pdf-acbac60f-6df7-466b-8ec3-458cb9b520c7', 'source_doc': 'AAPL-2021-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}, page_content='As of September 25, 2021 and September 26, 2020, the Company had total deferred revenue of $11.9 billion and $10.2 billion, respectively. As of September 25, 2021, the Company expects 64% of total deferred revenue to be realized in less than a year, 26% within one-to-two years, 8% within two-to-three years and 2% in greater than three years.\\n\\nDisaggregated Revenue\\n\\nNet sales disaggregated by significant products and services for 2021, 2020 and 2019 were as follows (in millions):\\n\\n2021\\n\\n2020'), Document(metadata={'id': 'chunk:AAPL-2023-10K.pdf-fa99912c-cee7-46b7-bd18-7e38bc09be3f', 'chunk_id': 'AAPL-2023-10K.pdf-fa99912c-cee7-46b7-bd18-7e38bc09be3f', 'source_doc': 'AAPL-2023-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}, page_content='As of September 30, 2023 and September 24, 2022, the Company had total deferred revenue of $12.1 billion and $12.4 billion, respectively. As of September 30, 2023, the Company expects 67% of total deferred revenue to be realized in less than a year, 25% within one-to-two years, 7% within two-to-three years and 1% in greater than three years.\\n\\nNote 3 – Earnings Per Share')]}\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "---QUERY Analysis---\n",
      "---QUERY rewrite---question_analysis=question_type='numeric' question_relevant=True new_question='What is the deferred revenue of Apple Inc. (AAPL) for the year 2022?' question_class='10K' note=\"The question is relevant to the finance domain and can be answered by referencing Apple's 2022 10-K filing.\"\n",
      "---Question is related to: 10K---\n",
      "---Alternate question is: What is the deferred revenue of Apple Inc. (AAPL) for the year 2022?---\n",
      "Output from node 'query_understanding_node':\n",
      "---\n",
      "{'filters': '@doc_type:{10K} AND (@ticker:{AAPL})', 'question_relevant': True, 'question_class': '10K', 'question_type': 'numeric', 'alternate_question': 'What is the deferred revenue of Apple Inc. (AAPL) for the year 2022?', 'question_note': \"The question is relevant to the finance domain and can be answered by referencing Apple's 2022 10-K filing.\", 'rewrite_num': 2}\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "---RETRIEVE FROM REDIS---\n",
      "---RETRIEVE FROM REDIS query=What was the deferred revenue of aapl in 2022? alternate_question=What is the deferred revenue of Apple Inc. (AAPL) for the year 2022? retries=2\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Output from node 'redis_retriever_node':\n",
      "---\n",
      "{'documents': [Document(metadata={'id': 'chunk:AAPL-2022-10K.pdf-fa116b44-b5ef-4e57-9e41-829b0ede6074', 'chunk_id': 'AAPL-2022-10K.pdf-fa116b44-b5ef-4e57-9e41-829b0ede6074', 'source_doc': 'AAPL-2022-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}, page_content='As of September 24, 2022 and September 25, 2021, the Company had total deferred revenue of $12.4 billion and $11.9 billion, respectively. As of September 24, 2022, the Company expects 64% of total deferred revenue to be realized in less than a year, 27% within one-to-two years, 7% within two-to-three years and 2% in greater than three years.\\n\\nApple Inc. | 2022 Form 10-K | 37\\n\\n2020\\n\\n137,781 28,622 23,724 30,620 53,768 274,515\\n\\nNote 3 – Financial Instruments'), Document(metadata={'id': 'chunk:AAPL-2023-10K.pdf-3034c36b-4125-41b6-a9bd-0841e72b746c', 'chunk_id': 'AAPL-2023-10K.pdf-3034c36b-4125-41b6-a9bd-0841e72b746c', 'source_doc': 'AAPL-2023-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}, page_content='Selling, general and administrative expense was relatively ﬂat in 2023 compared to 2022.\\n\\nApple Inc. | 2023 Form 10-K | 23\\n\\n2021\\n\\n105,126 47,710 152,836\\n\\n35.3 % 69.7 % 41.8 %\\n\\n2021\\n\\n21,914\\n\\n6 %\\n\\n21,973\\n\\n6 %\\n\\n43,887\\n\\n12 %\\n\\nProvision for Income Taxes\\n\\nProvision for income taxes, eﬀective tax rate and statutory federal income tax rate for 2023, 2022 and 2021 were as follows (dollars in millions):\\n\\n2023\\n\\n2022\\n\\nProvision for income taxes Eﬀective tax rate Statutory federal income tax rate\\n\\n$'), Document(metadata={'id': 'chunk:AAPL-2022-10K.pdf-f69d8921-af57-4602-b340-2b37c558bf46', 'chunk_id': 'AAPL-2022-10K.pdf-f69d8921-af57-4602-b340-2b37c558bf46', 'source_doc': 'AAPL-2022-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}, page_content='The year-over-year growth in selling, general and administrative expense in 2022 was driven primarily by increases in headcount- related expenses, advertising and professional services.\\n\\nApple Inc. | 2022 Form 10-K | 23\\n\\n2020\\n\\n69,461 35,495 104,956\\n\\n31.5 % 66.0 % 38.2 %\\n\\n2020 18,752\\n\\n7 %\\n\\n19,916\\n\\n7 %\\n\\n38,668\\n\\n14 %\\n\\nOther Income/(Expense), Net\\n\\nOther income/(expense), net (“OI&E”) for 2022, 2021 and 2020 was as follows (dollars in millions):\\n\\n2022\\n\\nChange\\n\\n2021\\n\\nChange'), Document(metadata={'id': 'chunk:AAPL-2023-10K.pdf-8cdbb389-068e-4ede-b66b-00bff4a83975', 'chunk_id': 'AAPL-2023-10K.pdf-8cdbb389-068e-4ede-b66b-00bff4a83975', 'source_doc': 'AAPL-2023-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}, page_content='Apple Inc. | 2023 Form 10-K | 34\\n\\nNet sales disaggregated by signiﬁcant products and services for 2023, 2022 and 2021 were as follows (in millions):\\n\\n2023\\n\\n2022\\n\\n2021\\n\\niPhone (1) Mac iPad Wearables, Home and Accessories Services\\n\\n(1)\\n\\n(1)\\n\\n(2)\\n\\nTotal net sales\\n\\n(1)\\n\\n$\\n\\n$\\n\\n200,583 $ 29,357 28,300 39,845 85,200 383,285 $\\n\\n205,489 $ 40,177 29,292 41,241 78,129 394,328 $\\n\\n191,973 35,190 31,862 38,367 68,425 365,817')]}\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "---GENERATE---\n",
      "\n",
      "DEBUG:GENERATE === question=What is the deferred revenue of Apple Inc. (AAPL) for the year 2022?\n",
      "DEBUG:GENERATE === context=As of September 24, 2022 and September 25, 2021, the Company had total deferred revenue of $12.4 billion and $11.9 billion, respectively. As of September 24, 2022, the Company expects 64% of total deferred revenue to be realized in less than a year, 27% within one-to-two years, 7% within two-to-three years and 2% in greater than three years.  Apple Inc. | 2022 Form 10-K | 37  2020  137,781 28,622 23,724 30,620 53,768 274,515  Note 3 – Financial Instruments\n",
      "Selling, general and administrative expense was relatively ﬂat in 2023 compared to 2022.  Apple Inc. | 2023 Form 10-K | 23  2021  105,126 47,710 152,836  35.3 % 69.7 % 41.8 %  2021  21,914  6 %  21,973  6 %  43,887  12 %  Provision for Income Taxes  Provision for income taxes, eﬀective tax rate and statutory federal income tax rate for 2023, 2022 and 2021 were as follows (dollars in millions):  2023  2022  Provision for income taxes Eﬀective tax rate Statutory federal income tax rate  $\n",
      "DEBUG:GENERATE === generation=Based on the provided context, the deferred revenue of Apple Inc. (AAPL) as of September 24, 2022 is $12.4 billion.\n",
      "Output from node 'generate_node':\n",
      "---\n",
      "{'generation': 'Based on the provided context, the deferred revenue of Apple Inc. (AAPL) as of September 24, 2022 is $12.4 billion.'}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"question\": \"What was the deferred revenue of aapl in 2022?\",\n",
    "}\n",
    "\n",
    "for output in graphapp.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34f4fb3c09949739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:28:13.974034Z",
     "start_time": "2024-06-17T19:27:51.925269Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---QUERY Analysis---\n",
      "---QUERY rewrite---question_analysis=question_type='explain' question_relevant=False new_question=None question_class=None note='The question appears to be a famous example of a sentence that is grammatically correct but semantically nonsensical, and is not relevant to the finance domain.'\n",
      "Output from node 'query_understanding_node':\n",
      "---\n",
      "{'filters': '@doc_type:{10K}', 'question_relevant': False, 'question_class': None, 'question_type': 'explain', 'alternate_question': None, 'question_note': 'The question appears to be a famous example of a sentence that is grammatically correct but semantically nonsensical, and is not relevant to the finance domain.', 'rewrite_num': 1}\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "---GENERATE---\n",
      "\n",
      "DEBUG:GENERATE === question=Why colorless green ideas are furiously sleeping?\n",
      "DEBUG:GENERATE === context=Your question does not seem to be relevant to finance. Please only ask questions that are relevant to financials of companies that are usually reported in 10K or earning calls.\n",
      "\n",
      "The question appears to be a famous example of a sentence that is grammatically correct but semantically nonsensical, and is not relevant to the finance domain.\n",
      "DEBUG:GENERATE === generation=I don't know the answer to this question as it appears to be a linguistic example rather than a financial query. The context provided indicates that the question is not relevant to financial data typically reported in 10K or earnings calls.\n",
      "Output from node 'generate_node':\n",
      "---\n",
      "{'generation': \"I don't know the answer to this question as it appears to be a linguistic example rather than a financial query. The context provided indicates that the question is not relevant to financial data typically reported in 10K or earnings calls.\"}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs2 = {\n",
    "    \"question\": \"Why colorless green ideas are furiously sleeping?\"\n",
    "}\n",
    "\n",
    "for output2 in graphapp.stream(inputs2):\n",
    "    for key2, value2 in output2.items():\n",
    "        print(f\"Output from node '{key2}':\")\n",
    "        print(\"---\")\n",
    "        print(value2)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d85443ffb20f9",
   "metadata": {},
   "source": [
    "![furiously_sleeping](chomsky_sleeping.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa74afdcc4245394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:28:58.235446Z",
     "start_time": "2024-06-17T19:28:22.589995Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---QUERY Analysis---\n",
      "---QUERY rewrite---question_analysis=question_type='explain' question_relevant=True new_question=\"What was the summary of Apple's financial data presented in the July 2020 earnings call?\" question_class='earnings_call' note='The question is relevant to the finance domain and can be answered by referencing the July 2020 earnings call of Apple Inc.'\n",
      "---Question is related to: earnings_call---\n",
      "---Alternate question is: What was the summary of Apple's financial data presented in the July 2020 earnings call?---\n",
      "Output from node 'query_understanding_node':\n",
      "---\n",
      "{'filters': '@doc_type:{earnings_call} AND (@ticker:{AAPL})', 'question_relevant': True, 'question_class': 'earnings_call', 'question_type': 'explain', 'alternate_question': \"What was the summary of Apple's financial data presented in the July 2020 earnings call?\", 'question_note': 'The question is relevant to the finance domain and can be answered by referencing the July 2020 earnings call of Apple Inc.', 'rewrite_num': 1}\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "---RETRIEVE FROM REDIS---\n",
      "---RETRIEVE FROM REDIS query=What was the summary of aapl financial data presented in july 2020 earning calls? alternate_question=What was the summary of Apple's financial data presented in the July 2020 earnings call? retries=1\n",
      "Output from node 'redis_retriever_node':\n",
      "---\n",
      "{'documents': []}\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "---QUERY Analysis---\n",
      "---QUERY rewrite---question_analysis=question_type='explain' question_relevant=True new_question=\"What was the summary of Apple's financial data presented in the July 2020 earnings call?\" question_class='earnings_call' note='The question is relevant to the finance domain and can be answered by referencing the July 2020 earnings call of Apple Inc.'\n",
      "---Question is related to: earnings_call---\n",
      "---Alternate question is: What was the summary of Apple's financial data presented in the July 2020 earnings call?---\n",
      "Output from node 'query_understanding_node':\n",
      "---\n",
      "{'filters': '@doc_type:{earnings_call} AND (@ticker:{AAPL})', 'question_relevant': True, 'question_class': 'earnings_call', 'question_type': 'explain', 'alternate_question': \"What was the summary of Apple's financial data presented in the July 2020 earnings call?\", 'question_note': 'The question is relevant to the finance domain and can be answered by referencing the July 2020 earnings call of Apple Inc.', 'rewrite_num': 2}\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "---RETRIEVE FROM REDIS---\n",
      "---RETRIEVE FROM REDIS query=What was the summary of aapl financial data presented in july 2020 earning calls? alternate_question=What was the summary of Apple's financial data presented in the July 2020 earnings call? retries=2\n",
      "Output from node 'redis_retriever_node':\n",
      "---\n",
      "{'documents': []}\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "---GENERATE---\n",
      "\n",
      "DEBUG:GENERATE === question=What was the summary of Apple's financial data presented in the July 2020 earnings call?\n",
      "DEBUG:GENERATE === context=\n",
      "DEBUG:GENERATE === generation=I don't know.\n",
      "Output from node 'generate_node':\n",
      "---\n",
      "{'generation': \"I don't know.\"}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs3 = {\n",
    "    \"question\": \"What was the summary of aapl financial data presented in july 2020 earning calls?\"\n",
    "}\n",
    "\n",
    "for output3 in graphapp.stream(inputs3):\n",
    "    for key3, value3 in output3.items():\n",
    "        print(f\"Output from node '{key3}':\")\n",
    "        print(\"---\")\n",
    "        print(value3)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de001bd05628fc",
   "metadata": {},
   "source": [
    "While this generated reasonable output, if you look closely, the retrieved documents are not from July 2020. We need to implement some kind of NER process for dates (or use an LLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59f853531650fd",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "Implement\n",
    "- Multiple index routing scenarios where based on the detected `question_class` we run retrieval queries on different Redis indices.\n",
    "- Implement a Caching strategy using powerful and superfast Redis features, including `SemanticCache`.\n",
    "- Replace the `RelevancyGrader` by a proper reranking model and node.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18e5f6c0f3b7fe",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The problem of search is still an old problem, and it is about representation of your information, storing it in your representation format and getting back what you search for via a retrieval method. In the new `RAG` paradigm, information representation is the vector representation. So the more you can capture the nuances of your data using better and better embedding models, the better the `Retrieval`. However, as demonstrated above, we still need to take advantage of our metadata to increase the accuracy and `Augment` the query and also augment the context that we present to the `Generation` component. And most recently, LLMs do a good job of generating proper answers in the target natural languages. The quality of LLMs and their awareness of the domain language increase the quality of generated response.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
