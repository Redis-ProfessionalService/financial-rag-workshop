{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <div><img src=\"../assets/redis_logo.svg\" style=\"width: 130px\"> </div>\n",
    "    <div style=\"display: inline-block; text-align: center; margin-bottom: 10px;\">\n",
    "        <span style=\"font-size: 36px;\"><b>Evaluation with RAGAS</b></span>\n",
    "        <br />\n",
    "    </div>\n",
    "    <br />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG\n",
    "\n",
    "The extent to which you can **evaluate** your system is the extent to which you can **improve** your system. Before going to prod, it is in your best interest to establish a framework for quickly and effectively understanding the quality of your RAG application. In this notebook, we will use the RAGAS framework, as proposed by [this paper](https://arxiv.org/pdf/2309.15217), to evaluate our RAG application.\n",
    "\n",
    "Before we dive into the theory though, let's setup the necessary environment and basic RAG application for evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv (from -r requirements.txt (line 2))\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting tiktoken (from -r requirements.txt (line 3))\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting langchain (from -r requirements.txt (line 4))\n",
      "  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community (from -r requirements.txt (line 5))\n",
      "  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting sentence-transformers (from -r requirements.txt (line 7))\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
      "Collecting pdf2image (from -r requirements.txt (line 9))\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting spacy (from -r requirements.txt (line 10))\n",
      "  Downloading spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting langgraph (from -r requirements.txt (line 11))\n",
      "  Downloading langgraph-0.1.19-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting redis (from -r requirements.txt (line 12))\n",
      "  Downloading redis-5.0.8-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting redisvl (from -r requirements.txt (line 13))\n",
      "  Downloading redisvl-0.3.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting langchain-openai (from -r requirements.txt (line 14))\n",
      "  Downloading langchain_openai-0.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting openai (from -r requirements.txt (line 15))\n",
      "  Downloading openai-1.39.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting ragas (from -r requirements.txt (line 17))\n",
      "  Downloading ragas-0.1.13-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting vllm (from -r requirements.txt (line 18))\n",
      "  Downloading vllm-0.5.4-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting llama-index (from -r requirements.txt (line 21))\n",
      "  Downloading llama_index-0.10.61-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-embeddings-huggingface (from -r requirements.txt (line 22))\n",
      "  Downloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl.metadata (769 bytes)\n",
      "Collecting llama-index-llms-ollama (from -r requirements.txt (line 23))\n",
      "  Downloading llama_index_llms_ollama-0.2.2-py3-none-any.whl.metadata (668 bytes)\n",
      "Collecting llama-index-embeddings-instructor (from -r requirements.txt (line 24))\n",
      "  Downloading llama_index_embeddings_instructor-0.1.3-py3-none-any.whl.metadata (810 bytes)\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.15.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 3)) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (4.0.3)\n",
      "Collecting langchain-core<0.3.0,>=0.2.27 (from langchain->-r requirements.txt (line 4))\n",
      "  Downloading langchain_core-0.2.28-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->-r requirements.txt (line 4))\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->-r requirements.txt (line 4))\n",
      "  Downloading langsmith-0.1.98-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (1.22.4)\n",
      "Collecting pydantic<3,>=1 (from langchain->-r requirements.txt (line 4))\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (8.3.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community->-r requirements.txt (line 5))\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: chardet in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (5.2.0)\n",
      "Collecting filetype (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading lxml-5.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (4.12.3)\n",
      "Collecting emoji (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting python-iso639 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rapidfuzz (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading rapidfuzz-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting backoff (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (4.12.1)\n",
      "Collecting unstructured-client (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_client-0.25.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (5.9.8)\n",
      "Collecting onnx (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting pdfminer.six (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pikepdf (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pikepdf-9.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Collecting pillow-heif (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pillow_heif-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
      "Collecting pypdf (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting pytesseract (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting google-cloud-vision (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting effdet (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting unstructured-inference==0.7.36 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.36-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting layoutparser (from unstructured-inference==0.7.36->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting python-multipart (from unstructured-inference==0.7.36->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting huggingface-hub (from unstructured-inference==0.7.36->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting opencv-python!=4.7.0.68 (from unstructured-inference==0.7.36->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "INFO: pip is looking at multiple versions of unstructured-inference to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.15.0-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading unstructured-0.14.10-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading unstructured-0.14.9-py3-none-any.whl.metadata (28 kB)\n",
      "  Downloading unstructured-0.14.8-py3-none-any.whl.metadata (28 kB)\n",
      "  Downloading unstructured-0.14.7-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting unstructured-inference==0.7.35 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.35-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.14.6-py3-none-any.whl.metadata (28 kB)\n",
      "  Downloading unstructured-0.14.5-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting unstructured-inference==0.7.33 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.33-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: pip is still looking at multiple versions of unstructured-inference to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.14.4-py3-none-any.whl.metadata (28 kB)\n",
      "  Downloading unstructured-0.14.3-py3-none-any.whl.metadata (31 kB)\n",
      "  Downloading unstructured-0.14.2-py3-none-any.whl.metadata (31 kB)\n",
      "  Downloading unstructured-0.14.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting unstructured-inference==0.7.31 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.31-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.13.7-py3-none-any.whl.metadata (30 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading unstructured-0.13.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting unstructured-inference==0.7.29 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.29-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.13.5-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading unstructured-0.13.4-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting unstructured-inference==0.7.28 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.28-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.13.3-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting unstructured-inference==0.7.27 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.27-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.13.2-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting unstructured-client<=0.18.0 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_client-0.18.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting unstructured-inference==0.7.25 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.25-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting onnxruntime<1.16 (from unstructured-inference==0.7.25->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting transformers>=4.25.1 (from unstructured-inference==0.7.25->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m788.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting coloredlogs (from onnxruntime<1.16->unstructured-inference==0.7.25->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime<1.16->unstructured-inference==0.7.25->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from onnxruntime<1.16->unstructured-inference==0.7.25->unstructured[pdf]->-r requirements.txt (line 6)) (21.3)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from onnxruntime<1.16->unstructured-inference==0.7.25->unstructured[pdf]->-r requirements.txt (line 6)) (4.25.3)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from onnxruntime<1.16->unstructured-inference==0.7.25->unstructured[pdf]->-r requirements.txt (line 6)) (1.12)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (10.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 8)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 8)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 8)) (2024.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 10)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 10)) (70.0.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->-r requirements.txt (line 10))\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai->-r requirements.txt (line 15)) (4.4.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai->-r requirements.txt (line 15))\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai->-r requirements.txt (line 15)) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai->-r requirements.txt (line 15)) (1.3.1)\n",
      "Collecting datasets (from ragas->-r requirements.txt (line 17))\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pysbd>=0.3.4 (from ragas->-r requirements.txt (line 17))\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: nest-asyncio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ragas->-r requirements.txt (line 17)) (1.6.0)\n",
      "Collecting appdirs (from ragas->-r requirements.txt (line 17))\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting cmake>=3.21 (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading cmake-3.30.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting ninja (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting sentencepiece (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from vllm->-r requirements.txt (line 18)) (9.0.0)\n",
      "Collecting tokenizers>=0.19.1 (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting fastapi (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading fastapi-0.112.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn[standard] (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading uvicorn-0.30.5-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: prometheus-client>=0.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from vllm->-r requirements.txt (line 18)) (0.20.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting lm-format-enforcer==0.10.3 (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading lm_format_enforcer-0.10.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting outlines<0.1,>=0.0.43 (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: filelock>=3.10.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from vllm->-r requirements.txt (line 18)) (3.14.0)\n",
      "Requirement already satisfied: pyzmq in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from vllm->-r requirements.txt (line 18)) (26.0.3)\n",
      "Collecting ray>=2.9 (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading ray-2.34.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting nvidia-ml-py (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading nvidia_ml_py-12.555.43-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting torchvision==0.19 (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting xformers==0.0.27.post2 (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting vllm-flash-attn==2.6.1 (from vllm->-r requirements.txt (line 18))\n",
      "  Downloading vllm_flash_attn-2.6.1-cp310-cp310-manylinux1_x86_64.whl.metadata (476 bytes)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.10.3->vllm->-r requirements.txt (line 18))\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7)) (3.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7)) (2024.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core==0.10.61 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_core-0.10.61-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.27 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_llms_openai-0.1.27-py3-none-any.whl.metadata (610 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.8-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_readers_file-0.1.32-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.10.61->llama-index->-r requirements.txt (line 21))\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.61->llama-index->-r requirements.txt (line 21))\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core==0.10.61->llama-index->-r requirements.txt (line 21))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting ollama>=0.3.0 (from llama-index-llms-ollama->-r requirements.txt (line 23))\n",
      "  Downloading ollama-0.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting instructorembedding<2.0.0,>=1.0.1 (from llama-index-embeddings-instructor->-r requirements.txt (line 24))\n",
      "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentence-transformers (from -r requirements.txt (line 7))\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 4)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 4)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 4)) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 15)) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 15)) (1.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 5))\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 15)) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 15)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 15)) (0.14.0)\n",
      "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 22))\n",
      "  Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.27->langchain->-r requirements.txt (line 4))\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging (from onnxruntime<1.16->unstructured-inference==0.7.25->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 10))\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 4))\n",
      "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m404.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_cloud-0.0.12-py3-none-any.whl.metadata (751 bytes)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 21))\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from beautifulsoup4->unstructured[pdf]->-r requirements.txt (line 6)) (2.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index->-r requirements.txt (line 21))\n",
      "  Downloading llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->unstructured[pdf]->-r requirements.txt (line 6)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->unstructured[pdf]->-r requirements.txt (line 6)) (1.4.2)\n",
      "Collecting lark (from outlines<0.1,>=0.0.43->vllm->-r requirements.txt (line 18))\n",
      "  Downloading lark-1.1.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: cloudpickle in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from outlines<0.1,>=0.0.43->vllm->-r requirements.txt (line 18)) (2.2.1)\n",
      "Collecting diskcache (from outlines<0.1,>=0.0.43->vllm->-r requirements.txt (line 18))\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numba in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from outlines<0.1,>=0.0.43->vllm->-r requirements.txt (line 18)) (0.59.1)\n",
      "Requirement already satisfied: referencing in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from outlines<0.1,>=0.0.43->vllm->-r requirements.txt (line 18)) (0.35.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from outlines<0.1,>=0.0.43->vllm->-r requirements.txt (line 18)) (4.22.0)\n",
      "Collecting pycountry (from outlines<0.1,>=0.0.43->vllm->-r requirements.txt (line 18))\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyairports (from outlines<0.1,>=0.0.43->vllm->-r requirements.txt (line 18))\n",
      "  Downloading pyairports-2.1.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting starlette<1.0.0,>=0.30.0 (from prometheus-fastapi-instrumentator>=7.0.0->vllm->-r requirements.txt (line 18))\n",
      "  Downloading starlette-0.38.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain->-r requirements.txt (line 4))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<3,>=1->langchain->-r requirements.txt (line 4))\n",
      "  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ray>=2.9->vllm->-r requirements.txt (line 18)) (1.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 3)) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 4)) (3.0.3)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 10))\n",
      "  Downloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 10))\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.25.1->unstructured-inference==0.7.25->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 10))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 10)) (13.7.1)\n",
      "Collecting dataclasses-json-speakeasy>=0.5.11 (from unstructured-client<=0.18.0->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpath-python>=1.0.6 (from unstructured-client<=0.18.0->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured-client<=0.18.0->unstructured[pdf]->-r requirements.txt (line 6)) (1.0.0)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 10))\n",
      "  Downloading cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 10))\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<1.16->unstructured-inference==0.7.25->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->ragas->-r requirements.txt (line 17)) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->ragas->-r requirements.txt (line 17)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->ragas->-r requirements.txt (line 17)) (0.3.8)\n",
      "Collecting xxhash (from datasets->ragas->-r requirements.txt (line 17))\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->ragas->-r requirements.txt (line 17)) (0.70.16)\n",
      "Collecting fsspec (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 7))\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting starlette<1.0.0,>=0.30.0 (from prometheus-fastapi-instrumentator>=7.0.0->vllm->-r requirements.txt (line 18))\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->spacy->-r requirements.txt (line 10)) (2.1.5)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pdfminer.six->unstructured[pdf]->-r requirements.txt (line 6)) (42.0.8)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 7)) (3.5.0)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]->vllm->-r requirements.txt (line 18))\n",
      "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm->-r requirements.txt (line 18))\n",
      "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]->vllm->-r requirements.txt (line 18))\n",
      "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]->vllm->-r requirements.txt (line 18))\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain->-r requirements.txt (line 4)) (2.4)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 10))\n",
      "  Downloading marisa_trie-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 10)) (2.18.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->outlines<0.1,>=0.0.43->vllm->-r requirements.txt (line 18)) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->outlines<0.1,>=0.0.43->vllm->-r requirements.txt (line 18)) (0.18.1)\n",
      "Collecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pdfplumber (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pdfplumber-0.11.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from numba->outlines<0.1,>=0.0.43->vllm->-r requirements.txt (line 18)) (0.42.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->onnxruntime<1.16->unstructured-inference==0.7.25->unstructured[pdf]->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]->-r requirements.txt (line 6)) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 10)) (0.1.2)\n",
      "Collecting timm>=0.9.2 (from effdet->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading timm-1.0.8-py3-none-any.whl.metadata (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycocotools>=2.0.2 (from effdet->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting omegaconf>=2.0 (from effdet->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting portalocker (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pdfminer.six (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m679.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pycocotools>=2.0.2->effdet->unstructured[pdf]->-r requirements.txt (line 6)) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->unstructured[pdf]->-r requirements.txt (line 6)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->unstructured[pdf]->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->unstructured[pdf]->-r requirements.txt (line 6)) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->unstructured[pdf]->-r requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->unstructured[pdf]->-r requirements.txt (line 6)) (3.1.2)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.2.12-py3-none-any.whl (990 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.2.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading unstructured_inference-0.7.25-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m513.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading langgraph-0.1.19-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m449.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading redis-5.0.8-py3-none-any.whl (255 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.6/255.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading redisvl-0.3.0-py3-none-any.whl (91 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m654.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.1.20-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m390.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.39.0-py3-none-any.whl (336 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.7/336.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ragas-0.1.13-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading vllm-0.5.4-cp38-abi3-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lm_format_enforcer-0.10.3-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m330.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading vllm_flash_attn-2.6.1-cp310-cp310-manylinux1_x86_64.whl (75.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m440.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index-0.10.61-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_core-0.10.61-py3-none-any.whl (15.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Downloading llama_index_llms_ollama-0.2.2-py3-none-any.whl (4.4 kB)\n",
      "Downloading llama_index_embeddings_instructor-0.1.3-py3-none-any.whl (3.6 kB)\n",
      "Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cmake-3.30.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m702.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
      "Downloading langchain_core-0.2.28-py3-none-any.whl (379 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.1.98-py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.2/140.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_agent_openai-0.2.9-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl (9.5 kB)\n",
      "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.1.27-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.8-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.1.7-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.32-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
      "Downloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Downloading ollama-0.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading outlines-0.0.46-py3-none-any.whl (101 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.9/156.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl (19 kB)\n",
      "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.34.0-cp310-cp310-manylinux2014_x86_64.whl (64.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.0/493.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.4/922.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m602.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
      "Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.112.0-py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading lxml-5.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.555.43-py3-none-any.whl (39 kB)\n",
      "Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pikepdf-9.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow_heif-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading rapidfuzz-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading unstructured-0.13.2-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_cloud-0.0.12-py3-none-any.whl (169 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\n",
      "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m870.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.5/435.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lark-1.1.9-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyairports-2.1.1-py3-none-any.whl (371 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading uvicorn-0.30.5-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pdfplumber-0.11.2-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading timm-1.0.8-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: langdetect, iopath, antlr4-python3-runtime\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=29206a646a9a88082a911b56a32e58fe65b3cb9c6e3bb20b428a77da50ef817e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=3d599bbd4ffdb6374404bd1b8704e77af956ebd42b83a0b0a79a48f7189978e8\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=381f88c454ed67e7395d36681703383bea173a3c3e8d27584386304d7ad5f0fe\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "Successfully built langdetect iopath antlr4-python3-runtime\n",
      "Installing collected packages: striprtf, sentencepiece, pyairports, nvidia-ml-py, ninja, instructorembedding, flatbuffers, filetype, dirtyjson, cymem, appdirs, antlr4-python3-runtime, xxhash, websockets, wasabi, uvloop, uvicorn, typing-inspect, triton, spacy-loggers, spacy-legacy, smart-open, shellingham, safetensors, redis, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, pysbd, pypdfium2, pypdf, pydantic-core, pycountry, portalocker, pillow-heif, pdf2image, packaging, orjson, opencv-python, onnx, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, murmurhash, minijinja, marisa-trie, lxml, lark, langdetect, jsonpath-python, jsonpatch, interegular, humanfriendly, httptools, fsspec, emoji, distro, diskcache, deprecated, cmake, cloudpathlib, catalogue, blis, backoff, annotated-types, watchfiles, unstructured.pytesseract, tiktoken, starlette, srsly, pytesseract, pydantic, preshed, pikepdf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, marshmallow, language-data, iopath, huggingface-hub, coloredlogs, typer, tokenizers, redisvl, pycocotools, prometheus-fastapi-instrumentator, pdfminer.six, openai, onnxruntime, ollama, nvidia-cusolver-cu12, lm-format-enforcer, llama-cloud, langsmith, langcodes, fastapi, dataclasses-json-speakeasy, dataclasses-json, confection, weasel, unstructured-client, transformers, torch, thinc, ray, pdfplumber, llama-index-legacy, llama-index-core, langchain-core, datasets, xformers, vllm-flash-attn, unstructured, torchvision, spacy, sentence-transformers, outlines, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-ollama, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, layoutparser, langgraph, langchain-text-splitters, langchain-openai, vllm, timm, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-embeddings-instructor, llama-index-embeddings-huggingface, llama-index-cli, llama-index-agent-openai, langchain, llama-index-program-openai, langchain-community, effdet, ragas, llama-index-question-gen-openai, unstructured-inference, llama-index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.0\n",
      "    Uninstalling fsspec-2024.6.0:\n",
      "      Successfully uninstalled fsspec-2024.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "astropy 6.1.0 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\n",
      "scikit-image 0.23.2 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.2 which is incompatible.\n",
      "sphinx 7.3.7 requires docutils<0.22,>=0.18.1, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 backoff-2.2.1 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 cmake-3.30.2 coloredlogs-15.0.1 confection-0.1.5 cymem-2.0.8 dataclasses-json-0.6.7 dataclasses-json-speakeasy-0.5.11 datasets-2.20.0 deprecated-1.2.14 dirtyjson-1.0.8 diskcache-5.6.3 distro-1.9.0 effdet-0.4.1 emoji-2.12.1 fastapi-0.112.0 filetype-1.2.0 flatbuffers-24.3.25 fsspec-2024.5.0 httptools-0.6.1 huggingface-hub-0.24.5 humanfriendly-10.0 instructorembedding-1.0.1 interegular-0.3.3 iopath-0.1.10 jsonpatch-1.33 jsonpath-python-1.0.6 langchain-0.2.12 langchain-community-0.2.11 langchain-core-0.2.28 langchain-openai-0.1.20 langchain-text-splitters-0.2.2 langcodes-3.4.0 langdetect-1.0.9 langgraph-0.1.19 langsmith-0.1.98 language-data-1.2.0 lark-1.1.9 layoutparser-0.3.4 llama-cloud-0.0.12 llama-index-0.10.61 llama-index-agent-openai-0.2.9 llama-index-cli-0.1.13 llama-index-core-0.10.61 llama-index-embeddings-huggingface-0.2.2 llama-index-embeddings-instructor-0.1.3 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.2.7 llama-index-legacy-0.9.48 llama-index-llms-ollama-0.2.2 llama-index-llms-openai-0.1.27 llama-index-multi-modal-llms-openai-0.1.8 llama-index-program-openai-0.1.7 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.32 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.9 lm-format-enforcer-0.10.3 lxml-5.2.2 marisa-trie-1.2.0 marshmallow-3.21.3 minijinja-2.0.1 murmurhash-1.0.10 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py-12.555.43 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 ollama-0.3.1 omegaconf-2.3.0 onnx-1.16.2 onnxruntime-1.15.1 openai-1.39.0 opencv-python-4.10.0.84 orjson-3.10.6 outlines-0.0.46 packaging-24.1 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.2 pikepdf-9.1.0 pillow-heif-0.18.0 portalocker-2.10.1 preshed-3.0.9 prometheus-fastapi-instrumentator-7.0.0 pyairports-2.1.1 pycocotools-2.0.8 pycountry-24.6.1 pydantic-2.8.2 pydantic-core-2.20.1 pypdf-4.3.1 pypdfium2-4.30.0 pysbd-0.3.4 pytesseract-0.3.10 python-dotenv-1.0.1 python-iso639-2024.4.27 python-magic-0.4.27 python-multipart-0.0.9 ragas-0.1.13 rapidfuzz-3.9.5 ray-2.34.0 redis-5.0.8 redisvl-0.3.0 safetensors-0.4.4 sentence-transformers-2.7.0 sentencepiece-0.2.0 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 starlette-0.37.2 striprtf-0.0.26 thinc-8.2.5 tiktoken-0.7.0 timm-1.0.8 tokenizers-0.19.1 torch-2.4.0 torchvision-0.19.0 transformers-4.43.4 triton-3.0.0 typer-0.12.3 typing-inspect-0.9.0 unstructured-0.13.2 unstructured-client-0.18.0 unstructured-inference-0.7.25 unstructured.pytesseract-0.3.12 uvicorn-0.30.5 uvloop-0.19.0 vllm-0.5.4 vllm-flash-attn-2.6.1 wasabi-1.1.3 watchfiles-0.22.0 weasel-0.4.1 websockets-12.0 xformers-0.0.27.post2 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import dotenv\n",
    "# mute warnings\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')\n",
    "# load env vars from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "parent_directory = os.path.dirname(dir_path)\n",
    "os.environ[\"ROOT_DIR\"] = parent_directory\n",
    "\n",
    "#setting the local downloaded sentence transformer models f\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{parent_directory}/models\"\n",
    "SCHEMA_PATH = f\"{parent_directory}/2_RAG_patterns_with_redis/sec_index.yaml\"\n",
    "SOURCE_DOC = '../resources/filings/AAPL/AAPL-2023-10K.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Redis and create chunks to populate the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init Redis connection and index\n",
    "import os\n",
    "from redisvl.index import SearchIndex\n",
    "from redis import Redis\n",
    "\n",
    "# init Redis connection\n",
    "# Replace values below with your own if using Redis Cloud instance\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "\n",
    "prefix = 'chunk'\n",
    "client = Redis.from_url(REDIS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `UnstructuredFileLoader` was deprecated in LangChain 0.2.8 and will be removed in 0.4.0. An updated version of the class exists in the langchain-unstructured package and should be used instead. To use it run `pip install -U langchain-unstructured` and import as `from langchain_unstructured import UnstructuredLoader`.\n",
      "  warn_deprecated(\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", cache_folder=os.getenv(\"TRANSFORMERS_CACHE\", f\"{parent_directory}/models\"))\n",
    "\n",
    "loader = UnstructuredFileLoader(SOURCE_DOC, mode=\"single\", strategy=\"fast\")\n",
    "\n",
    "# for use later with parent-doc index\n",
    "source_doc = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "chunks = loader.load_and_split(text_splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_objs = [\n",
    "    {\n",
    "        \"chunk_id\": f\"{chunk.metadata['source']}-{str(uuid.uuid4())}\",\n",
    "        \"source_doc\": f\"{chunk.metadata['source']}\",\n",
    "        \"content\": chunk.page_content,\n",
    "        \"doc_type\": \"10k\",\n",
    "        \"text_embedding\": np.array(embeddings.embed_query(chunk.page_content)).astype(np.float32).tobytes()\n",
    "    }\n",
    "    for chunk in chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.schema import IndexSchema\n",
    "\n",
    "index_name = 'eval'\n",
    "\n",
    "schema = IndexSchema.from_dict(\n",
    "    {\n",
    "        \"index\": {\n",
    "            \"name\": index_name,\n",
    "            \"prefix\": prefix,\n",
    "            \"storage_type\": \"hash\",\n",
    "        },\n",
    "        \"fields\": [\n",
    "            {\"name\": \"chunk_id\", \"type\": \"tag\"},\n",
    "            {\"name\": \"source_doc\", \"type\": \"tag\"},\n",
    "            {\"name\": \"doc_type\", \"type\": \"tag\"},\n",
    "            {\"name\": \"content\", \"type\": \"text\"},\n",
    "            {\n",
    "                \"name\": \"text_embedding\", \n",
    "                \"type\": \"vector\", \n",
    "                \"attrs\": {\"type\": \"float32\", \"dims\": 384, \"distance_metric\": \"COSINE\", \"algorithm\": \"flat\"},\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# create an index from schema and the client\n",
    "index = SearchIndex(schema, client)\n",
    "index.create(overwrite=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = index.load(index_objs, id_field=\"chunk_id\")\n",
    "len(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vector store\n",
    "This is the same processes as we have done in the previous examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Redis as LangChainRedis\n",
    "\n",
    "\n",
    "# with langchain we can manually modify the default vector schema configuration\n",
    "vector_schema = {\n",
    "    \"name\": \"text_embedding\",        # name of the vector field in langchain\n",
    "    \"algorithm\": \"HNSW\",           # could use HNSW instead\n",
    "    \"dims\": 384,                   # set based on the HF model embedding dimension\n",
    "    \"distance_metric\": \"COSINE\",   # could use EUCLIDEAN or IP\n",
    "    \"datatype\": \"FLOAT32\",\n",
    "}\n",
    "\n",
    "# here we can define the entire schema spec for our index in LangChain\n",
    "index_schema = {\n",
    "    \"vector\": [vector_schema],\n",
    "    \"text\": [{\"name\": \"content\"}, {\"name\": \"source_doc\"}, {\"name\": \"doc_type\"}, {\"name\": \"chunk_id\"}],\n",
    "    \"content_vector_key\": \"text_embedding\" ,   # name of the vector field in langchain\n",
    "}\n",
    "\n",
    "\n",
    "rds = LangChainRedis.from_existing_index(\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name,\n",
    "    schema=index_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out!\n",
    "We can see the vector store is populated and returning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10K.pdf-bfda24f1-fe81-47e1-b3e3-85dd6b4d3c4e', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10K.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10K.pdf-bfda24f1-fe81-47e1-b3e3-85dd6b4d3c4e'}, page_content='Apple Inc. | 2023 Form 10-K | 27\\n\\nPage\\n\\n28\\n\\n29 30\\n\\n31\\n\\n32 33 49\\n\\nApple Inc.\\n\\nCONSOLIDATED STATEMENTS OF OPERATIONS (In millions, except number of shares, which are reﬂected in thousands, and per-share amounts)\\n\\nYears ended\\n\\nSeptember 30, 2023\\n\\nSeptember 24, 2022\\n\\nSeptember 25, 2021\\n\\nNet sales: Products Services\\n\\nTotal net sales\\n\\n$\\n\\n298,085 $ 85,200 383,285\\n\\n316,199 $ 78,129 394,328\\n\\n297,392 68,425 365,817\\n\\nCost of sales: Products Services\\n\\nTotal cost of sales Gross margin\\n\\n189,282 24,855 214,137 169,148\\n\\n201,471 22,075 223,546 170,782\\n\\n192,266 20,715 212,981 152,836\\n\\nOperating expenses:\\n\\nResearch and development Selling, general and administrative\\n\\nTotal operating expenses\\n\\n29,915 24,932 54,847\\n\\n26,251 25,094 51,345\\n\\n21,914 21,973 43,887\\n\\nOperating income Other income/(expense), net Income before provision for income taxes Provision for income taxes Net income\\n\\n$\\n\\n114,301 (565) 113,736 16,741 96,995 $\\n\\n119,437 (334) 119,103 19,300 99,803 $\\n\\n108,949 258 109,207 14,527 94,680')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rds.similarity_search(\"What was apples revenue last year?\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup RAG\n",
    "\n",
    "Initialize llm examples shown for Ollama, OpenAI, and VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama, VLLMOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "# for Ollama use => increase context window\n",
    "# llm = Ollama(model=\"llama3\", num_ctx=4097, temperature=0.1)\n",
    "\n",
    "llm = VLLMOpenAI(\n",
    "            openai_api_key=os.environ[\"HF_MODEL_HUB_TOKEN\"], # vllm token key for huggingface through openai like interface\n",
    "            openai_api_base=os.environ[\"VLLM_URL\"],\n",
    "            model_name=os.environ[\"LOCAL_VLLM_MODEL\"],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     model=\"gpt-3.5-turbo-16k\",\n",
    "#     max_tokens=None\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt():\n",
    "    \"\"\"Create the QA chain.\"\"\"\n",
    "    from langchain.prompts import PromptTemplate\n",
    "\n",
    "    # Define our prompt\n",
    "    prompt_template = \"\"\"Use the following pieces of context from financial 10k filings data to answer the user question at the end. Only use the result from tools and evidence provided to you. If you don't know the answer, say that you don't know, don't try to make up an answer. Provide the source of the document that you used to get the answer.\n",
    "\n",
    "    This should be in the following format:\n",
    "\n",
    "    Question: [question here]\n",
    "    Answer: [answer here]\n",
    "    Source: [source document here]\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Context:\n",
    "    ---------\n",
    "    {context}\n",
    "    ---------\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def get_search_kwargs(filters, distance_threshold):\n",
    "    return {\"distance_threshold\":distance_threshold,\"filter\":filters}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=rds.as_retriever(search_type=\"similarity_distance_threshold\", search_kwargs={\"distance_threshold\":0.8, 'include_metadata': True}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": get_prompt()},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have our RAG QA to test out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': \"What was Apple's revenue last year compared to this year??\",\n",
       " 'result': \" Apple's total net sales decreased 3% or $11.0 billion during 2023 compared to 2022.\\n    Source: Apple Inc. | 2023 Form 10-K | 27\\n\\nPlease note that the answer is based on the provided context and may not be the exact answer you are looking for. If you need more information or clarification, please let me know.\",\n",
       " 'source_documents': [Document(metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10K.pdf-bfda24f1-fe81-47e1-b3e3-85dd6b4d3c4e', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10K.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10K.pdf-bfda24f1-fe81-47e1-b3e3-85dd6b4d3c4e'}, page_content='Apple Inc. | 2023 Form 10-K | 27\\n\\nPage\\n\\n28\\n\\n29 30\\n\\n31\\n\\n32 33 49\\n\\nApple Inc.\\n\\nCONSOLIDATED STATEMENTS OF OPERATIONS (In millions, except number of shares, which are reﬂected in thousands, and per-share amounts)\\n\\nYears ended\\n\\nSeptember 30, 2023\\n\\nSeptember 24, 2022\\n\\nSeptember 25, 2021\\n\\nNet sales: Products Services\\n\\nTotal net sales\\n\\n$\\n\\n298,085 $ 85,200 383,285\\n\\n316,199 $ 78,129 394,328\\n\\n297,392 68,425 365,817\\n\\nCost of sales: Products Services\\n\\nTotal cost of sales Gross margin\\n\\n189,282 24,855 214,137 169,148\\n\\n201,471 22,075 223,546 170,782\\n\\n192,266 20,715 212,981 152,836\\n\\nOperating expenses:\\n\\nResearch and development Selling, general and administrative\\n\\nTotal operating expenses\\n\\n29,915 24,932 54,847\\n\\n26,251 25,094 51,345\\n\\n21,914 21,973 43,887\\n\\nOperating income Other income/(expense), net Income before provision for income taxes Provision for income taxes Net income\\n\\n$\\n\\n114,301 (565) 113,736 16,741 96,995 $\\n\\n119,437 (334) 119,103 19,300 99,803 $\\n\\n108,949 258 109,207 14,527 94,680'),\n",
       "  Document(metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10K.pdf-b5603122-b65e-4861-b62f-1b7595944f51', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10K.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10K.pdf-b5603122-b65e-4861-b62f-1b7595944f51'}, page_content='Wearables, Home and Accessories net sales decreased 3% or $1.4 billion during 2023 compared to 2022 due primarily to lower net sales of Wearables and Accessories.\\n\\nServices\\n\\nServices net sales increased 9% or $7.1 billion during 2023 compared to 2022 due to higher net sales across all lines of business.\\n\\nApple Inc. | 2023 Form 10-K | 22\\n\\n2021\\n\\n191,973 35,190 31,862 38,367 68,425 365,817\\n\\nGross Margin\\n\\nProducts and Services gross margin and gross margin percentage for 2023, 2022 and 2021 were as follows (dollars in millions):\\n\\n2023\\n\\n2022\\n\\nGross margin:\\n\\nProducts Services\\n\\nTotal gross margin\\n\\n$\\n\\n$\\n\\n108,803 $ 60,345 169,148 $\\n\\n114,728 $ 56,054 170,782 $\\n\\nGross margin percentage:\\n\\nProducts Services\\n\\nTotal gross margin percentage\\n\\n36.5 % 70.8 % 44.1 %\\n\\n36.3 % 71.7 % 43.3 %\\n\\nProducts Gross Margin'),\n",
       "  Document(metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10K.pdf-24a01be2-3a84-4ac2-a28e-4e8149ab2917', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10K.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10K.pdf-24a01be2-3a84-4ac2-a28e-4e8149ab2917'}, page_content='$ $\\n\\n24,257 $ 11,888 $\\n\\n25,977 $ 12,257 $\\n\\nRest of Asia Paciﬁc:\\n\\nNet sales Operating income\\n\\n$ $\\n\\n29,615 $ 12,066 $\\n\\n29,375 $ 11,569 $\\n\\nA reconciliation of the Company’s segment operating income to the Consolidated Statements of Operations for 2023, 2022 and 2021 is as follows (in millions):\\n\\n2023\\n\\n2022\\n\\nSegment operating income Research and development expense Other corporate expenses, net Total operating income\\n\\n(1)\\n\\n$\\n\\n$\\n\\n150,888 $ (29,915) (6,672) 114,301 $\\n\\n152,895 $ (26,251) (7,207) 119,437 $\\n\\n(1)\\n\\nIncludes corporate marketing expenses, certain share-based compensation expenses, various nonrecurring charges, and other separately managed general and administrative costs.\\n\\nApple Inc. | 2023 Form 10-K | 47\\n\\n2021\\n\\n153,306 53,382\\n\\n89,307 32,505\\n\\n68,366 28,504\\n\\n28,482 12,798\\n\\n26,356 9,817\\n\\n2021\\n\\n137,006 (21,914) (6,143) 108,949'),\n",
       "  Document(metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10K.pdf-96a715ad-9ba9-4b66-9695-9e67af80168f', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10K.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10K.pdf-96a715ad-9ba9-4b66-9695-9e67af80168f'}, page_content='Fiscal Year Highlights\\n\\nThe Company’s total net sales were $383.3 billion and net income was $97.0 billion during 2023.\\n\\nThe Company’s total net sales decreased 3% or $11.0 billion during 2023 compared to 2022. The weakness in foreign currencies relative to the U.S. dollar accounted for more than the entire year-over-year decrease in total net sales, which consisted primarily of lower net sales of Mac and iPhone, partially oﬀset by higher net sales of Services.\\n\\nThe Company announces new product, service and software oﬀerings at various times during the year. Signiﬁcant announcements during ﬁscal year 2023 included the following:\\n\\nFirst Quarter 2023:\\n\\n• • MLS Season Pass, a Major League Soccer subscription streaming service.\\n\\niPad and iPad Pro; Next-generation Apple TV 4K; and\\n\\nSecond Quarter 2023:\\n\\nMacBook Pro 14”, MacBook Pro 16” and Mac mini; and • Second-generation HomePod.\\n\\nThird Quarter 2023:\\n\\nMacBook Air 15”, Mac Studio and Mac Pro; •')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What was Apple's revenue last year compared to this year??\"\n",
    "res=qa(query)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup complete!\n",
    "\n",
    "In the resources we have included a pre-generated set of test data for evaluation generated with the TestsetGenerator class from the ragas library for demo speed sake. The code used to generate this data is provided as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "testset = pd.read_csv(\"resources/full_testset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TestsetGenerator example code for generate testset\n",
    "\n",
    "This can be a time consuming process so we have gone ahead and pregenerated this with the following code. See more on creating test sets [here](https://docs.ragas.io/en/latest/getstarted/testset_generation.html).\n",
    "\n",
    "Note: while we are using synthetic test set here RAGAS can be utilized with human labeled data and [self created test sets](https://docs.ragas.io/en/stable/howtos/applications/data_preparation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if problems with nltk data\n",
    "# import os\n",
    "# os.environ[\"NLTK_DATA\"] = '/Users/<user>/nltk_data'\n",
    "\n",
    "if not len(testset):\n",
    "    from ragas.testset.generator import TestsetGenerator\n",
    "    from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "    from llama_index.llms.ollama import Ollama\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "    generator_llm = llm\n",
    "    critic_llm = llm\n",
    "    embeddings = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "    generator = TestsetGenerator.from_llama_index(\n",
    "        generator_llm=generator_llm,\n",
    "        critic_llm=critic_llm,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "\n",
    "    reader = SimpleDirectoryReader(input_files=[SOURCE_DOC])\n",
    "\n",
    "    documents = reader.load_data()\n",
    "\n",
    "    testset = generator.generate_with_llamaindex_docs(\n",
    "        documents,\n",
    "        test_size=20,\n",
    "        distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
    "    )\n",
    "\n",
    "    testset.to_pandas().to_csv(\"full_testset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin evaluation\n",
    "[The ragas library](https://docs.ragas.io/en/stable/index.html) provides helpful classes for abstracting the complexity of creating test sets and evaluating apps that use generative technology. Above we demonstrated how the TestsetGenerator class can be used to create an example dataset with. Now we will create a few helper functions to store and aggregate the answers/ context generated/retrieved from the RAG QA app we defined earlier. This data will be what we pass to the ragas library for calculating our performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "def parse_contexts(source_docs):\n",
    "    return [doc.page_content for doc in source_docs]\n",
    "\n",
    "def create_evaluation_dataset(chain, testset):\n",
    "    res_set = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": [],\n",
    "        \"ground_truth\": []\n",
    "    }\n",
    "\n",
    "    for _, row in testset.iterrows():\n",
    "        # call QA chain\n",
    "        result = chain.invoke(row[\"question\"])\n",
    "\n",
    "        res_set[\"question\"].append(row[\"question\"])\n",
    "        res_set[\"answer\"].append(result[\"result\"])\n",
    "\n",
    "        contexts = parse_contexts(result[\"source_documents\"])\n",
    "        \n",
    "        if not len(contexts):\n",
    "            print(f\"no contexts found for question: {row['question']}\")\n",
    "        res_set[\"contexts\"].append(contexts)\n",
    "        res_set[\"ground_truth\"].append(str(row[\"ground_truth\"]))\n",
    "\n",
    "    return Dataset.from_dict(res_set)\n",
    "\n",
    "def evaluate_dataset(eval_dataset, metrics, llm, embeddings):\n",
    "\n",
    "    run_config = RunConfig()\n",
    "    run_config.max_retries = 1\n",
    "\n",
    "\n",
    "    eval_result = evaluate(\n",
    "        eval_dataset,\n",
    "        metrics=metrics,\n",
    "        run_config=run_config,\n",
    "        llm=llm,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "    eval_df = eval_result.to_pandas()\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset\n",
    "\n",
    "Input: chain to be evaluated, testset\n",
    "Output: dataset to pass to ragas evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = create_evaluation_dataset(qa, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What services does Apple offer through its Pay...</td>\n",
       "      <td>Apple offers payment services, including Appl...</td>\n",
       "      <td>[The Company operates various platforms, inclu...</td>\n",
       "      <td>Apple offers payment services through Apple Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the estimated maximum one-day loss in ...</td>\n",
       "      <td>$1.0 billion\\n    Source: Apple Inc. | 2023 F...</td>\n",
       "      <td>[The Company applied a value-at-risk (“VAR”) m...</td>\n",
       "      <td>$1.0 billion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What drives Apple Inc.'s competitive edge &amp; ho...</td>\n",
       "      <td>Apple Inc.'s competitive edge is driven by it...</td>\n",
       "      <td>[The Company has a minority market share in th...</td>\n",
       "      <td>The information provided in the context sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are potential risks for Apple if it doesn...</td>\n",
       "      <td>\\n    Source: \\n\\nPlease provide the answer a...</td>\n",
       "      <td>[Apple Inc. | 2023 Form 10-K | 7\\n\\nThe Compan...</td>\n",
       "      <td>If Apple fails to meet regulatory expectations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What factors contributed to the 7% boost in iP...</td>\n",
       "      <td>The answer is not provided in the given conte...</td>\n",
       "      <td>[(1) Products net sales include amortization o...</td>\n",
       "      <td>The 7% boost in iPhone sales was primarily due...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What services does Apple offer through its Pay...   \n",
       "1  What is the estimated maximum one-day loss in ...   \n",
       "2  What drives Apple Inc.'s competitive edge & ho...   \n",
       "3  What are potential risks for Apple if it doesn...   \n",
       "4  What factors contributed to the 7% boost in iP...   \n",
       "\n",
       "                                              answer  \\\n",
       "0   Apple offers payment services, including Appl...   \n",
       "1   $1.0 billion\\n    Source: Apple Inc. | 2023 F...   \n",
       "2   Apple Inc.'s competitive edge is driven by it...   \n",
       "3   \\n    Source: \\n\\nPlease provide the answer a...   \n",
       "4   The answer is not provided in the given conte...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [The Company operates various platforms, inclu...   \n",
       "1  [The Company applied a value-at-risk (“VAR”) m...   \n",
       "2  [The Company has a minority market share in th...   \n",
       "3  [Apple Inc. | 2023 Form 10-K | 7\\n\\nThe Compan...   \n",
       "4  [(1) Products net sales include amortization o...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  Apple offers payment services through Apple Ca...  \n",
       "1                                       $1.0 billion  \n",
       "2  The information provided in the context sugges...  \n",
       "3  If Apple fails to meet regulatory expectations...  \n",
       "4  The 7% boost in iPhone sales was primarily due...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate generation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9bdc10a65446bb90e18d239bba111f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "\n",
    "# first generate for faithfulness\n",
    "faithfulness_metrics = evaluate_dataset(eval_dataset, [faithfulness], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db006e9eb6e429faa82d57568144f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# next for answer_relevancy\n",
    "answer_relevancy_metrics = evaluate_dataset(eval_dataset, [answer_relevancy], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.551016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.384250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.638030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfulness  answer_relevancy\n",
       "count      9.000000         19.000000\n",
       "mean       0.833333          0.551016\n",
       "std        0.353553          0.384250\n",
       "min        0.000000          0.000000\n",
       "25%        1.000000          0.194516\n",
       "50%        1.000000          0.638030\n",
       "75%        1.000000          0.863987\n",
       "max        1.000000          0.995077"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_metrics_default = faithfulness_metrics\n",
    "gen_metrics_default[\"answer_relevancy\"] = answer_relevancy_metrics[\"answer_relevancy\"]\n",
    "\n",
    "gen_metrics_default.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do these number mean and how were they calculated?\n",
    "\n",
    "Note: the following examples are paraphrased from the [ragas docs](https://docs.ragas.io/en/stable/concepts/metrics/index.html)\n",
    "\n",
    "------\n",
    "\n",
    "### Faithfulness\n",
    "\n",
    "An answer to a question can be said to be \"faithful\" if the **claims** that are made in the answer **can be inferred** from the **context**.\n",
    "\n",
    "#### Mathematically:\n",
    "\n",
    "$$\n",
    "Faithfullness\\ score = \\frac{Number\\ of\\ claims\\ in\\ the\\ generated\\ answer\\ that\\ can\\ be\\ inferred\\ from\\ the\\ given\\ context}{Total\\ number\\ of\\ claim\\ in\\ the\\ generated\\ answer}\n",
    "$$\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "> Question: Where and when was Einstein born?\n",
    "> \n",
    "> Context: Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time\n",
    ">\n",
    "> answer: Einstein was born in Germany on 20th March 1879.\n",
    "\n",
    "Step 1: Use LLM to break generated answer into individual statements.\n",
    "- “Einstein was born in Germany.”\n",
    "- “Einstein was born on 20th March 1879.”\n",
    "\n",
    "Step 2: For each statement use LLM to verify if it can be inferred from the context.\n",
    "- “Einstein was born in Germany.” => yes. \n",
    "- “Einstein was born on 20th March 1879.” => no.\n",
    "\n",
    "Step 3: plug into formula\n",
    "\n",
    "Number of claims inferred from context = 1\n",
    "Total number of claims = 2\n",
    "Faithfulness = 1/2\n",
    "\n",
    "### Answer Relevance\n",
    "\n",
    "An answer can be said to be relevant if it directly addresses the question (intuitively).\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "1. Use an LLM to generate \"hypothetical\" questions to a given answer with the following prompt:\n",
    "\n",
    "    > Generate a question for the given answer.\n",
    "    > answer: [answer]\n",
    "\n",
    "2. Embed the generated \"hypothetical\" questions as vectors.\n",
    "3. Calculate the cosine similarity of the hypothetical questions and the original question, sum those similarities, and divide by n.\n",
    "\n",
    "With data:\n",
    "\n",
    "> Question: Where is France and what is it’s capital?\n",
    "> \n",
    "> answer: France is in western Europe.\n",
    "\n",
    "Step 1 - use LLM to create 'n' variants of question from the generated answer.\n",
    "\n",
    "- “In which part of Europe is France located?”\n",
    "- “What is the geographical location of France within Europe?”\n",
    "- “Can you identify the region of Europe where France is situated?”\n",
    "\n",
    "Step 2 - Calculate the mean cosine similarity between the generated questions and the actual question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate retrieval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679b1df4b8494c0d82a66503e2f490e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import context_recall, context_precision\n",
    "\n",
    "context_recall_metrics = evaluate_dataset(eval_dataset, [context_recall], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fba3bed27b4427db19eec69b301a5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n",
      "Exception raised in Job[10]: ValidationError(2 validation errors for ContextPrecisionVerifications\n",
      "__root__ -> 0 -> reason\n",
      "  field required (type=value_error.missing)\n",
      "__root__ -> 0 -> verdict\n",
      "  field required (type=value_error.missing))\n"
     ]
    }
   ],
   "source": [
    "context_precision_metrics = evaluate_dataset(eval_dataset, [context_precision], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.637346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.453861</td>\n",
       "      <td>0.391155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.694444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       context_recall  context_precision\n",
       "count       16.000000          18.000000\n",
       "mean         0.578125           0.637346\n",
       "std          0.453861           0.391155\n",
       "min          0.000000           0.000000\n",
       "25%          0.000000           0.375000\n",
       "50%          0.750000           0.694444\n",
       "75%          1.000000           1.000000\n",
       "max          1.000000           1.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_metrics_default = context_recall_metrics\n",
    "ret_metrics_default[\"context_precision\"] = context_precision_metrics[\"context_precision\"]\n",
    "\n",
    "ret_metrics_default.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do these numbers mean?\n",
    "\n",
    "Retrieval metrics quantify how well the system performed at fetching the best possible context for generation. Like before please review the definitions below to understand what happens under-the-hood when we execute the evaluation code. \n",
    "\n",
    "-----\n",
    "\n",
    "### Context Relevance\n",
    "\n",
    "\"The context is considered relevant to the extent that it exclusively contains information that is needed to answer the question.\"\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "1. Use the following LLM prompt to extract a subset of sentences necessary to answer the question. The context is defined as the formatted search result from the vector database.\n",
    "\n",
    "    > Please extract relevant sentences from\n",
    "    > the provided context that can potentially\n",
    "    > help answer the following `{question}`. If no\n",
    "    > relevant sentences are found, or if you\n",
    "    > believe the question cannot be answered\n",
    "    > from the given context, return the phrase\n",
    "    > \"Insufficient Information\". While extracting candidate sentences you’re not allowed to make any changes to sentences\n",
    "    > from given `{context}`.\n",
    "\n",
    "2. Compute the context relevance score = (number of extracted sentences) / (total number of sentences in context)\n",
    "\n",
    "Moving from the initial paper to the active evaluation library ragas there are a few more insightful metrics to evaluate. From the library [source](https://docs.ragas.io/en/stable/concepts/metrics/index.html) let's introduce `context precision` and `context recall`. \n",
    "\n",
    "### Context recall\n",
    "Context can be said to have high recall if retrieved context aligns with the ground truth answer.\n",
    "\n",
    "#### Mathematically:\n",
    "\n",
    "$$\n",
    "Context\\ recall = \\frac{Ground\\ Truth\\ sentences\\ that\\ can\\ be\\ attributed\\ to\\ context}{Total\\ number\\ of\\ sentences\\ in\\ the\\ ground\\ truth}\n",
    "$$\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "Data:\n",
    "> question: Where is France and what is it’s capital?\n",
    "> ground truth answer: France is in Western Europe and its capital is Paris.\n",
    "> context: France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and the vast Palace of Versailles attest to its rich history.\n",
    ">\n",
    "> Note: ground truth answer can be created by critic LLM of with own human labeled data set.\n",
    "\n",
    "Step 1 - use an LLM to break the ground truth down into individual statements:\n",
    "- `France is in Western Europe`\n",
    "- `Its capital is Paris`\n",
    "\n",
    "Step 2 - for each ground truth statement, use an LLM to determine if it can be attributed from the context.\n",
    "- `France is in Western Europe` => yes\n",
    "- `Its capital is Paris` => no\n",
    "\n",
    "\n",
    "Step 3 - plug in to formula\n",
    "\n",
    "context recall = (1 + 0) / 2 = 0.5\n",
    "\n",
    "### Context precision\n",
    "\n",
    "This metrics relates to how chunks are ranked in a response. Ideally the most relevant chunks are at the top.\n",
    "\n",
    "#### Mathematically:\n",
    "\n",
    "$$\n",
    "Context\\ Precision@k = \\frac{precision@k}{total\\ number\\ relevant\\ items\\ in\\ the\\ top\\ k\\ results}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Precision@k = \\frac{true\\ positive@k}{true\\ positives@k + false\\ positives@k}\n",
    "$$\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "Data:\n",
    "> Question: Where is France and what is it’s capital?\n",
    "> \n",
    "> Ground truth: France is in Western Europe and its capital is Paris.\n",
    "> \n",
    "> Context: [ “The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and”, “France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower”]\n",
    "\n",
    "Step 1 - for each chunk use the LLM to check if it's relevant or not to the ground truth answer.\n",
    "\n",
    "Step 2 - for each chunk in the context calculate the precision defined as: ``\n",
    "- `“The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and”` => precision = 0/1 or 0.\n",
    "- `“France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower”` => the precision would be (1) / (1 true positive + 1 false positive) = 0.5. \n",
    "\n",
    "\n",
    "Step 3 - calculate the overall context precision = (0 + 0.5) / 1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement alternative chain for comparison: Parent Document Retriever\n",
    "\n",
    "Now that we've established baseline metrics let's implement a chain using the parent document retriever approach to see how the retrieval strategies compare.\n",
    "\n",
    "The parent document retriever attempts to optimize two competing objectives within RAG:\n",
    "\n",
    "1. smaller chunks can lead to better embeddings since there is less context to lose the point (so to speak) \n",
    "2. larger chunks help retain what could be valuable overall context to retrieval. \n",
    "\n",
    "In theory, this approach allows for the initial query search on smaller chunks for specificity but returns the larger chunks for more complete context.\n",
    "\n",
    "Let's implement and see if it improves our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import TextLoader, UnstructuredFileLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.redis import Redis as LangChainRedis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will make a new index for this example defined directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "PARENT_CHUNK_SIZE = 2500\n",
    "CHILD_CHUNK_SIZE = 500\n",
    "\n",
    "# This text splitter is used to create the parent documents aka larger chunks\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=PARENT_CHUNK_SIZE)\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHILD_CHUNK_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# embeddings for redis vector store\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: it is **critical** that our index includes the `doc_id` field otherwise the parent document linking will not happen correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with langchain we can manually modify the default vector schema configuration\n",
    "vector_schema = {\n",
    "    \"name\": \"chunk_vector\",        # name of the vector field in langchain\n",
    "    \"algorithm\": \"HNSW\",           # could use HNSW instead\n",
    "    \"dims\": 384,                   # set based on the HF model embedding dimension\n",
    "    \"distance_metric\": \"COSINE\",   # could use EUCLIDEAN or IP\n",
    "    \"datatype\": \"FLOAT32\",\n",
    "}\n",
    "\n",
    "# here we can define the entire schema spec for our index in LangChain\n",
    "index_schema = {\n",
    "    \"vector\": [vector_schema],\n",
    "    \"text\": [{\"name\": \"content\"}, {\"name\": \"doc_id\"}],\n",
    "    \"content_vector_key\": \"chunk_vector\" ,   # name of the vector field in langchain\n",
    "}\n",
    "\n",
    "vector_store = LangChainRedis(\n",
    "    REDIS_URL,\n",
    "    \"child_docs\",\n",
    "    embeddings,\n",
    "    index_schema=index_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage.encoder_backed import EncoderBackedStore\n",
    "from langchain.storage import RedisStore\n",
    "import pickle\n",
    "\n",
    "def key_encoder(key: int | str) -> str:\n",
    "    return str(key)\n",
    "\n",
    "def value_serializer(value: float) -> str:\n",
    "    return pickle.dumps(value)\n",
    "\n",
    "def value_deserializer(serialized_value: str) -> float:\n",
    "    return pickle.loads(serialized_value)\n",
    "\n",
    "# Create an instance of the abstract store\n",
    "base_store = RedisStore(redis_url=\"redis://localhost:6379\", namespace=\"parent_docs\")\n",
    "\n",
    "# Create an instance of the encoder-backed store\n",
    "encoder_store = EncoderBackedStore(\n",
    "    store=base_store,\n",
    "    key_encoder=key_encoder,\n",
    "    value_serializer=value_serializer,\n",
    "    value_deserializer=value_deserializer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "parent_doc_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=encoder_store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we are adding the source documents and the ParentDocumentRetriever will automatically split them into parent and child documents\n",
    "parent_doc_retriever.add_documents(source_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '../resources/filings/AAPL/AAPL-2023-10K.pdf'}, page_content='2023\\n\\n2022\\n\\nGross margin:\\n\\nProducts Services\\n\\nTotal gross margin\\n\\n$\\n\\n$\\n\\n108,803 $ 60,345 169,148 $\\n\\n114,728 $ 56,054 170,782 $\\n\\nGross margin percentage:\\n\\nProducts Services\\n\\nTotal gross margin percentage\\n\\n36.5 % 70.8 % 44.1 %\\n\\n36.3 % 71.7 % 43.3 %\\n\\nProducts Gross Margin\\n\\nProducts gross margin decreased during 2023 compared to 2022 due to the weakness in foreign currencies relative to the U.S. dollar and lower Products volume, partially oﬀset by cost savings and a diﬀerent Products mix.\\n\\nProducts gross margin percentage increased during 2023 compared to 2022 due to cost savings and a diﬀerent Products mix, partially oﬀset by the weakness in foreign currencies relative to the U.S. dollar and decreased leverage.\\n\\nServices Gross Margin\\n\\nServices gross margin increased during 2023 compared to 2022 due primarily to higher Services net sales, partially oﬀset by the weakness in foreign currencies relative to the U.S. dollar and higher Services costs.\\n\\nServices gross margin percentage decreased during 2023 compared to 2022 due to higher Services costs and the weakness in foreign currencies relative to the U.S. dollar, partially oﬀset by a diﬀerent Services mix.\\n\\nThe Company’s future gross margins can be impacted by a variety of factors, as discussed in Part I, Item 1A of this Form 10-K under the heading “Risk Factors.” As a result, the Company believes, in general, gross margins will be subject to volatility and downward pressure.\\n\\nOperating Expenses\\n\\nOperating expenses for 2023, 2022 and 2021 were as follows (dollars in millions):\\n\\n2023\\n\\nChange\\n\\n2022\\n\\nChange\\n\\nResearch and development\\n\\n$\\n\\n29,915\\n\\n14 % $\\n\\n26,251\\n\\n20 % $\\n\\nPercentage of total net sales Selling, general and administrative Percentage of total net sales\\n\\n$\\n\\n8 %\\n\\n24,932\\n\\n7 %\\n\\n(1)% $\\n\\n7 %\\n\\n25,094\\n\\n6 %\\n\\n14 % $\\n\\nTotal operating expenses\\n\\n$\\n\\n54,847\\n\\n7 % $\\n\\n51,345\\n\\n17 % $\\n\\nPercentage of total net sales\\n\\n14 %\\n\\n13 %\\n\\nResearch and Development\\n\\nThe year-over-year growth in R&D expense in 2023 was driven primarily by increases in headcount-related expenses.\\n\\nSelling, General and Administrative\\n\\nSelling, general and administrative expense was relatively ﬂat in 2023 compared to 2022.\\n\\nApple Inc. | 2023 Form 10-K | 23\\n\\n2021\\n\\n105,126 47,710 152,836\\n\\n35.3 % 69.7 % 41.8 %\\n\\n2021\\n\\n21,914\\n\\n6 %\\n\\n21,973\\n\\n6 %\\n\\n43,887\\n\\n12 %\\n\\nProvision for Income Taxes\\n\\nProvision for income taxes, eﬀective tax rate and statutory federal income tax rate for 2023, 2022 and 2021 were as follows (dollars in millions):\\n\\n2023\\n\\n2022')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test that the retirever works\n",
    "retrieved_docs = parent_doc_retriever.invoke(\"apples's revenue 2023\")\n",
    "retrieved_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the same but use our new retriever\n",
    "parent_doc_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=parent_doc_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": get_prompt()},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Like before let's first create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = create_evaluation_dataset(parent_doc_qa, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What services does Apple offer through its Pay...</td>\n",
       "      <td>I don't know.\\n    Source: None (I couldn't f...</td>\n",
       "      <td>[Rest of Asia Paciﬁc\\n\\nRest of Asia Paciﬁc ne...</td>\n",
       "      <td>Apple offers payment services through Apple Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the estimated maximum one-day loss in ...</td>\n",
       "      <td>$1.0 billion\\n    Source: Apple Inc. | 2023 F...</td>\n",
       "      <td>[The Company applied a value-at-risk (“VAR”) m...</td>\n",
       "      <td>$1.0 billion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What drives Apple Inc.'s competitive edge &amp; ho...</td>\n",
       "      <td>Apple Inc.'s competitive edge is driven by it...</td>\n",
       "      <td>[The Company’s ability to compete successfully...</td>\n",
       "      <td>The information provided in the context sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are potential risks for Apple if it doesn...</td>\n",
       "      <td>\\n    Source: \\n\\nPlease provide the answer a...</td>\n",
       "      <td>[Critical Accounting Estimates\\n\\nThe preparat...</td>\n",
       "      <td>If Apple fails to meet regulatory expectations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What factors contributed to the 7% boost in iP...</td>\n",
       "      <td>I don't know.\\n    Source: None\\n\\nQuestion: ...</td>\n",
       "      <td>[Rest of Asia Paciﬁc\\n\\nRest of Asia Paciﬁc ne...</td>\n",
       "      <td>The 7% boost in iPhone sales was primarily due...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What services does Apple offer through its Pay...   \n",
       "1  What is the estimated maximum one-day loss in ...   \n",
       "2  What drives Apple Inc.'s competitive edge & ho...   \n",
       "3  What are potential risks for Apple if it doesn...   \n",
       "4  What factors contributed to the 7% boost in iP...   \n",
       "\n",
       "                                              answer  \\\n",
       "0   I don't know.\\n    Source: None (I couldn't f...   \n",
       "1   $1.0 billion\\n    Source: Apple Inc. | 2023 F...   \n",
       "2   Apple Inc.'s competitive edge is driven by it...   \n",
       "3   \\n    Source: \\n\\nPlease provide the answer a...   \n",
       "4   I don't know.\\n    Source: None\\n\\nQuestion: ...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Rest of Asia Paciﬁc\\n\\nRest of Asia Paciﬁc ne...   \n",
       "1  [The Company applied a value-at-risk (“VAR”) m...   \n",
       "2  [The Company’s ability to compete successfully...   \n",
       "3  [Critical Accounting Estimates\\n\\nThe preparat...   \n",
       "4  [Rest of Asia Paciﬁc\\n\\nRest of Asia Paciﬁc ne...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  Apple offers payment services through Apple Ca...  \n",
       "1                                       $1.0 billion  \n",
       "2  The information provided in the context sugges...  \n",
       "3  If Apple fails to meet regulatory expectations...  \n",
       "4  The 7% boost in iPhone sales was primarily due...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8469c0d592d741409e84c8b7ed7a9c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "\n",
    "parent_doc_faithfulness_metrics = evaluate_dataset(eval_dataset, [faithfulness], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d151e4102701493e966fb57cac24d352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n"
     ]
    }
   ],
   "source": [
    "parent_doc_answer_relevancy_metrics = evaluate_dataset(eval_dataset, [answer_relevancy], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>parent_doc_faithfulness</th>\n",
       "      <th>parent_doc_answer_relevancy</th>\n",
       "      <th>parent_doc_answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.551016</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.591196</td>\n",
       "      <td>0.591196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.384250</td>\n",
       "      <td>0.247758</td>\n",
       "      <td>0.330661</td>\n",
       "      <td>0.330661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194516</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.301259</td>\n",
       "      <td>0.301259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.638030</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.653231</td>\n",
       "      <td>0.653231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846860</td>\n",
       "      <td>0.846860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfulness  answer_relevancy  parent_doc_faithfulness  \\\n",
       "count      9.000000         19.000000                 8.000000   \n",
       "mean       0.833333          0.551016                 0.718750   \n",
       "std        0.353553          0.384250                 0.247758   \n",
       "min        0.000000          0.000000                 0.500000   \n",
       "25%        1.000000          0.194516                 0.500000   \n",
       "50%        1.000000          0.638030                 0.625000   \n",
       "75%        1.000000          0.863987                 1.000000   \n",
       "max        1.000000          0.995077                 1.000000   \n",
       "\n",
       "       parent_doc_answer_relevancy  parent_doc_answer_relevancy  \n",
       "count                    18.000000                    18.000000  \n",
       "mean                      0.591196                     0.591196  \n",
       "std                       0.330661                     0.330661  \n",
       "min                       0.000000                     0.000000  \n",
       "25%                       0.301259                     0.301259  \n",
       "50%                       0.653231                     0.653231  \n",
       "75%                       0.846860                     0.846860  \n",
       "max                       1.000000                     1.000000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_doc_gen_metrics = parent_doc_faithfulness_metrics\n",
    "parent_doc_gen_metrics[\"answer_relevancy\"] = parent_doc_answer_relevancy_metrics[\"answer_relevancy\"]\n",
    "\n",
    "parent_doc_gen_metrics.rename(columns={\"faithfulness\": \"parent_doc_faithfulness\", \"answer_relevancy\": \"parent_doc_answer_relevancy\"}, inplace=True)\n",
    "\n",
    "overall_gen_metrics = pd.concat([gen_metrics_default, parent_doc_gen_metrics], axis=1)\n",
    "overall_gen_metrics.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the same for the retrieval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2052e30e0c794ce49a4b29e01cf4fa21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import context_recall, context_precision\n",
    "\n",
    "parent_doc_context_recall_metrics = evaluate_dataset(eval_dataset, [context_recall], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddfc521d17149bf90af6b6857efadac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n",
      "Exception raised in Job[2]: ValidationError(2 validation errors for ContextPrecisionVerifications\n",
      "__root__ -> 1 -> reason\n",
      "  field required (type=value_error.missing)\n",
      "__root__ -> 1 -> verdict\n",
      "  field required (type=value_error.missing))\n"
     ]
    }
   ],
   "source": [
    "parent_doc_context_precision_metrics = evaluate_dataset(eval_dataset, [context_precision], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>parent_doc_context_recall</th>\n",
       "      <th>parent_doc_context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.849415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.427491</td>\n",
       "      <td>0.311283</td>\n",
       "      <td>0.427491</td>\n",
       "      <td>0.329494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       context_recall  context_precision  parent_doc_context_recall  \\\n",
       "count       19.000000          19.000000                  19.000000   \n",
       "mean         0.728070           0.815789                   0.728070   \n",
       "std          0.427491           0.311283                   0.427491   \n",
       "min          0.000000           0.000000                   0.000000   \n",
       "25%          0.416667           0.708333                   0.416667   \n",
       "50%          1.000000           1.000000                   1.000000   \n",
       "75%          1.000000           1.000000                   1.000000   \n",
       "max          1.000000           1.000000                   1.000000   \n",
       "\n",
       "       parent_doc_context_precision  \n",
       "count                     19.000000  \n",
       "mean                       0.849415  \n",
       "std                        0.329494  \n",
       "min                        0.000000  \n",
       "25%                        0.958333  \n",
       "50%                        1.000000  \n",
       "75%                        1.000000  \n",
       "max                        1.000000  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_doc_ret_metrics = parent_doc_context_recall_metrics\n",
    "parent_doc_ret_metrics[\"context_precision\"] = parent_doc_context_precision_metrics[\"context_precision\"]\n",
    "\n",
    "parent_doc_ret_metrics.rename(columns={\"context_precision\": \"parent_doc_context_precision\", \"context_recall\": \"parent_doc_context_recall\"}, inplace=True)\n",
    "\n",
    "overall_ret_metrics = pd.concat([ret_metrics_default, parent_doc_ret_metrics], axis=1)\n",
    "\n",
    "overall_ret_metrics.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "In this case, we observe that the increased context provided by the parent document retriever had a slightly negative effect on the generation metrics, potentially reducing answer clarity via increased information. Basically no effect on context recall and a slightly positive effect on context precision, indicating that the smaller chunks for query comparison helped order the relevant context, but it appears that wasn't a limiting factor from the base case for this test. More conclusive testing would be needed to draw more authoritative conclusions, but this example shows us how to compare options in order to find the highest priority strategies for a given application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "\n",
    "In this notebook we covered:\n",
    "- why it's important to have an evaluation framework\n",
    "- the basic theory of RAGAS\n",
    "- how to calculate and generate faithfulness, answer_relevancy, context_precision, and context_recall\n",
    "- code to evaluate two different RAG chains to monitor how using a different retrieval strategy effects performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps: end-to-end evaluation\n",
    "\n",
    "As your pipeline matures and human labeled ground truth data is created the following metrics can be added for increased rigor. These additional metrics can be implemented similarly as the ones showcased above.\n",
    "\n",
    "\n",
    "## Answer correctness\n",
    "\n",
    "A weighted average of semantic and factual similarity where weights can be passed as a parameter.\n",
    "\n",
    "## Answer semantic similarity\n",
    "\n",
    "Measure distance between ground truth and the generated answer.\n",
    "\n",
    "#### Example process:\n",
    "- vectorize the ground truth answer and the generated answer\n",
    "- compute the cosine similarity.\n",
    "\n",
    "## Answer factual similarity\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "F1\\ Score = \\frac{TP}{TP + 0.5(FP + FN)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "TP (True Positive): Facts or statements that are present in both the ground truth and the generated answer.\n",
    "\n",
    "FP (False Positive): Facts or statements that are present in the generated answer but not in the ground truth.\n",
    "\n",
    "FN (False Negative): Facts or statements that are present in the ground truth but not in the generated answer.\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "data:\n",
    "> Ground truth: Einstein was born in 1879 in Germany.\n",
    "> Generated Answer: Einstein was born in Spain in 1879.\n",
    "\n",
    "TP: [Einstein was born in 1879]\n",
    "\n",
    "FP: [Einstein was born in Spain]\n",
    "\n",
    "FN: [Einstein was born in Germany]\n",
    "\n",
    "F1 = (1 / 1 + 0.5(1 + 1)) = 1/2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
