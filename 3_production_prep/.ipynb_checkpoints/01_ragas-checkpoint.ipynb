{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <div><img src=\"../assets/redis_logo.svg\" style=\"width: 130px\"> </div>\n",
    "    <div style=\"display: inline-block; text-align: center; margin-bottom: 10px;\">\n",
    "        <span style=\"font-size: 36px;\"><b>Evaluation with RAGAS</b></span>\n",
    "        <br />\n",
    "    </div>\n",
    "    <br />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG\n",
    "\n",
    "The extent to which you can **evaluate** your system is the extent to which you can **improve** your system. Before going to prod, it is in your best interest to establish a framework for quickly and effectively understanding the quality of your RAG application. In this notebook, we will use the RAGAS framework, as proposed by [this paper](https://arxiv.org/pdf/2309.15217), to evaluate our RAG application.\n",
    "\n",
    "Before we dive into the theory though, let's setup the necessary environment and basic RAG application for evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv (from -r requirements.txt (line 2))\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting tiktoken (from -r requirements.txt (line 3))\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting langchain (from -r requirements.txt (line 4))\n",
      "  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community (from -r requirements.txt (line 5))\n",
      "  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting sentence-transformers (from -r requirements.txt (line 7))\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
      "Collecting pdf2image (from -r requirements.txt (line 9))\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting spacy (from -r requirements.txt (line 10))\n",
      "  Downloading spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting langgraph (from -r requirements.txt (line 11))\n",
      "  Downloading langgraph-0.1.19-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting redis (from -r requirements.txt (line 12))\n",
      "  Downloading redis-5.0.8-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting redisvl (from -r requirements.txt (line 13))\n",
      "  Downloading redisvl-0.3.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting langchain-openai (from -r requirements.txt (line 14))\n",
      "  Downloading langchain_openai-0.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting openai (from -r requirements.txt (line 15))\n",
      "  Downloading openai-1.39.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting ragas (from -r requirements.txt (line 17))\n",
      "  Downloading ragas-0.1.13-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting vllm (from -r requirements.txt (line 18))\n",
      "  Downloading vllm-0.5.4-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting llama-index (from -r requirements.txt (line 21))\n",
      "  Downloading llama_index-0.10.61-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-embeddings-huggingface (from -r requirements.txt (line 22))\n",
      "  Downloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl.metadata (769 bytes)\n",
      "Collecting llama-index-llms-ollama (from -r requirements.txt (line 23))\n",
      "  Downloading llama_index_llms_ollama-0.2.2-py3-none-any.whl.metadata (668 bytes)\n",
      "Collecting llama-index-embeddings-instructor (from -r requirements.txt (line 24))\n",
      "  Downloading llama_index_embeddings_instructor-0.1.3-py3-none-any.whl.metadata (810 bytes)\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.15.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 3)) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (4.0.3)\n",
      "Collecting langchain-core<0.3.0,>=0.2.27 (from langchain->-r requirements.txt (line 4))\n",
      "  Downloading langchain_core-0.2.28-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->-r requirements.txt (line 4))\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->-r requirements.txt (line 4))\n",
      "  Downloading langsmith-0.1.98-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (1.22.4)\n",
      "Collecting pydantic<3,>=1 (from langchain->-r requirements.txt (line 4))\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (8.3.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community->-r requirements.txt (line 5))\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: chardet in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (5.2.0)\n",
      "Collecting filetype (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading lxml-5.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (4.12.3)\n",
      "Collecting emoji (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting python-iso639 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rapidfuzz (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading rapidfuzz-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting backoff (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (4.12.1)\n",
      "Collecting unstructured-client (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_client-0.25.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from unstructured[pdf]->-r requirements.txt (line 6)) (5.9.8)\n",
      "Collecting onnx (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting pdfminer.six (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pikepdf (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pikepdf-9.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Collecting pillow-heif (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pillow_heif-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
      "Collecting pypdf (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting pytesseract (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting google-cloud-vision (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting effdet (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting unstructured-inference==0.7.36 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.36-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting layoutparser (from unstructured-inference==0.7.36->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting python-multipart (from unstructured-inference==0.7.36->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting huggingface-hub (from unstructured-inference==0.7.36->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting opencv-python!=4.7.0.68 (from unstructured-inference==0.7.36->unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "INFO: pip is looking at multiple versions of unstructured-inference to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.15.0-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading unstructured-0.14.10-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading unstructured-0.14.9-py3-none-any.whl.metadata (28 kB)\n",
      "  Downloading unstructured-0.14.8-py3-none-any.whl.metadata (28 kB)\n",
      "  Downloading unstructured-0.14.7-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting unstructured-inference==0.7.35 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.35-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.14.6-py3-none-any.whl.metadata (28 kB)\n",
      "  Downloading unstructured-0.14.5-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting unstructured-inference==0.7.33 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.33-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: pip is still looking at multiple versions of unstructured-inference to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.14.4-py3-none-any.whl.metadata (28 kB)\n",
      "  Downloading unstructured-0.14.3-py3-none-any.whl.metadata (31 kB)\n",
      "  Downloading unstructured-0.14.2-py3-none-any.whl.metadata (31 kB)\n",
      "  Downloading unstructured-0.14.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting unstructured-inference==0.7.31 (from unstructured[pdf]->-r requirements.txt (line 6))\n",
      "  Downloading unstructured_inference-0.7.31-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting unstructured[pdf] (from -r requirements.txt (line 6))\n",
      "  Downloading unstructured-0.13.7-py3-none-any.whl.metadata (30 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# mute warnings\u001b[39;00m\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLANGCHAIN_TRACING_V2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import dotenv\n",
    "# mute warnings\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')\n",
    "# load env vars from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "parent_directory = os.path.dirname(dir_path)\n",
    "os.environ[\"ROOT_DIR\"] = parent_directory\n",
    "\n",
    "#setting the local downloaded sentence transformer models f\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{parent_directory}/models\"\n",
    "SCHEMA_PATH = f\"{parent_directory}/2_RAG_patterns_with_redis/sec_index.yaml\"\n",
    "SOURCE_DOC = '../resources/filings/AAPL/AAPL-2023-10K.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Redis and create chunks to populate the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init Redis connection and index\n",
    "import os\n",
    "from redisvl.index import SearchIndex\n",
    "from redis import Redis\n",
    "\n",
    "# init Redis connection\n",
    "# Replace values below with your own if using Redis Cloud instance\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "\n",
    "prefix = 'chunk'\n",
    "client = Redis.from_url(REDIS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", cache_folder=os.getenv(\"TRANSFORMERS_CACHE\", f\"{parent_directory}/models\"))\n",
    "\n",
    "loader = UnstructuredFileLoader(SOURCE_DOC, mode=\"single\", strategy=\"fast\")\n",
    "\n",
    "# for use later with parent-doc index\n",
    "source_doc = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "chunks = loader.load_and_split(text_splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_objs = [\n",
    "    {\n",
    "        \"chunk_id\": f\"{chunk.metadata['source']}-{str(uuid.uuid4())}\",\n",
    "        \"source_doc\": f\"{chunk.metadata['source']}\",\n",
    "        \"content\": chunk.page_content,\n",
    "        \"doc_type\": \"10k\",\n",
    "        \"text_embedding\": np.array(embeddings.embed_query(chunk.page_content)).astype(np.float32).tobytes()\n",
    "    }\n",
    "    for chunk in chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.schema import IndexSchema\n",
    "\n",
    "index_name = 'eval'\n",
    "\n",
    "schema = IndexSchema.from_dict(\n",
    "    {\n",
    "        \"index\": {\n",
    "            \"name\": index_name,\n",
    "            \"prefix\": prefix,\n",
    "            \"storage_type\": \"hash\",\n",
    "        },\n",
    "        \"fields\": [\n",
    "            {\"name\": \"chunk_id\", \"type\": \"tag\"},\n",
    "            {\"name\": \"source_doc\", \"type\": \"tag\"},\n",
    "            {\"name\": \"doc_type\", \"type\": \"tag\"},\n",
    "            {\"name\": \"content\", \"type\": \"text\"},\n",
    "            {\n",
    "                \"name\": \"text_embedding\", \n",
    "                \"type\": \"vector\", \n",
    "                \"attrs\": {\"type\": \"float32\", \"dims\": 384, \"distance_metric\": \"COSINE\", \"algorithm\": \"flat\"},\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# create an index from schema and the client\n",
    "index = SearchIndex(schema, client)\n",
    "index.create(overwrite=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = index.load(index_objs, id_field=\"chunk_id\")\n",
    "len(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vector store\n",
    "This is the same processes as we have done in the previous examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Redis as LangChainRedis\n",
    "\n",
    "\n",
    "# with langchain we can manually modify the default vector schema configuration\n",
    "vector_schema = {\n",
    "    \"name\": \"text_embedding\",        # name of the vector field in langchain\n",
    "    \"algorithm\": \"HNSW\",           # could use HNSW instead\n",
    "    \"dims\": 384,                   # set based on the HF model embedding dimension\n",
    "    \"distance_metric\": \"COSINE\",   # could use EUCLIDEAN or IP\n",
    "    \"datatype\": \"FLOAT32\",\n",
    "}\n",
    "\n",
    "# here we can define the entire schema spec for our index in LangChain\n",
    "index_schema = {\n",
    "    \"vector\": [vector_schema],\n",
    "    \"text\": [{\"name\": \"content\"}, {\"name\": \"source_doc\"}, {\"name\": \"doc_type\"}, {\"name\": \"chunk_id\"}],\n",
    "    \"content_vector_key\": \"text_embedding\" ,   # name of the vector field in langchain\n",
    "}\n",
    "\n",
    "\n",
    "rds = LangChainRedis.from_existing_index(\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name,\n",
    "    schema=index_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out!\n",
    "We can see the vector store is populated and returning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Apple Inc. | 2023 Form 10-K | 27\\n\\nPage\\n\\n28\\n\\n29 30\\n\\n31\\n\\n32 33 49\\n\\nApple Inc.\\n\\nCONSOLIDATED STATEMENTS OF OPERATIONS (In millions, except number of shares, which are reﬂected in thousands, and per-share amounts)\\n\\nYears ended\\n\\nSeptember 30, 2023\\n\\nSeptember 24, 2022\\n\\nSeptember 25, 2021\\n\\nNet sales: Products Services\\n\\nTotal net sales\\n\\n$\\n\\n298,085 $ 85,200 383,285\\n\\n316,199 $ 78,129 394,328\\n\\n297,392 68,425 365,817\\n\\nCost of sales: Products Services\\n\\nTotal cost of sales Gross margin\\n\\n189,282 24,855 214,137 169,148\\n\\n201,471 22,075 223,546 170,782\\n\\n192,266 20,715 212,981 152,836\\n\\nOperating expenses:\\n\\nResearch and development Selling, general and administrative\\n\\nTotal operating expenses\\n\\n29,915 24,932 54,847\\n\\n26,251 25,094 51,345\\n\\n21,914 21,973 43,887\\n\\nOperating income Other income/(expense), net Income before provision for income taxes Provision for income taxes Net income\\n\\n$\\n\\n114,301 (565) 113,736 16,741 96,995 $\\n\\n119,437 (334) 119,103 19,300 99,803 $\\n\\n108,949 258 109,207 14,527 94,680', metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10K.pdf-4ebd05cb-df0d-490f-a55d-243fd02a61ad', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10K.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10K.pdf-4ebd05cb-df0d-490f-a55d-243fd02a61ad'})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rds.similarity_search(\"What was apples revenue last year?\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup RAG\n",
    "\n",
    "Initialize llm examples shown for Ollama, OpenAI, and VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama, VLLMOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "# for Ollama use => increase context window\n",
    "# llm = Ollama(model=\"llama3\", num_ctx=4097, temperature=0.1)\n",
    "\n",
    "llm = VLLMOpenAI(\n",
    "            openai_api_key=os.environ[\"HF_MODEL_HUB_TOKEN\"], # vllm token key for huggingface through openai like interface\n",
    "            openai_api_base=os.environ[\"VLLM_URL\"],\n",
    "            model_name=os.environ[\"LOCAL_VLLM_MODEL\"],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     model=\"gpt-3.5-turbo-16k\",\n",
    "#     max_tokens=None\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt():\n",
    "    \"\"\"Create the QA chain.\"\"\"\n",
    "    from langchain.prompts import PromptTemplate\n",
    "\n",
    "    # Define our prompt\n",
    "    prompt_template = \"\"\"Use the following pieces of context from financial 10k filings data to answer the user question at the end. Only use the result from tools and evidence provided to you. If you don't know the answer, say that you don't know, don't try to make up an answer. Provide the source of the document that you used to get the answer.\n",
    "\n",
    "    This should be in the following format:\n",
    "\n",
    "    Question: [question here]\n",
    "    Answer: [answer here]\n",
    "    Source: [source document here]\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Context:\n",
    "    ---------\n",
    "    {context}\n",
    "    ---------\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def get_search_kwargs(filters, distance_threshold):\n",
    "    return {\"distance_threshold\":distance_threshold,\"filter\":filters}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=rds.as_retriever(search_type=\"similarity_distance_threshold\", search_kwargs={\"distance_threshold\":0.8, 'include_metadata': True}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": get_prompt()},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have our RAG QA to test out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': \"What was Apple's revenue last year compared to this year??\",\n",
       " 'result': \"Answer: Apple's revenue last year was $394.3 billion, while this year it was $383.3 billion.\\nSource: Apple Inc. | 2023 Form 10-K | 27\",\n",
       " 'source_documents': [Document(page_content='Apple Inc. | 2023 Form 10-K | 27\\n\\nPage\\n\\n28\\n\\n29 30\\n\\n31\\n\\n32 33 49\\n\\nApple Inc.\\n\\nCONSOLIDATED STATEMENTS OF OPERATIONS (In millions, except number of shares, which are reﬂected in thousands, and per-share amounts)\\n\\nYears ended\\n\\nSeptember 30, 2023\\n\\nSeptember 24, 2022\\n\\nSeptember 25, 2021\\n\\nNet sales: Products Services\\n\\nTotal net sales\\n\\n$\\n\\n298,085 $ 85,200 383,285\\n\\n316,199 $ 78,129 394,328\\n\\n297,392 68,425 365,817\\n\\nCost of sales: Products Services\\n\\nTotal cost of sales Gross margin\\n\\n189,282 24,855 214,137 169,148\\n\\n201,471 22,075 223,546 170,782\\n\\n192,266 20,715 212,981 152,836\\n\\nOperating expenses:\\n\\nResearch and development Selling, general and administrative\\n\\nTotal operating expenses\\n\\n29,915 24,932 54,847\\n\\n26,251 25,094 51,345\\n\\n21,914 21,973 43,887\\n\\nOperating income Other income/(expense), net Income before provision for income taxes Provision for income taxes Net income\\n\\n$\\n\\n114,301 (565) 113,736 16,741 96,995 $\\n\\n119,437 (334) 119,103 19,300 99,803 $\\n\\n108,949 258 109,207 14,527 94,680', metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10K.pdf-4ebd05cb-df0d-490f-a55d-243fd02a61ad', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10K.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10K.pdf-4ebd05cb-df0d-490f-a55d-243fd02a61ad'}),\n",
       "  Document(page_content='Wearables, Home and Accessories net sales decreased 3% or $1.4 billion during 2023 compared to 2022 due primarily to lower net sales of Wearables and Accessories.\\n\\nServices\\n\\nServices net sales increased 9% or $7.1 billion during 2023 compared to 2022 due to higher net sales across all lines of business.\\n\\nApple Inc. | 2023 Form 10-K | 22\\n\\n2021\\n\\n191,973 35,190 31,862 38,367 68,425 365,817\\n\\nGross Margin\\n\\nProducts and Services gross margin and gross margin percentage for 2023, 2022 and 2021 were as follows (dollars in millions):\\n\\n2023\\n\\n2022\\n\\nGross margin:\\n\\nProducts Services\\n\\nTotal gross margin\\n\\n$\\n\\n$\\n\\n108,803 $ 60,345 169,148 $\\n\\n114,728 $ 56,054 170,782 $\\n\\nGross margin percentage:\\n\\nProducts Services\\n\\nTotal gross margin percentage\\n\\n36.5 % 70.8 % 44.1 %\\n\\n36.3 % 71.7 % 43.3 %\\n\\nProducts Gross Margin', metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10K.pdf-dba37dd1-94ba-467c-bba4-ce6f35a468d6', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10K.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10K.pdf-dba37dd1-94ba-467c-bba4-ce6f35a468d6'}),\n",
       "  Document(page_content='$ $\\n\\n24,257 $ 11,888 $\\n\\n25,977 $ 12,257 $\\n\\nRest of Asia Paciﬁc:\\n\\nNet sales Operating income\\n\\n$ $\\n\\n29,615 $ 12,066 $\\n\\n29,375 $ 11,569 $\\n\\nA reconciliation of the Company’s segment operating income to the Consolidated Statements of Operations for 2023, 2022 and 2021 is as follows (in millions):\\n\\n2023\\n\\n2022\\n\\nSegment operating income Research and development expense Other corporate expenses, net Total operating income\\n\\n(1)\\n\\n$\\n\\n$\\n\\n150,888 $ (29,915) (6,672) 114,301 $\\n\\n152,895 $ (26,251) (7,207) 119,437 $\\n\\n(1)\\n\\nIncludes corporate marketing expenses, certain share-based compensation expenses, various nonrecurring charges, and other separately managed general and administrative costs.\\n\\nApple Inc. | 2023 Form 10-K | 47\\n\\n2021\\n\\n153,306 53,382\\n\\n89,307 32,505\\n\\n68,366 28,504\\n\\n28,482 12,798\\n\\n26,356 9,817\\n\\n2021\\n\\n137,006 (21,914) (6,143) 108,949', metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10K.pdf-c7518c8b-42f5-40d7-8407-00bb272bfd65', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10K.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10K.pdf-c7518c8b-42f5-40d7-8407-00bb272bfd65'}),\n",
       "  Document(page_content='Fiscal Year Highlights\\n\\nThe Company’s total net sales were $383.3 billion and net income was $97.0 billion during 2023.\\n\\nThe Company’s total net sales decreased 3% or $11.0 billion during 2023 compared to 2022. The weakness in foreign currencies relative to the U.S. dollar accounted for more than the entire year-over-year decrease in total net sales, which consisted primarily of lower net sales of Mac and iPhone, partially oﬀset by higher net sales of Services.\\n\\nThe Company announces new product, service and software oﬀerings at various times during the year. Signiﬁcant announcements during ﬁscal year 2023 included the following:\\n\\nFirst Quarter 2023:\\n\\n• • MLS Season Pass, a Major League Soccer subscription streaming service.\\n\\niPad and iPad Pro; Next-generation Apple TV 4K; and\\n\\nSecond Quarter 2023:\\n\\nMacBook Pro 14”, MacBook Pro 16” and Mac mini; and • Second-generation HomePod.\\n\\nThird Quarter 2023:\\n\\nMacBook Air 15”, Mac Studio and Mac Pro; •', metadata={'id': 'chunk:../resources/filings/AAPL/AAPL-2023-10K.pdf-5cc5e059-67c8-4b76-9b88-37b9016a4946', 'source_doc': '../resources/filings/AAPL/AAPL-2023-10K.pdf', 'doc_type': '10k', 'chunk_id': '../resources/filings/AAPL/AAPL-2023-10K.pdf-5cc5e059-67c8-4b76-9b88-37b9016a4946'})]}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What was Apple's revenue last year compared to this year??\"\n",
    "res=qa(query)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup complete!\n",
    "\n",
    "In the resources we have included a pre-generated set of test data for evaluation generated with the TestsetGenerator class from the ragas library for demo speed sake. The code used to generate this data is provided as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "testset = pd.read_csv(\"resources/full_testset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TestsetGenerator example code for generate testset\n",
    "\n",
    "This can be a time consuming process so we have gone ahead and pregenerated this with the following code. See more on creating test sets [here](https://docs.ragas.io/en/latest/getstarted/testset_generation.html).\n",
    "\n",
    "Note: while we are using synthetic test set here RAGAS can be utilized with human labeled data and [self created test sets](https://docs.ragas.io/en/stable/howtos/applications/data_preparation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if problems with nltk data\n",
    "# import os\n",
    "# os.environ[\"NLTK_DATA\"] = '/Users/<user>/nltk_data'\n",
    "\n",
    "if not len(testset):\n",
    "    from ragas.testset.generator import TestsetGenerator\n",
    "    from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "    from llama_index.llms.ollama import Ollama\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "    generator_llm = llm\n",
    "    critic_llm = llm\n",
    "    embeddings = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "    generator = TestsetGenerator.from_llama_index(\n",
    "        generator_llm=generator_llm,\n",
    "        critic_llm=critic_llm,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "\n",
    "    reader = SimpleDirectoryReader(input_files=[SOURCE_DOC])\n",
    "\n",
    "    documents = reader.load_data()\n",
    "\n",
    "    testset = generator.generate_with_llamaindex_docs(\n",
    "        documents,\n",
    "        test_size=20,\n",
    "        distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
    "    )\n",
    "\n",
    "    testset.to_pandas().to_csv(\"full_testset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin evaluation\n",
    "[The ragas library](https://docs.ragas.io/en/stable/index.html) provides helpful classes for abstracting the complexity of creating test sets and evaluating apps that use generative technology. Above we demonstrated how the TestsetGenerator class can be used to create an example dataset with. Now we will create a few helper functions to store and aggregate the answers/ context generated/retrieved from the RAG QA app we defined earlier. This data will be what we pass to the ragas library for calculating our performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "def parse_contexts(source_docs):\n",
    "    return [doc.page_content for doc in source_docs]\n",
    "\n",
    "def create_evaluation_dataset(chain, testset):\n",
    "    res_set = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": [],\n",
    "        \"ground_truth\": []\n",
    "    }\n",
    "\n",
    "    for _, row in testset.iterrows():\n",
    "        # call QA chain\n",
    "        result = chain.invoke(row[\"question\"])\n",
    "\n",
    "        res_set[\"question\"].append(row[\"question\"])\n",
    "        res_set[\"answer\"].append(result[\"result\"])\n",
    "\n",
    "        contexts = parse_contexts(result[\"source_documents\"])\n",
    "        \n",
    "        if not len(contexts):\n",
    "            print(f\"no contexts found for question: {row['question']}\")\n",
    "        res_set[\"contexts\"].append(contexts)\n",
    "        res_set[\"ground_truth\"].append(str(row[\"ground_truth\"]))\n",
    "\n",
    "    return Dataset.from_dict(res_set)\n",
    "\n",
    "def evaluate_dataset(eval_dataset, metrics, llm, embeddings):\n",
    "\n",
    "    run_config = RunConfig()\n",
    "    run_config.max_retries = 1\n",
    "\n",
    "\n",
    "    eval_result = evaluate(\n",
    "        eval_dataset,\n",
    "        metrics=metrics,\n",
    "        run_config=run_config,\n",
    "        llm=llm,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "    eval_df = eval_result.to_pandas()\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset\n",
    "\n",
    "Input: chain to be evaluated, testset\n",
    "Output: dataset to pass to ragas evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = create_evaluation_dataset(qa, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What services does Apple offer through its Pay...</td>\n",
       "      <td>Apple offers two services through its Payment ...</td>\n",
       "      <td>[The Company operates various platforms, inclu...</td>\n",
       "      <td>Apple offers payment services through Apple Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the estimated maximum one-day loss in ...</td>\n",
       "      <td>The estimated maximum one-day loss in fair val...</td>\n",
       "      <td>[The Company applied a value-at-risk (“VAR”) m...</td>\n",
       "      <td>$1.0 billion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What drives Apple Inc.'s competitive edge &amp; ho...</td>\n",
       "      <td>The competitive edge of Apple Inc. is driven b...</td>\n",
       "      <td>[The Company has a minority market share in th...</td>\n",
       "      <td>The information provided in the context sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are potential risks for Apple if it doesn...</td>\n",
       "      <td>Answer: The potential risks for Apple if it do...</td>\n",
       "      <td>[Apple Inc. | 2023 Form 10-K | 7\\n\\nThe Compan...</td>\n",
       "      <td>If Apple fails to meet regulatory expectations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What factors contributed to the 7% boost in iP...</td>\n",
       "      <td>Answer: The information provided does not spec...</td>\n",
       "      <td>[(1) Products net sales include amortization o...</td>\n",
       "      <td>The 7% boost in iPhone sales was primarily due...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What services does Apple offer through its Pay...   \n",
       "1  What is the estimated maximum one-day loss in ...   \n",
       "2  What drives Apple Inc.'s competitive edge & ho...   \n",
       "3  What are potential risks for Apple if it doesn...   \n",
       "4  What factors contributed to the 7% boost in iP...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Apple offers two services through its Payment ...   \n",
       "1  The estimated maximum one-day loss in fair val...   \n",
       "2  The competitive edge of Apple Inc. is driven b...   \n",
       "3  Answer: The potential risks for Apple if it do...   \n",
       "4  Answer: The information provided does not spec...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [The Company operates various platforms, inclu...   \n",
       "1  [The Company applied a value-at-risk (“VAR”) m...   \n",
       "2  [The Company has a minority market share in th...   \n",
       "3  [Apple Inc. | 2023 Form 10-K | 7\\n\\nThe Compan...   \n",
       "4  [(1) Products net sales include amortization o...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  Apple offers payment services through Apple Ca...  \n",
       "1                                       $1.0 billion  \n",
       "2  The information provided in the context sugges...  \n",
       "3  If Apple fails to meet regulatory expectations...  \n",
       "4  The 7% boost in iPhone sales was primarily due...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate generation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa40b436862c4541b35270c817e690c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "\n",
    "# first generate for faithfulness\n",
    "faithfulness_metrics = evaluate_dataset(eval_dataset, [faithfulness], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f48ec27a2343fbb8e21e8f20a5b7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# next for answer_relevancy\n",
    "answer_relevancy_metrics = evaluate_dataset(eval_dataset, [answer_relevancy], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.554265</td>\n",
       "      <td>0.714271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.425378</td>\n",
       "      <td>0.383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.118056</td>\n",
       "      <td>0.788299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.887609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfulness  answer_relevancy\n",
       "count     19.000000         19.000000\n",
       "mean       0.554265          0.714271\n",
       "std        0.425378          0.383305\n",
       "min        0.000000          0.000000\n",
       "25%        0.118056          0.788299\n",
       "50%        0.642857          0.887609\n",
       "75%        1.000000          0.930006\n",
       "max        1.000000          1.000000"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_metrics_default = faithfulness_metrics\n",
    "gen_metrics_default[\"answer_relevancy\"] = answer_relevancy_metrics[\"answer_relevancy\"]\n",
    "\n",
    "gen_metrics_default.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do these number mean and how were they calculated?\n",
    "\n",
    "Note: the following examples are paraphrased from the [ragas docs](https://docs.ragas.io/en/stable/concepts/metrics/index.html)\n",
    "\n",
    "------\n",
    "\n",
    "### Faithfulness\n",
    "\n",
    "An answer to a question can be said to be \"faithful\" if the **claims** that are made in the answer **can be inferred** from the **context**.\n",
    "\n",
    "#### Mathematically:\n",
    "\n",
    "$$\n",
    "Faithfullness\\ score = \\frac{Number\\ of\\ claims\\ in\\ the\\ generated\\ answer\\ that\\ can\\ be\\ inferred\\ from\\ the\\ given\\ context}{Total\\ number\\ of\\ claim\\ in\\ the\\ generated\\ answer}\n",
    "$$\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "> Question: Where and when was Einstein born?\n",
    "> \n",
    "> Context: Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time\n",
    ">\n",
    "> answer: Einstein was born in Germany on 20th March 1879.\n",
    "\n",
    "Step 1: Use LLM to break generated answer into individual statements.\n",
    "- “Einstein was born in Germany.”\n",
    "- “Einstein was born on 20th March 1879.”\n",
    "\n",
    "Step 2: For each statement use LLM to verify if it can be inferred from the context.\n",
    "- “Einstein was born in Germany.” => yes. \n",
    "- “Einstein was born on 20th March 1879.” => no.\n",
    "\n",
    "Step 3: plug into formula\n",
    "\n",
    "Number of claims inferred from context = 1\n",
    "Total number of claims = 2\n",
    "Faithfulness = 1/2\n",
    "\n",
    "### Answer Relevance\n",
    "\n",
    "An answer can be said to be relevant if it directly addresses the question (intuitively).\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "1. Use an LLM to generate \"hypothetical\" questions to a given answer with the following prompt:\n",
    "\n",
    "    > Generate a question for the given answer.\n",
    "    > answer: [answer]\n",
    "\n",
    "2. Embed the generated \"hypothetical\" questions as vectors.\n",
    "3. Calculate the cosine similarity of the hypothetical questions and the original question, sum those similarities, and divide by n.\n",
    "\n",
    "With data:\n",
    "\n",
    "> Question: Where is France and what is it’s capital?\n",
    "> \n",
    "> answer: France is in western Europe.\n",
    "\n",
    "Step 1 - use LLM to create 'n' variants of question from the generated answer.\n",
    "\n",
    "- “In which part of Europe is France located?”\n",
    "- “What is the geographical location of France within Europe?”\n",
    "- “Can you identify the region of Europe where France is situated?”\n",
    "\n",
    "Step 2 - Calculate the mean cosine similarity between the generated questions and the actual question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate retrieval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107ac318abe34da0bb08b9b74b1b643f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.metrics import context_recall, context_precision\n",
    "\n",
    "context_recall_metrics = evaluate_dataset(eval_dataset, [context_recall], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1e2fbf8102459e8b564f6eade2caac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_precision_metrics = evaluate_dataset(eval_dataset, [context_precision], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.815789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.427491</td>\n",
       "      <td>0.311283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       context_recall  context_precision\n",
       "count       19.000000          19.000000\n",
       "mean         0.728070           0.815789\n",
       "std          0.427491           0.311283\n",
       "min          0.000000           0.000000\n",
       "25%          0.416667           0.708333\n",
       "50%          1.000000           1.000000\n",
       "75%          1.000000           1.000000\n",
       "max          1.000000           1.000000"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_metrics_default = context_recall_metrics\n",
    "ret_metrics_default[\"context_precision\"] = context_precision_metrics[\"context_precision\"]\n",
    "\n",
    "ret_metrics_default.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do these numbers mean?\n",
    "\n",
    "Retrieval metrics quantify how well the system performed at fetching the best possible context for generation. Like before please review the definitions below to understand what happens under-the-hood when we execute the evaluation code. \n",
    "\n",
    "-----\n",
    "\n",
    "### Context Relevance\n",
    "\n",
    "\"The context is considered relevant to the extent that it exclusively contains information that is needed to answer the question.\"\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "1. Use the following LLM prompt to extract a subset of sentences necessary to answer the question. The context is defined as the formatted search result from the vector database.\n",
    "\n",
    "    > Please extract relevant sentences from\n",
    "    > the provided context that can potentially\n",
    "    > help answer the following `{question}`. If no\n",
    "    > relevant sentences are found, or if you\n",
    "    > believe the question cannot be answered\n",
    "    > from the given context, return the phrase\n",
    "    > \"Insufficient Information\". While extracting candidate sentences you’re not allowed to make any changes to sentences\n",
    "    > from given `{context}`.\n",
    "\n",
    "2. Compute the context relevance score = (number of extracted sentences) / (total number of sentences in context)\n",
    "\n",
    "Moving from the initial paper to the active evaluation library ragas there are a few more insightful metrics to evaluate. From the library [source](https://docs.ragas.io/en/stable/concepts/metrics/index.html) let's introduce `context precision` and `context recall`. \n",
    "\n",
    "### Context recall\n",
    "Context can be said to have high recall if retrieved context aligns with the ground truth answer.\n",
    "\n",
    "#### Mathematically:\n",
    "\n",
    "$$\n",
    "Context\\ recall = \\frac{Ground\\ Truth\\ sentences\\ that\\ can\\ be\\ attributed\\ to\\ context}{Total\\ number\\ of\\ sentences\\ in\\ the\\ ground\\ truth}\n",
    "$$\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "Data:\n",
    "> question: Where is France and what is it’s capital?\n",
    "> ground truth answer: France is in Western Europe and its capital is Paris.\n",
    "> context: France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and the vast Palace of Versailles attest to its rich history.\n",
    ">\n",
    "> Note: ground truth answer can be created by critic LLM of with own human labeled data set.\n",
    "\n",
    "Step 1 - use an LLM to break the ground truth down into individual statements:\n",
    "- `France is in Western Europe`\n",
    "- `Its capital is Paris`\n",
    "\n",
    "Step 2 - for each ground truth statement, use an LLM to determine if it can be attributed from the context.\n",
    "- `France is in Western Europe` => yes\n",
    "- `Its capital is Paris` => no\n",
    "\n",
    "\n",
    "Step 3 - plug in to formula\n",
    "\n",
    "context recall = (1 + 0) / 2 = 0.5\n",
    "\n",
    "### Context precision\n",
    "\n",
    "This metrics relates to how chunks are ranked in a response. Ideally the most relevant chunks are at the top.\n",
    "\n",
    "#### Mathematically:\n",
    "\n",
    "$$\n",
    "Context\\ Precision@k = \\frac{precision@k}{total\\ number\\ relevant\\ items\\ in\\ the\\ top\\ k\\ results}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Precision@k = \\frac{true\\ positive@k}{true\\ positives@k + false\\ positives@k}\n",
    "$$\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "Data:\n",
    "> Question: Where is France and what is it’s capital?\n",
    "> \n",
    "> Ground truth: France is in Western Europe and its capital is Paris.\n",
    "> \n",
    "> Context: [ “The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and”, “France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower”]\n",
    "\n",
    "Step 1 - for each chunk use the LLM to check if it's relevant or not to the ground truth answer.\n",
    "\n",
    "Step 2 - for each chunk in the context calculate the precision defined as: ``\n",
    "- `“The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and”` => precision = 0/1 or 0.\n",
    "- `“France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower”` => the precision would be (1) / (1 true positive + 1 false positive) = 0.5. \n",
    "\n",
    "\n",
    "Step 3 - calculate the overall context precision = (0 + 0.5) / 1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement alternative chain for comparison: Parent Document Retriever\n",
    "\n",
    "Now that we've established baseline metrics let's implement a chain using the parent document retriever approach to see how the retrieval strategies compare.\n",
    "\n",
    "The parent document retriever attempts to optimize two competing objectives within RAG:\n",
    "\n",
    "1. smaller chunks can lead to better embeddings since there is less context to lose the point (so to speak) \n",
    "2. larger chunks help retain what could be valuable overall context to retrieval. \n",
    "\n",
    "In theory, this approach allows for the initial query search on smaller chunks for specificity but returns the larger chunks for more complete context.\n",
    "\n",
    "Let's implement and see if it improves our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import TextLoader, UnstructuredFileLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.redis import Redis as LangChainRedis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will make a new index for this example defined directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "PARENT_CHUNK_SIZE = 2500\n",
    "CHILD_CHUNK_SIZE = 500\n",
    "\n",
    "# This text splitter is used to create the parent documents aka larger chunks\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=PARENT_CHUNK_SIZE)\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHILD_CHUNK_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings for redis vector store\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: it is **critical** that our index includes the `doc_id` field otherwise the parent document linking will not happen correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with langchain we can manually modify the default vector schema configuration\n",
    "vector_schema = {\n",
    "    \"name\": \"chunk_vector\",        # name of the vector field in langchain\n",
    "    \"algorithm\": \"HNSW\",           # could use HNSW instead\n",
    "    \"dims\": 384,                   # set based on the HF model embedding dimension\n",
    "    \"distance_metric\": \"COSINE\",   # could use EUCLIDEAN or IP\n",
    "    \"datatype\": \"FLOAT32\",\n",
    "}\n",
    "\n",
    "# here we can define the entire schema spec for our index in LangChain\n",
    "index_schema = {\n",
    "    \"vector\": [vector_schema],\n",
    "    \"text\": [{\"name\": \"content\"}, {\"name\": \"doc_id\"}],\n",
    "    \"content_vector_key\": \"chunk_vector\" ,   # name of the vector field in langchain\n",
    "}\n",
    "\n",
    "vector_store = LangChainRedis(\n",
    "    REDIS_URL,\n",
    "    \"child_docs\",\n",
    "    embeddings,\n",
    "    index_schema=index_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage.encoder_backed import EncoderBackedStore\n",
    "from langchain.storage import RedisStore\n",
    "import pickle\n",
    "\n",
    "def key_encoder(key: int | str) -> str:\n",
    "    return str(key)\n",
    "\n",
    "def value_serializer(value: float) -> str:\n",
    "    return pickle.dumps(value)\n",
    "\n",
    "def value_deserializer(serialized_value: str) -> float:\n",
    "    return pickle.loads(serialized_value)\n",
    "\n",
    "# Create an instance of the abstract store\n",
    "base_store = RedisStore(redis_url=\"redis://localhost:6379\", namespace=\"parent_docs\")\n",
    "\n",
    "# Create an instance of the encoder-backed store\n",
    "encoder_store = EncoderBackedStore(\n",
    "    store=base_store,\n",
    "    key_encoder=key_encoder,\n",
    "    value_serializer=value_serializer,\n",
    "    value_deserializer=value_deserializer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "parent_doc_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=encoder_store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we are adding the source documents and the ParentDocumentRetriever will automatically split them into parent and child documents\n",
    "parent_doc_retriever.add_documents(source_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='2023\\n\\n2022\\n\\nGross margin:\\n\\nProducts Services\\n\\nTotal gross margin\\n\\n$\\n\\n$\\n\\n108,803 $ 60,345 169,148 $\\n\\n114,728 $ 56,054 170,782 $\\n\\nGross margin percentage:\\n\\nProducts Services\\n\\nTotal gross margin percentage\\n\\n36.5 % 70.8 % 44.1 %\\n\\n36.3 % 71.7 % 43.3 %\\n\\nProducts Gross Margin\\n\\nProducts gross margin decreased during 2023 compared to 2022 due to the weakness in foreign currencies relative to the U.S. dollar and lower Products volume, partially oﬀset by cost savings and a diﬀerent Products mix.\\n\\nProducts gross margin percentage increased during 2023 compared to 2022 due to cost savings and a diﬀerent Products mix, partially oﬀset by the weakness in foreign currencies relative to the U.S. dollar and decreased leverage.\\n\\nServices Gross Margin\\n\\nServices gross margin increased during 2023 compared to 2022 due primarily to higher Services net sales, partially oﬀset by the weakness in foreign currencies relative to the U.S. dollar and higher Services costs.\\n\\nServices gross margin percentage decreased during 2023 compared to 2022 due to higher Services costs and the weakness in foreign currencies relative to the U.S. dollar, partially oﬀset by a diﬀerent Services mix.\\n\\nThe Company’s future gross margins can be impacted by a variety of factors, as discussed in Part I, Item 1A of this Form 10-K under the heading “Risk Factors.” As a result, the Company believes, in general, gross margins will be subject to volatility and downward pressure.\\n\\nOperating Expenses\\n\\nOperating expenses for 2023, 2022 and 2021 were as follows (dollars in millions):\\n\\n2023\\n\\nChange\\n\\n2022\\n\\nChange\\n\\nResearch and development\\n\\n$\\n\\n29,915\\n\\n14 % $\\n\\n26,251\\n\\n20 % $\\n\\nPercentage of total net sales Selling, general and administrative Percentage of total net sales\\n\\n$\\n\\n8 %\\n\\n24,932\\n\\n7 %\\n\\n(1)% $\\n\\n7 %\\n\\n25,094\\n\\n6 %\\n\\n14 % $\\n\\nTotal operating expenses\\n\\n$\\n\\n54,847\\n\\n7 % $\\n\\n51,345\\n\\n17 % $\\n\\nPercentage of total net sales\\n\\n14 %\\n\\n13 %\\n\\nResearch and Development\\n\\nThe year-over-year growth in R&D expense in 2023 was driven primarily by increases in headcount-related expenses.\\n\\nSelling, General and Administrative\\n\\nSelling, general and administrative expense was relatively ﬂat in 2023 compared to 2022.\\n\\nApple Inc. | 2023 Form 10-K | 23\\n\\n2021\\n\\n105,126 47,710 152,836\\n\\n35.3 % 69.7 % 41.8 %\\n\\n2021\\n\\n21,914\\n\\n6 %\\n\\n21,973\\n\\n6 %\\n\\n43,887\\n\\n12 %\\n\\nProvision for Income Taxes\\n\\nProvision for income taxes, eﬀective tax rate and statutory federal income tax rate for 2023, 2022 and 2021 were as follows (dollars in millions):\\n\\n2023\\n\\n2022', metadata={'source': '../resources/filings/AAPL/AAPL-2023-10K.pdf'})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test that the retirever works\n",
    "retrieved_docs = parent_doc_retriever.invoke(\"apples's revenue 2023\")\n",
    "retrieved_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the same but use our new retriever\n",
    "parent_doc_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=parent_doc_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": get_prompt()},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Like before let's first create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = create_evaluation_dataset(parent_doc_qa, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What services does Apple offer through its Pay...</td>\n",
       "      <td>Answer: Apple offers various services through ...</td>\n",
       "      <td>[Rest of Asia Paciﬁc\\n\\nRest of Asia Paciﬁc ne...</td>\n",
       "      <td>Apple offers payment services through Apple Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the estimated maximum one-day loss in ...</td>\n",
       "      <td>Answer: The estimated maximum one-day loss in ...</td>\n",
       "      <td>[The Company applied a value-at-risk (“VAR”) m...</td>\n",
       "      <td>$1.0 billion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What drives Apple Inc.'s competitive edge &amp; ho...</td>\n",
       "      <td>Apple Inc.'s competitive edge is driven by its...</td>\n",
       "      <td>[The Company’s ability to compete successfully...</td>\n",
       "      <td>The information provided in the context sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are potential risks for Apple if it doesn...</td>\n",
       "      <td>Answer: The provided context does not directly...</td>\n",
       "      <td>[Critical Accounting Estimates\\n\\nThe preparat...</td>\n",
       "      <td>If Apple fails to meet regulatory expectations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What factors contributed to the 7% boost in iP...</td>\n",
       "      <td>Answer: The 7% boost in iPhone sales was prima...</td>\n",
       "      <td>[Rest of Asia Paciﬁc\\n\\nRest of Asia Paciﬁc ne...</td>\n",
       "      <td>The 7% boost in iPhone sales was primarily due...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What services does Apple offer through its Pay...   \n",
       "1  What is the estimated maximum one-day loss in ...   \n",
       "2  What drives Apple Inc.'s competitive edge & ho...   \n",
       "3  What are potential risks for Apple if it doesn...   \n",
       "4  What factors contributed to the 7% boost in iP...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Answer: Apple offers various services through ...   \n",
       "1  Answer: The estimated maximum one-day loss in ...   \n",
       "2  Apple Inc.'s competitive edge is driven by its...   \n",
       "3  Answer: The provided context does not directly...   \n",
       "4  Answer: The 7% boost in iPhone sales was prima...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Rest of Asia Paciﬁc\\n\\nRest of Asia Paciﬁc ne...   \n",
       "1  [The Company applied a value-at-risk (“VAR”) m...   \n",
       "2  [The Company’s ability to compete successfully...   \n",
       "3  [Critical Accounting Estimates\\n\\nThe preparat...   \n",
       "4  [Rest of Asia Paciﬁc\\n\\nRest of Asia Paciﬁc ne...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  Apple offers payment services through Apple Ca...  \n",
       "1                                       $1.0 billion  \n",
       "2  The information provided in the context sugges...  \n",
       "3  If Apple fails to meet regulatory expectations...  \n",
       "4  The 7% boost in iPhone sales was primarily due...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a1eff7088c4ddcae7beb4a465b1e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "\n",
    "parent_doc_faithfulness_metrics = evaluate_dataset(eval_dataset, [faithfulness], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0a0edde96d412b9efd43d3c0f240cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parent_doc_answer_relevancy_metrics = evaluate_dataset(eval_dataset, [answer_relevancy], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>parent_doc_faithfulness</th>\n",
       "      <th>parent_doc_answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.554265</td>\n",
       "      <td>0.714271</td>\n",
       "      <td>0.542544</td>\n",
       "      <td>0.611580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.425378</td>\n",
       "      <td>0.383305</td>\n",
       "      <td>0.446064</td>\n",
       "      <td>0.438532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.118056</td>\n",
       "      <td>0.788299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.887609</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.840875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfulness  answer_relevancy  parent_doc_faithfulness  \\\n",
       "count     19.000000         19.000000                19.000000   \n",
       "mean       0.554265          0.714271                 0.542544   \n",
       "std        0.425378          0.383305                 0.446064   \n",
       "min        0.000000          0.000000                 0.000000   \n",
       "25%        0.118056          0.788299                 0.000000   \n",
       "50%        0.642857          0.887609                 0.600000   \n",
       "75%        1.000000          0.930006                 1.000000   \n",
       "max        1.000000          1.000000                 1.000000   \n",
       "\n",
       "       parent_doc_answer_relevancy  \n",
       "count                    19.000000  \n",
       "mean                      0.611580  \n",
       "std                       0.438532  \n",
       "min                       0.000000  \n",
       "25%                       0.000000  \n",
       "50%                       0.840875  \n",
       "75%                       0.960617  \n",
       "max                       1.000000  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_doc_gen_metrics = parent_doc_faithfulness_metrics\n",
    "parent_doc_gen_metrics[\"answer_relevancy\"] = parent_doc_answer_relevancy_metrics[\"answer_relevancy\"]\n",
    "\n",
    "parent_doc_gen_metrics.rename(columns={\"faithfulness\": \"parent_doc_faithfulness\", \"answer_relevancy\": \"parent_doc_answer_relevancy\"}, inplace=True)\n",
    "\n",
    "overall_gen_metrics = pd.concat([gen_metrics_default, parent_doc_gen_metrics], axis=1)\n",
    "overall_gen_metrics.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the same for the retrieval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc91d722fbe146d1afbd81399de2da3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.metrics import context_recall, context_precision\n",
    "\n",
    "parent_doc_context_recall_metrics = evaluate_dataset(eval_dataset, [context_recall], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f924a889124a0e8595039e6532ff16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parent_doc_context_precision_metrics = evaluate_dataset(eval_dataset, [context_precision], llm, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>parent_doc_context_recall</th>\n",
       "      <th>parent_doc_context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.849415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.427491</td>\n",
       "      <td>0.311283</td>\n",
       "      <td>0.427491</td>\n",
       "      <td>0.329494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       context_recall  context_precision  parent_doc_context_recall  \\\n",
       "count       19.000000          19.000000                  19.000000   \n",
       "mean         0.728070           0.815789                   0.728070   \n",
       "std          0.427491           0.311283                   0.427491   \n",
       "min          0.000000           0.000000                   0.000000   \n",
       "25%          0.416667           0.708333                   0.416667   \n",
       "50%          1.000000           1.000000                   1.000000   \n",
       "75%          1.000000           1.000000                   1.000000   \n",
       "max          1.000000           1.000000                   1.000000   \n",
       "\n",
       "       parent_doc_context_precision  \n",
       "count                     19.000000  \n",
       "mean                       0.849415  \n",
       "std                        0.329494  \n",
       "min                        0.000000  \n",
       "25%                        0.958333  \n",
       "50%                        1.000000  \n",
       "75%                        1.000000  \n",
       "max                        1.000000  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_doc_ret_metrics = parent_doc_context_recall_metrics\n",
    "parent_doc_ret_metrics[\"context_precision\"] = parent_doc_context_precision_metrics[\"context_precision\"]\n",
    "\n",
    "parent_doc_ret_metrics.rename(columns={\"context_precision\": \"parent_doc_context_precision\", \"context_recall\": \"parent_doc_context_recall\"}, inplace=True)\n",
    "\n",
    "overall_ret_metrics = pd.concat([ret_metrics_default, parent_doc_ret_metrics], axis=1)\n",
    "\n",
    "overall_ret_metrics.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "In this case, we observe that the increased context provided by the parent document retriever had a slightly negative effect on the generation metrics, potentially reducing answer clarity via increased information. Basically no effect on context recall and a slightly positive effect on context precision, indicating that the smaller chunks for query comparison helped order the relevant context, but it appears that wasn't a limiting factor from the base case for this test. More conclusive testing would be needed to draw more authoritative conclusions, but this example shows us how to compare options in order to find the highest priority strategies for a given application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "\n",
    "In this notebook we covered:\n",
    "- why it's important to have an evaluation framework\n",
    "- the basic theory of RAGAS\n",
    "- how to calculate and generate faithfulness, answer_relevancy, context_precision, and context_recall\n",
    "- code to evaluate two different RAG chains to monitor how using a different retrieval strategy effects performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps: end-to-end evaluation\n",
    "\n",
    "As your pipeline matures and human labeled ground truth data is created the following metrics can be added for increased rigor. These additional metrics can be implemented similarly as the ones showcased above.\n",
    "\n",
    "\n",
    "## Answer correctness\n",
    "\n",
    "A weighted average of semantic and factual similarity where weights can be passed as a parameter.\n",
    "\n",
    "## Answer semantic similarity\n",
    "\n",
    "Measure distance between ground truth and the generated answer.\n",
    "\n",
    "#### Example process:\n",
    "- vectorize the ground truth answer and the generated answer\n",
    "- compute the cosine similarity.\n",
    "\n",
    "## Answer factual similarity\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "F1\\ Score = \\frac{TP}{TP + 0.5(FP + FN)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "TP (True Positive): Facts or statements that are present in both the ground truth and the generated answer.\n",
    "\n",
    "FP (False Positive): Facts or statements that are present in the generated answer but not in the ground truth.\n",
    "\n",
    "FN (False Negative): Facts or statements that are present in the ground truth but not in the generated answer.\n",
    "\n",
    "#### Example process:\n",
    "\n",
    "data:\n",
    "> Ground truth: Einstein was born in 1879 in Germany.\n",
    "> Generated Answer: Einstein was born in Spain in 1879.\n",
    "\n",
    "TP: [Einstein was born in 1879]\n",
    "\n",
    "FP: [Einstein was born in Spain]\n",
    "\n",
    "FN: [Einstein was born in Germany]\n",
    "\n",
    "F1 = (1 / 1 + 0.5(1 + 1)) = 1/2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
